{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy-only 2-layer Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using UCI Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from neural_net import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header names\n",
    "\n",
    "headers = ['age', 'sex','chest_pain','resting_blood_pressure',  \n",
    "        'serum_cholestoral', 'fasting_blood_sugar', 'resting_ecg_results',\n",
    "        'max_heart_rate_achieved', 'exercise_induced_angina', 'oldpeak',\"slope of the peak\",\n",
    "        'num_of_major_vessels','thal', 'heart_disease']\n",
    "\n",
    "# Make DF\n",
    "\n",
    "heart_df = pd.read_csv('heart.dat', sep = ' ', names = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 14)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        0\n",
       "sex                        0\n",
       "chest_pain                 0\n",
       "resting_blood_pressure     0\n",
       "serum_cholestoral          0\n",
       "fasting_blood_sugar        0\n",
       "resting_ecg_results        0\n",
       "max_heart_rate_achieved    0\n",
       "exercise_induced_angina    0\n",
       "oldpeak                    0\n",
       "slope of the peak          0\n",
       "num_of_major_vessels       0\n",
       "thal                       0\n",
       "heart_disease              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "heart_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        float64\n",
       "sex                        float64\n",
       "chest_pain                 float64\n",
       "resting_blood_pressure     float64\n",
       "serum_cholestoral          float64\n",
       "fasting_blood_sugar        float64\n",
       "resting_ecg_results        float64\n",
       "max_heart_rate_achieved    float64\n",
       "exercise_induced_angina    float64\n",
       "oldpeak                    float64\n",
       "slope of the peak          float64\n",
       "num_of_major_vessels       float64\n",
       "thal                       float64\n",
       "heart_disease                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "heart_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X = heart_df.drop(columns = ['heart_disease'])\n",
    "\n",
    "# Enumerate target class i.e. labels\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(1,0)\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(2,1)\n",
    "\n",
    "y_label = heart_df['heart_disease'].values.reshape(X.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (216, 13)\n",
      "Shape of test set is (54, 13)\n",
      "Shape of train label is (216, 1)\n",
      "Shape of test labels is (54, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale values\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(Xtrain)\n",
    "Xtrain = sc.transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)\n",
    "\n",
    "print(f\"Shape of train set is {Xtrain.shape}\")\n",
    "print(f\"Shape of test set is {Xtest.shape}\")\n",
    "print(f\"Shape of train label is {ytrain.shape}\")\n",
    "print(f\"Shape of test labels is {ytest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Use it to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class of Neural Net using default parameters\n",
    "\n",
    "nn = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, calculated loss: 1.406818099455712\n",
      "Training epoch 1, calculated loss: 1.2283644538724772\n",
      "Training epoch 2, calculated loss: 1.1027579343396263\n",
      "Training epoch 3, calculated loss: 1.010277834670556\n",
      "Training epoch 4, calculated loss: 0.9375456811690593\n",
      "Training epoch 5, calculated loss: 0.8771342674193834\n",
      "Training epoch 6, calculated loss: 0.8256029193292977\n",
      "Training epoch 7, calculated loss: 0.7809976438171825\n",
      "Training epoch 8, calculated loss: 0.7419983603962279\n",
      "Training epoch 9, calculated loss: 0.707447080573352\n",
      "Training epoch 10, calculated loss: 0.6761308099914313\n",
      "Training epoch 11, calculated loss: 0.6479719529677453\n",
      "Training epoch 12, calculated loss: 0.6225758410329949\n",
      "Training epoch 13, calculated loss: 0.5995873203513066\n",
      "Training epoch 14, calculated loss: 0.5787978131048888\n",
      "Training epoch 15, calculated loss: 0.5599182115487729\n",
      "Training epoch 16, calculated loss: 0.5427118970422696\n",
      "Training epoch 17, calculated loss: 0.5270878681118275\n",
      "Training epoch 18, calculated loss: 0.5127870376789314\n",
      "Training epoch 19, calculated loss: 0.499702605855535\n",
      "Training epoch 20, calculated loss: 0.48738521253819006\n",
      "Training epoch 21, calculated loss: 0.476190151824692\n",
      "Training epoch 22, calculated loss: 0.4660017416103337\n",
      "Training epoch 23, calculated loss: 0.4566221760866736\n",
      "Training epoch 24, calculated loss: 0.44801274903895083\n",
      "Training epoch 25, calculated loss: 0.44006195321689057\n",
      "Training epoch 26, calculated loss: 0.432662096983625\n",
      "Training epoch 27, calculated loss: 0.42579673582802635\n",
      "Training epoch 28, calculated loss: 0.4193526960161447\n",
      "Training epoch 29, calculated loss: 0.41324847065923803\n",
      "Training epoch 30, calculated loss: 0.4074599294185932\n",
      "Training epoch 31, calculated loss: 0.40202222246799607\n",
      "Training epoch 32, calculated loss: 0.3968899842644129\n",
      "Training epoch 33, calculated loss: 0.3920397357291702\n",
      "Training epoch 34, calculated loss: 0.3874509214748642\n",
      "Training epoch 35, calculated loss: 0.3830958350064345\n",
      "Training epoch 36, calculated loss: 0.3789607677725741\n",
      "Training epoch 37, calculated loss: 0.37502934313790387\n",
      "Training epoch 38, calculated loss: 0.37128637914437745\n",
      "Training epoch 39, calculated loss: 0.36771249418561974\n",
      "Training epoch 40, calculated loss: 0.3642833861662177\n",
      "Training epoch 41, calculated loss: 0.3610015600693919\n",
      "Training epoch 42, calculated loss: 0.3578906297976741\n",
      "Training epoch 43, calculated loss: 0.3549156523498778\n",
      "Training epoch 44, calculated loss: 0.3520709025825898\n",
      "Training epoch 45, calculated loss: 0.3493598084388319\n",
      "Training epoch 46, calculated loss: 0.346768845132244\n",
      "Training epoch 47, calculated loss: 0.34431068879480353\n",
      "Training epoch 48, calculated loss: 0.34194282652162544\n",
      "Training epoch 49, calculated loss: 0.3396583732933476\n",
      "Training epoch 50, calculated loss: 0.3374581250738244\n",
      "Training epoch 51, calculated loss: 0.33534760607313313\n",
      "Training epoch 52, calculated loss: 0.33330547606352195\n",
      "Training epoch 53, calculated loss: 0.33133938501962\n",
      "Training epoch 54, calculated loss: 0.32943711682743476\n",
      "Training epoch 55, calculated loss: 0.327561226258419\n",
      "Training epoch 56, calculated loss: 0.32575478917638145\n",
      "Training epoch 57, calculated loss: 0.3240014979718713\n",
      "Training epoch 58, calculated loss: 0.3222911256958769\n",
      "Training epoch 59, calculated loss: 0.320614686718432\n",
      "Training epoch 60, calculated loss: 0.3189950287443454\n",
      "Training epoch 61, calculated loss: 0.31743139477363524\n",
      "Training epoch 62, calculated loss: 0.315898744016534\n",
      "Training epoch 63, calculated loss: 0.3144261330659574\n",
      "Training epoch 64, calculated loss: 0.312979010473956\n",
      "Training epoch 65, calculated loss: 0.3115704557826012\n",
      "Training epoch 66, calculated loss: 0.3102387909138682\n",
      "Training epoch 67, calculated loss: 0.30892274240067386\n",
      "Training epoch 68, calculated loss: 0.30765010587991815\n",
      "Training epoch 69, calculated loss: 0.30639677395985704\n",
      "Training epoch 70, calculated loss: 0.3052113102149649\n",
      "Training epoch 71, calculated loss: 0.3040603570147317\n",
      "Training epoch 72, calculated loss: 0.30293845881919423\n",
      "Training epoch 73, calculated loss: 0.30183492009912677\n",
      "Training epoch 74, calculated loss: 0.3007443230734181\n",
      "Training epoch 75, calculated loss: 0.29967130343718507\n",
      "Training epoch 76, calculated loss: 0.29862341684204396\n",
      "Training epoch 77, calculated loss: 0.2975953115831883\n",
      "Training epoch 78, calculated loss: 0.2965805948342925\n",
      "Training epoch 79, calculated loss: 0.2955858072889425\n",
      "Training epoch 80, calculated loss: 0.2946025425015101\n",
      "Training epoch 81, calculated loss: 0.2936528622103815\n",
      "Training epoch 82, calculated loss: 0.29270959325424567\n",
      "Training epoch 83, calculated loss: 0.2917867375977857\n",
      "Training epoch 84, calculated loss: 0.2908766664032826\n",
      "Training epoch 85, calculated loss: 0.2899858794918911\n",
      "Training epoch 86, calculated loss: 0.2891217726542763\n",
      "Training epoch 87, calculated loss: 0.2882534060741006\n",
      "Training epoch 88, calculated loss: 0.28741232695069635\n",
      "Training epoch 89, calculated loss: 0.28658222439112885\n",
      "Training epoch 90, calculated loss: 0.2857393049629605\n",
      "Training epoch 91, calculated loss: 0.28488377530196873\n",
      "Training epoch 92, calculated loss: 0.2840357881439885\n",
      "Training epoch 93, calculated loss: 0.2831916166264467\n",
      "Training epoch 94, calculated loss: 0.2823680182360841\n",
      "Training epoch 95, calculated loss: 0.28153740084295914\n",
      "Training epoch 96, calculated loss: 0.28071874261248086\n",
      "Training epoch 97, calculated loss: 0.2799062326813389\n",
      "Training epoch 98, calculated loss: 0.2790974743022777\n",
      "Training epoch 99, calculated loss: 0.2783107364057317\n",
      "Training epoch 100, calculated loss: 0.27752267595856\n",
      "Training epoch 101, calculated loss: 0.276746158103033\n",
      "Training epoch 102, calculated loss: 0.27598304715732486\n",
      "Training epoch 103, calculated loss: 0.27526560371594744\n",
      "Training epoch 104, calculated loss: 0.2745564389923945\n",
      "Training epoch 105, calculated loss: 0.27386237629456256\n",
      "Training epoch 106, calculated loss: 0.27319294620008727\n",
      "Training epoch 107, calculated loss: 0.27252215813264585\n",
      "Training epoch 108, calculated loss: 0.2718580260151962\n",
      "Training epoch 109, calculated loss: 0.2711923110045857\n",
      "Training epoch 110, calculated loss: 0.2705296337982729\n",
      "Training epoch 111, calculated loss: 0.2698683935245679\n",
      "Training epoch 112, calculated loss: 0.269213933482101\n",
      "Training epoch 113, calculated loss: 0.2685728735027599\n",
      "Training epoch 114, calculated loss: 0.2679320976157896\n",
      "Training epoch 115, calculated loss: 0.26729387112097147\n",
      "Training epoch 116, calculated loss: 0.26666744357147076\n",
      "Training epoch 117, calculated loss: 0.2660492049364576\n",
      "Training epoch 118, calculated loss: 0.2654278058608459\n",
      "Training epoch 119, calculated loss: 0.26481833920220205\n",
      "Training epoch 120, calculated loss: 0.2642025845392992\n",
      "Training epoch 121, calculated loss: 0.2635980803620852\n",
      "Training epoch 122, calculated loss: 0.2629979179693585\n",
      "Training epoch 123, calculated loss: 0.26240444035825994\n",
      "Training epoch 124, calculated loss: 0.2618081094455079\n",
      "Training epoch 125, calculated loss: 0.26122323147646254\n",
      "Training epoch 126, calculated loss: 0.26063438451572873\n",
      "Training epoch 127, calculated loss: 0.2600492974146751\n",
      "Training epoch 128, calculated loss: 0.25946706587215085\n",
      "Training epoch 129, calculated loss: 0.2589003214821959\n",
      "Training epoch 130, calculated loss: 0.25832673077991153\n",
      "Training epoch 131, calculated loss: 0.25775349782981116\n",
      "Training epoch 132, calculated loss: 0.2571809593347549\n",
      "Training epoch 133, calculated loss: 0.2566217662792042\n",
      "Training epoch 134, calculated loss: 0.25605310226066813\n",
      "Training epoch 135, calculated loss: 0.2554915633487888\n",
      "Training epoch 136, calculated loss: 0.2549334344446226\n",
      "Training epoch 137, calculated loss: 0.25437693001749573\n",
      "Training epoch 138, calculated loss: 0.25382471688608255\n",
      "Training epoch 139, calculated loss: 0.25327245432645695\n",
      "Training epoch 140, calculated loss: 0.2527279179406165\n",
      "Training epoch 141, calculated loss: 0.25217915831365895\n",
      "Training epoch 142, calculated loss: 0.25163781252928474\n",
      "Training epoch 143, calculated loss: 0.2510910290687681\n",
      "Training epoch 144, calculated loss: 0.2505554057293129\n",
      "Training epoch 145, calculated loss: 0.25001024432803864\n",
      "Training epoch 146, calculated loss: 0.24947413070800084\n",
      "Training epoch 147, calculated loss: 0.24894197281573963\n",
      "Training epoch 148, calculated loss: 0.24841204559725721\n",
      "Training epoch 149, calculated loss: 0.2478847909568514\n",
      "Training epoch 150, calculated loss: 0.24735737248299336\n",
      "Training epoch 151, calculated loss: 0.2468379362565074\n",
      "Training epoch 152, calculated loss: 0.2463162713651728\n",
      "Training epoch 153, calculated loss: 0.24580155619808317\n",
      "Training epoch 154, calculated loss: 0.2452840582774365\n",
      "Training epoch 155, calculated loss: 0.24476858805520996\n",
      "Training epoch 156, calculated loss: 0.2442588084173961\n",
      "Training epoch 157, calculated loss: 0.2437435362084511\n",
      "Training epoch 158, calculated loss: 0.24323462312043742\n",
      "Training epoch 159, calculated loss: 0.24272893998407646\n",
      "Training epoch 160, calculated loss: 0.24223230619843128\n",
      "Training epoch 161, calculated loss: 0.24174067957462392\n",
      "Training epoch 162, calculated loss: 0.2412492025963735\n",
      "Training epoch 163, calculated loss: 0.24075542394516838\n",
      "Training epoch 164, calculated loss: 0.24026088887203656\n",
      "Training epoch 165, calculated loss: 0.23977173927195933\n",
      "Training epoch 166, calculated loss: 0.2392809939288868\n",
      "Training epoch 167, calculated loss: 0.23879307544349948\n",
      "Training epoch 168, calculated loss: 0.23831239789946876\n",
      "Training epoch 169, calculated loss: 0.2378248066660743\n",
      "Training epoch 170, calculated loss: 0.23734146228200687\n",
      "Training epoch 171, calculated loss: 0.23685621606243795\n",
      "Training epoch 172, calculated loss: 0.2363680485279514\n",
      "Training epoch 173, calculated loss: 0.23587090742674685\n",
      "Training epoch 174, calculated loss: 0.23537863633274222\n",
      "Training epoch 175, calculated loss: 0.2348903195279528\n",
      "Training epoch 176, calculated loss: 0.2344042567609047\n",
      "Training epoch 177, calculated loss: 0.23391175624870872\n",
      "Training epoch 178, calculated loss: 0.23343017770514024\n",
      "Training epoch 179, calculated loss: 0.23293822165842706\n",
      "Training epoch 180, calculated loss: 0.2324608315998643\n",
      "Training epoch 181, calculated loss: 0.23197788724357066\n",
      "Training epoch 182, calculated loss: 0.23149935858861018\n",
      "Training epoch 183, calculated loss: 0.2310124666397743\n",
      "Training epoch 184, calculated loss: 0.23053584748400333\n",
      "Training epoch 185, calculated loss: 0.23004806467142344\n",
      "Training epoch 186, calculated loss: 0.22957871113285785\n",
      "Training epoch 187, calculated loss: 0.22909778836401165\n",
      "Training epoch 188, calculated loss: 0.22862656328057268\n",
      "Training epoch 189, calculated loss: 0.22814334755455365\n",
      "Training epoch 190, calculated loss: 0.2276709835482682\n",
      "Training epoch 191, calculated loss: 0.22719038218911317\n",
      "Training epoch 192, calculated loss: 0.22672138605294875\n",
      "Training epoch 193, calculated loss: 0.22624451954760144\n",
      "Training epoch 194, calculated loss: 0.22578737541844868\n",
      "Training epoch 195, calculated loss: 0.2253161523571191\n",
      "Training epoch 196, calculated loss: 0.22485424516447283\n",
      "Training epoch 197, calculated loss: 0.22439134919349\n",
      "Training epoch 198, calculated loss: 0.2239280865753238\n",
      "Training epoch 199, calculated loss: 0.22346958764249777\n",
      "Training epoch 200, calculated loss: 0.22301092124771404\n",
      "Training epoch 201, calculated loss: 0.22255614764838536\n",
      "Training epoch 202, calculated loss: 0.22209400264876789\n",
      "Training epoch 203, calculated loss: 0.22164462420329273\n",
      "Training epoch 204, calculated loss: 0.22119825473660013\n",
      "Training epoch 205, calculated loss: 0.2207642169610009\n",
      "Training epoch 206, calculated loss: 0.22033261045718436\n",
      "Training epoch 207, calculated loss: 0.2199035580153558\n",
      "Training epoch 208, calculated loss: 0.2194706450749502\n",
      "Training epoch 209, calculated loss: 0.21904803815622734\n",
      "Training epoch 210, calculated loss: 0.21863185760205067\n",
      "Training epoch 211, calculated loss: 0.21819885067632558\n",
      "Training epoch 212, calculated loss: 0.21778363187425878\n",
      "Training epoch 213, calculated loss: 0.21735914641300724\n",
      "Training epoch 214, calculated loss: 0.2169282705446066\n",
      "Training epoch 215, calculated loss: 0.21651063110765484\n",
      "Training epoch 216, calculated loss: 0.21608748592860538\n",
      "Training epoch 217, calculated loss: 0.21566699955088842\n",
      "Training epoch 218, calculated loss: 0.21526024515912645\n",
      "Training epoch 219, calculated loss: 0.2148374729826649\n",
      "Training epoch 220, calculated loss: 0.21440432542116394\n",
      "Training epoch 221, calculated loss: 0.2139958620223014\n",
      "Training epoch 222, calculated loss: 0.2135679645465027\n",
      "Training epoch 223, calculated loss: 0.21315127653190946\n",
      "Training epoch 224, calculated loss: 0.2127425135582099\n",
      "Training epoch 225, calculated loss: 0.21232183920557945\n",
      "Training epoch 226, calculated loss: 0.2119010342251094\n",
      "Training epoch 227, calculated loss: 0.21148229859970416\n",
      "Training epoch 228, calculated loss: 0.21106058291309945\n",
      "Training epoch 229, calculated loss: 0.21065369209809542\n",
      "Training epoch 230, calculated loss: 0.21023967067439361\n",
      "Training epoch 231, calculated loss: 0.20982902672714915\n",
      "Training epoch 232, calculated loss: 0.20939613306028493\n",
      "Training epoch 233, calculated loss: 0.2089876071558346\n",
      "Training epoch 234, calculated loss: 0.20857927972805704\n",
      "Training epoch 235, calculated loss: 0.20816528682003066\n",
      "Training epoch 236, calculated loss: 0.2077553652163357\n",
      "Training epoch 237, calculated loss: 0.20733396932847467\n",
      "Training epoch 238, calculated loss: 0.20691243935101947\n",
      "Training epoch 239, calculated loss: 0.2064980819185977\n",
      "Training epoch 240, calculated loss: 0.20609397478307614\n",
      "Training epoch 241, calculated loss: 0.20566760276359936\n",
      "Training epoch 242, calculated loss: 0.20525538044164046\n",
      "Training epoch 243, calculated loss: 0.20484343749057887\n",
      "Training epoch 244, calculated loss: 0.20441927750730154\n",
      "Training epoch 245, calculated loss: 0.2040126296924069\n",
      "Training epoch 246, calculated loss: 0.20362076530831855\n",
      "Training epoch 247, calculated loss: 0.20319828652626068\n",
      "Training epoch 248, calculated loss: 0.20277780790284053\n",
      "Training epoch 249, calculated loss: 0.20236571298216968\n",
      "Training epoch 250, calculated loss: 0.20195238099992036\n",
      "Training epoch 251, calculated loss: 0.20156261838124942\n",
      "Training epoch 252, calculated loss: 0.20115952703475037\n",
      "Training epoch 253, calculated loss: 0.2007483563441138\n",
      "Training epoch 254, calculated loss: 0.2003377703110201\n",
      "Training epoch 255, calculated loss: 0.1999290356881397\n",
      "Training epoch 256, calculated loss: 0.19954320203371445\n",
      "Training epoch 257, calculated loss: 0.199182485115241\n",
      "Training epoch 258, calculated loss: 0.19880105043699905\n",
      "Training epoch 259, calculated loss: 0.19840192422729222\n",
      "Training epoch 260, calculated loss: 0.1980207292858923\n",
      "Training epoch 261, calculated loss: 0.19764848083957062\n",
      "Training epoch 262, calculated loss: 0.19726230645401954\n",
      "Training epoch 263, calculated loss: 0.19688050465637108\n",
      "Training epoch 264, calculated loss: 0.19651275770998625\n",
      "Training epoch 265, calculated loss: 0.19612384179538422\n",
      "Training epoch 266, calculated loss: 0.1957630147609834\n",
      "Training epoch 267, calculated loss: 0.1953722569566314\n",
      "Training epoch 268, calculated loss: 0.19498930212044344\n",
      "Training epoch 269, calculated loss: 0.19461641793214018\n",
      "Training epoch 270, calculated loss: 0.1942441575982174\n",
      "Training epoch 271, calculated loss: 0.19386863167293533\n",
      "Training epoch 272, calculated loss: 0.19348478056753204\n",
      "Training epoch 273, calculated loss: 0.19311064629426253\n",
      "Training epoch 274, calculated loss: 0.192726628477748\n",
      "Training epoch 275, calculated loss: 0.19235661948829577\n",
      "Training epoch 276, calculated loss: 0.19196502553151357\n",
      "Training epoch 277, calculated loss: 0.1915986397577637\n",
      "Training epoch 278, calculated loss: 0.19122341022500783\n",
      "Training epoch 279, calculated loss: 0.19083085175095132\n",
      "Training epoch 280, calculated loss: 0.19046145055031888\n",
      "Training epoch 281, calculated loss: 0.19009102370011108\n",
      "Training epoch 282, calculated loss: 0.18970648638050977\n",
      "Training epoch 283, calculated loss: 0.18934881202258527\n",
      "Training epoch 284, calculated loss: 0.1889738749204568\n",
      "Training epoch 285, calculated loss: 0.1886218398356439\n",
      "Training epoch 286, calculated loss: 0.18825106361899036\n",
      "Training epoch 287, calculated loss: 0.18786522308620968\n",
      "Training epoch 288, calculated loss: 0.1875090812628159\n",
      "Training epoch 289, calculated loss: 0.1871345473867985\n",
      "Training epoch 290, calculated loss: 0.18676501545272986\n",
      "Training epoch 291, calculated loss: 0.18641840187728403\n",
      "Training epoch 292, calculated loss: 0.18604482687305232\n",
      "Training epoch 293, calculated loss: 0.18568545639933703\n",
      "Training epoch 294, calculated loss: 0.18532975861852727\n",
      "Training epoch 295, calculated loss: 0.18498483777153138\n",
      "Training epoch 296, calculated loss: 0.18461185314995485\n",
      "Training epoch 297, calculated loss: 0.18425013332752715\n",
      "Training epoch 298, calculated loss: 0.18391152812839598\n",
      "Training epoch 299, calculated loss: 0.18355248732093069\n",
      "Training epoch 300, calculated loss: 0.18320601006803303\n",
      "Training epoch 301, calculated loss: 0.1828540115677848\n",
      "Training epoch 302, calculated loss: 0.18251686411492968\n",
      "Training epoch 303, calculated loss: 0.1821732197656081\n",
      "Training epoch 304, calculated loss: 0.1818170142445932\n",
      "Training epoch 305, calculated loss: 0.1814719297134536\n",
      "Training epoch 306, calculated loss: 0.18113902574112584\n",
      "Training epoch 307, calculated loss: 0.1807825447128808\n",
      "Training epoch 308, calculated loss: 0.18045659233714778\n",
      "Training epoch 309, calculated loss: 0.1801078197240961\n",
      "Training epoch 310, calculated loss: 0.17979673876733465\n",
      "Training epoch 311, calculated loss: 0.1794675592851685\n",
      "Training epoch 312, calculated loss: 0.1791372893996969\n",
      "Training epoch 313, calculated loss: 0.1788165049438514\n",
      "Training epoch 314, calculated loss: 0.17849558383987\n",
      "Training epoch 315, calculated loss: 0.17816066147475693\n",
      "Training epoch 316, calculated loss: 0.1778349304644559\n",
      "Training epoch 317, calculated loss: 0.17751311246325255\n",
      "Training epoch 318, calculated loss: 0.17719705052409218\n",
      "Training epoch 319, calculated loss: 0.17686148026632312\n",
      "Training epoch 320, calculated loss: 0.17655129211747075\n",
      "Training epoch 321, calculated loss: 0.17623781820387602\n",
      "Training epoch 322, calculated loss: 0.1759291444132664\n",
      "Training epoch 323, calculated loss: 0.1756326973705929\n",
      "Training epoch 324, calculated loss: 0.17534433945718303\n",
      "Training epoch 325, calculated loss: 0.17505326948737784\n",
      "Training epoch 326, calculated loss: 0.17476336279446636\n",
      "Training epoch 327, calculated loss: 0.17447883345769297\n",
      "Training epoch 328, calculated loss: 0.17419661358090496\n",
      "Training epoch 329, calculated loss: 0.1739487350853175\n",
      "Training epoch 330, calculated loss: 0.1736765846606785\n",
      "Training epoch 331, calculated loss: 0.1734207068236491\n",
      "Training epoch 332, calculated loss: 0.1731866336973259\n",
      "Training epoch 333, calculated loss: 0.17294227082277783\n",
      "Training epoch 334, calculated loss: 0.1726946777873684\n",
      "Training epoch 335, calculated loss: 0.17243647813121277\n",
      "Training epoch 336, calculated loss: 0.1721803136433498\n",
      "Training epoch 337, calculated loss: 0.17195028784810754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 338, calculated loss: 0.17169346297302743\n",
      "Training epoch 339, calculated loss: 0.17145706532276012\n",
      "Training epoch 340, calculated loss: 0.1711999783518241\n",
      "Training epoch 341, calculated loss: 0.17097264265344975\n",
      "Training epoch 342, calculated loss: 0.17072128054425517\n",
      "Training epoch 343, calculated loss: 0.17048209845709925\n",
      "Training epoch 344, calculated loss: 0.17023203315163624\n",
      "Training epoch 345, calculated loss: 0.1699941945008134\n",
      "Training epoch 346, calculated loss: 0.16974128122812893\n",
      "Training epoch 347, calculated loss: 0.16948323256936773\n",
      "Training epoch 348, calculated loss: 0.16924536060296808\n",
      "Training epoch 349, calculated loss: 0.16898732423520862\n",
      "Training epoch 350, calculated loss: 0.1687340466210543\n",
      "Training epoch 351, calculated loss: 0.16847927733185036\n",
      "Training epoch 352, calculated loss: 0.16825200969880444\n",
      "Training epoch 353, calculated loss: 0.1680184489296365\n",
      "Training epoch 354, calculated loss: 0.16775547357820084\n",
      "Training epoch 355, calculated loss: 0.16751159568829121\n",
      "Training epoch 356, calculated loss: 0.1672623117203635\n",
      "Training epoch 357, calculated loss: 0.1670342009015608\n",
      "Training epoch 358, calculated loss: 0.16680226621584215\n",
      "Training epoch 359, calculated loss: 0.16657145511818808\n",
      "Training epoch 360, calculated loss: 0.16630767040088318\n",
      "Training epoch 361, calculated loss: 0.16607419005800161\n",
      "Training epoch 362, calculated loss: 0.16585328429306176\n",
      "Training epoch 363, calculated loss: 0.16560557466739984\n",
      "Training epoch 364, calculated loss: 0.16536030056040155\n",
      "Training epoch 365, calculated loss: 0.16513041112805482\n",
      "Training epoch 366, calculated loss: 0.1648936309149001\n",
      "Training epoch 367, calculated loss: 0.16466072675561894\n",
      "Training epoch 368, calculated loss: 0.1644214020694981\n",
      "Training epoch 369, calculated loss: 0.16418212751306865\n",
      "Training epoch 370, calculated loss: 0.16394143349632975\n",
      "Training epoch 371, calculated loss: 0.16371446101932746\n",
      "Training epoch 372, calculated loss: 0.16349024136128779\n",
      "Training epoch 373, calculated loss: 0.16324980532824224\n",
      "Training epoch 374, calculated loss: 0.16301616856081388\n",
      "Training epoch 375, calculated loss: 0.16277348779936215\n",
      "Training epoch 376, calculated loss: 0.16255532024251432\n",
      "Training epoch 377, calculated loss: 0.16231595238518823\n",
      "Training epoch 378, calculated loss: 0.1620866097438773\n",
      "Training epoch 379, calculated loss: 0.16184450053069116\n",
      "Training epoch 380, calculated loss: 0.16163600750490625\n",
      "Training epoch 381, calculated loss: 0.16140096811266103\n",
      "Training epoch 382, calculated loss: 0.16116446442739135\n",
      "Training epoch 383, calculated loss: 0.16093534478141389\n",
      "Training epoch 384, calculated loss: 0.1607157263221618\n",
      "Training epoch 385, calculated loss: 0.1604797973286627\n",
      "Training epoch 386, calculated loss: 0.1602524754063829\n",
      "Training epoch 387, calculated loss: 0.16002321456542906\n",
      "Training epoch 388, calculated loss: 0.15980338227215293\n",
      "Training epoch 389, calculated loss: 0.15958713099884503\n",
      "Training epoch 390, calculated loss: 0.15938264923304102\n",
      "Training epoch 391, calculated loss: 0.15914168270242113\n",
      "Training epoch 392, calculated loss: 0.1589254563886855\n",
      "Training epoch 393, calculated loss: 0.15871048884938777\n",
      "Training epoch 394, calculated loss: 0.15849417766186147\n",
      "Training epoch 395, calculated loss: 0.158271723475272\n",
      "Training epoch 396, calculated loss: 0.15804571410781076\n",
      "Training epoch 397, calculated loss: 0.15784562631186577\n",
      "Training epoch 398, calculated loss: 0.1576369728677055\n",
      "Training epoch 399, calculated loss: 0.15742515984689748\n",
      "Training epoch 400, calculated loss: 0.1571858211718213\n",
      "Training epoch 401, calculated loss: 0.15698599481324108\n",
      "Training epoch 402, calculated loss: 0.1567750978752191\n",
      "Training epoch 403, calculated loss: 0.15655796423517826\n",
      "Training epoch 404, calculated loss: 0.1563291245160032\n",
      "Training epoch 405, calculated loss: 0.1561085345953448\n",
      "Training epoch 406, calculated loss: 0.155913303447273\n",
      "Training epoch 407, calculated loss: 0.15569104338576284\n",
      "Training epoch 408, calculated loss: 0.1554782958364362\n",
      "Training epoch 409, calculated loss: 0.15524938642490338\n",
      "Training epoch 410, calculated loss: 0.1550566188530461\n",
      "Training epoch 411, calculated loss: 0.15484971009912843\n",
      "Training epoch 412, calculated loss: 0.15462897713674464\n",
      "Training epoch 413, calculated loss: 0.1543972762265905\n",
      "Training epoch 414, calculated loss: 0.1542018293446751\n",
      "Training epoch 415, calculated loss: 0.15398717441734588\n",
      "Training epoch 416, calculated loss: 0.1537806086744322\n",
      "Training epoch 417, calculated loss: 0.15354170684750246\n",
      "Training epoch 418, calculated loss: 0.15335972380353524\n",
      "Training epoch 419, calculated loss: 0.15314548771175726\n",
      "Training epoch 420, calculated loss: 0.15292860566390828\n",
      "Training epoch 421, calculated loss: 0.15271025318384124\n",
      "Training epoch 422, calculated loss: 0.1525127899603239\n",
      "Training epoch 423, calculated loss: 0.15231247878553822\n",
      "Training epoch 424, calculated loss: 0.1521108764328087\n",
      "Training epoch 425, calculated loss: 0.15190066513571876\n",
      "Training epoch 426, calculated loss: 0.15174397750898871\n",
      "Training epoch 427, calculated loss: 0.15154324967660396\n",
      "Training epoch 428, calculated loss: 0.1513364468688277\n",
      "Training epoch 429, calculated loss: 0.1511480397586336\n",
      "Training epoch 430, calculated loss: 0.15095051070204293\n",
      "Training epoch 431, calculated loss: 0.1507677867515939\n",
      "Training epoch 432, calculated loss: 0.1505720638455779\n",
      "Training epoch 433, calculated loss: 0.1503764324283442\n",
      "Training epoch 434, calculated loss: 0.15019125186645826\n",
      "Training epoch 435, calculated loss: 0.14999649628648593\n",
      "Training epoch 436, calculated loss: 0.14981817237114212\n",
      "Training epoch 437, calculated loss: 0.14960372682190795\n",
      "Training epoch 438, calculated loss: 0.14943510557256834\n",
      "Training epoch 439, calculated loss: 0.14923156198711446\n",
      "Training epoch 440, calculated loss: 0.1490323110302897\n",
      "Training epoch 441, calculated loss: 0.14883953673094974\n",
      "Training epoch 442, calculated loss: 0.1486518024178283\n",
      "Training epoch 443, calculated loss: 0.14846139440125317\n",
      "Training epoch 444, calculated loss: 0.14826404317333006\n",
      "Training epoch 445, calculated loss: 0.14807262470931376\n",
      "Training epoch 446, calculated loss: 0.14789467609569903\n",
      "Training epoch 447, calculated loss: 0.14768747847749986\n",
      "Training epoch 448, calculated loss: 0.1474960417560906\n",
      "Training epoch 449, calculated loss: 0.14730509278325882\n",
      "Training epoch 450, calculated loss: 0.1471359462572863\n",
      "Training epoch 451, calculated loss: 0.14693179436827272\n",
      "Training epoch 452, calculated loss: 0.1467399177914218\n",
      "Training epoch 453, calculated loss: 0.14655330684991322\n",
      "Training epoch 454, calculated loss: 0.14637761127055562\n",
      "Training epoch 455, calculated loss: 0.1461791588962648\n",
      "Training epoch 456, calculated loss: 0.14598535187828576\n",
      "Training epoch 457, calculated loss: 0.14580865595688222\n",
      "Training epoch 458, calculated loss: 0.14562088851220337\n",
      "Training epoch 459, calculated loss: 0.14544189874108715\n",
      "Training epoch 460, calculated loss: 0.14524241900311582\n",
      "Training epoch 461, calculated loss: 0.14507407066121558\n",
      "Training epoch 462, calculated loss: 0.14487802313960552\n",
      "Training epoch 463, calculated loss: 0.14469551592094915\n",
      "Training epoch 464, calculated loss: 0.14451702463635938\n",
      "Training epoch 465, calculated loss: 0.14433866494052291\n",
      "Training epoch 466, calculated loss: 0.14415349748542902\n",
      "Training epoch 467, calculated loss: 0.1439582078598358\n",
      "Training epoch 468, calculated loss: 0.14378872473525617\n",
      "Training epoch 469, calculated loss: 0.14360374420805855\n",
      "Training epoch 470, calculated loss: 0.14341147973412957\n",
      "Training epoch 471, calculated loss: 0.14322867211484686\n",
      "Training epoch 472, calculated loss: 0.14305041911098115\n",
      "Training epoch 473, calculated loss: 0.142869843422236\n",
      "Training epoch 474, calculated loss: 0.14268017204880018\n",
      "Training epoch 475, calculated loss: 0.1425192685560983\n",
      "Training epoch 476, calculated loss: 0.14233459141981308\n",
      "Training epoch 477, calculated loss: 0.1421460365936759\n",
      "Training epoch 478, calculated loss: 0.1419797122652719\n",
      "Training epoch 479, calculated loss: 0.14182147289761057\n",
      "Training epoch 480, calculated loss: 0.14163200080777402\n",
      "Training epoch 481, calculated loss: 0.14143658552482288\n",
      "Training epoch 482, calculated loss: 0.14128617243524833\n",
      "Training epoch 483, calculated loss: 0.14111372747242554\n",
      "Training epoch 484, calculated loss: 0.14092477685885882\n",
      "Training epoch 485, calculated loss: 0.1407490590506156\n",
      "Training epoch 486, calculated loss: 0.14058417728075964\n",
      "Training epoch 487, calculated loss: 0.14042727248020903\n",
      "Training epoch 488, calculated loss: 0.1402195545926085\n",
      "Training epoch 489, calculated loss: 0.14006221153078655\n",
      "Training epoch 490, calculated loss: 0.13990036718235757\n",
      "Training epoch 491, calculated loss: 0.13972298261865274\n",
      "Training epoch 492, calculated loss: 0.13955754921744454\n",
      "Training epoch 493, calculated loss: 0.1393711758206643\n",
      "Training epoch 494, calculated loss: 0.139227297141135\n",
      "Training epoch 495, calculated loss: 0.1390445106681748\n",
      "Training epoch 496, calculated loss: 0.1388813906825923\n",
      "Training epoch 497, calculated loss: 0.13870089357746185\n",
      "Training epoch 498, calculated loss: 0.13854056453284114\n",
      "Training epoch 499, calculated loss: 0.13838941320395895\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "nn.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model loss\n",
    "\n",
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 94%\n",
      "Test accuracy is 68%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn.accuracy(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {}%\".format(nn.accuracy(ytest, test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Banknote Authentication Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe\n",
    "\n",
    "banknote_df = pd.read_csv('data_banknote_authentication.txt', sep = ',', names = ['variance', 'skewness', 'kurtosis', 'entropy', 'classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "banknote_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance          0\n",
       "skewness          0\n",
       "kurtosis          0\n",
       "entropy           0\n",
       "classification    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "banknote_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance          float64\n",
       "skewness          float64\n",
       "kurtosis          float64\n",
       "entropy           float64\n",
       "classification      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "banknote_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X = np.array(banknote_df.drop(columns = ['classification']))\n",
    "\n",
    "y_label = banknote_df['classification'].values.reshape(X.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new neural network model with layers corresponding to input dimensions of 4\n",
    "\n",
    "nn2 = NeuralNet(layers=[4, 5, 1], epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, calculated loss: 3.8625017265011303\n",
      "Training epoch 1, calculated loss: 11.670013483237073\n",
      "Training epoch 2, calculated loss: 7.9218480979297325\n",
      "Training epoch 3, calculated loss: 1.3376990520810872\n",
      "Training epoch 4, calculated loss: 1.185604916421076\n",
      "Training epoch 5, calculated loss: 2.716737949651961\n",
      "Training epoch 6, calculated loss: 1.5912730656731606\n",
      "Training epoch 7, calculated loss: 0.6997145578412753\n",
      "Training epoch 8, calculated loss: 0.2310135510007064\n",
      "Training epoch 9, calculated loss: 0.15066663624774287\n",
      "Training epoch 10, calculated loss: 0.12133747211933064\n",
      "Training epoch 11, calculated loss: 0.11123391870562074\n",
      "Training epoch 12, calculated loss: 0.10537085218392667\n",
      "Training epoch 13, calculated loss: 0.10025463044055273\n",
      "Training epoch 14, calculated loss: 0.09571135721160844\n",
      "Training epoch 15, calculated loss: 0.09146951998376135\n",
      "Training epoch 16, calculated loss: 0.08764151774536387\n",
      "Training epoch 17, calculated loss: 0.08409170454188078\n",
      "Training epoch 18, calculated loss: 0.08073019341699256\n",
      "Training epoch 19, calculated loss: 0.07764446872030316\n",
      "Training epoch 20, calculated loss: 0.07474525034455946\n",
      "Training epoch 21, calculated loss: 0.07198214123443537\n",
      "Training epoch 22, calculated loss: 0.06938683319927862\n",
      "Training epoch 23, calculated loss: 0.06695833237010505\n",
      "Training epoch 24, calculated loss: 0.06466745628317416\n",
      "Training epoch 25, calculated loss: 0.06248028566413134\n",
      "Training epoch 26, calculated loss: 0.060447785273257976\n",
      "Training epoch 27, calculated loss: 0.058490257506068864\n",
      "Training epoch 28, calculated loss: 0.05665206783907129\n",
      "Training epoch 29, calculated loss: 0.054879913692370004\n",
      "Training epoch 30, calculated loss: 0.053195770650328045\n",
      "Training epoch 31, calculated loss: 0.05158242873584255\n",
      "Training epoch 32, calculated loss: 0.05004899264893665\n",
      "Training epoch 33, calculated loss: 0.04858709325561621\n",
      "Training epoch 34, calculated loss: 0.04719723497138675\n",
      "Training epoch 35, calculated loss: 0.04585087336281223\n",
      "Training epoch 36, calculated loss: 0.04457156644074771\n",
      "Training epoch 37, calculated loss: 0.04335366248549646\n",
      "Training epoch 38, calculated loss: 0.04218603055258175\n",
      "Training epoch 39, calculated loss: 0.04107405764836594\n",
      "Training epoch 40, calculated loss: 0.04000958100872093\n",
      "Training epoch 41, calculated loss: 0.038983130216635714\n",
      "Training epoch 42, calculated loss: 0.038001526504160936\n",
      "Training epoch 43, calculated loss: 0.03705610719302189\n",
      "Training epoch 44, calculated loss: 0.036103061250598575\n",
      "Training epoch 45, calculated loss: 0.03518039159276871\n",
      "Training epoch 46, calculated loss: 0.03429022212789166\n",
      "Training epoch 47, calculated loss: 0.033436273310029685\n",
      "Training epoch 48, calculated loss: 0.03261942823740642\n",
      "Training epoch 49, calculated loss: 0.031835049424115955\n",
      "Training epoch 50, calculated loss: 0.031082295223865897\n",
      "Training epoch 51, calculated loss: 0.030354140691880048\n",
      "Training epoch 52, calculated loss: 0.029658113243004512\n",
      "Training epoch 53, calculated loss: 0.028997480208455592\n",
      "Training epoch 54, calculated loss: 0.02836141809406987\n",
      "Training epoch 55, calculated loss: 0.027746282018902654\n",
      "Training epoch 56, calculated loss: 0.027157159108200452\n",
      "Training epoch 57, calculated loss: 0.026589420421969098\n",
      "Training epoch 58, calculated loss: 0.02604295004569911\n",
      "Training epoch 59, calculated loss: 0.025508554057078626\n",
      "Training epoch 60, calculated loss: 0.024982136442495265\n",
      "Training epoch 61, calculated loss: 0.02447511892856326\n",
      "Training epoch 62, calculated loss: 0.023988134193978106\n",
      "Training epoch 63, calculated loss: 0.023518038490767168\n",
      "Training epoch 64, calculated loss: 0.023063908019326188\n",
      "Training epoch 65, calculated loss: 0.022624786186949387\n",
      "Training epoch 66, calculated loss: 0.02219984872174476\n",
      "Training epoch 67, calculated loss: 0.021789177311011442\n",
      "Training epoch 68, calculated loss: 0.02139022801406251\n",
      "Training epoch 69, calculated loss: 0.021002479651499584\n",
      "Training epoch 70, calculated loss: 0.020627521198770635\n",
      "Training epoch 71, calculated loss: 0.02026442932266399\n",
      "Training epoch 72, calculated loss: 0.01991285018119068\n",
      "Training epoch 73, calculated loss: 0.019571829355501457\n",
      "Training epoch 74, calculated loss: 0.019241557118426306\n",
      "Training epoch 75, calculated loss: 0.01892130422429101\n",
      "Training epoch 76, calculated loss: 0.01861048105203225\n",
      "Training epoch 77, calculated loss: 0.01830946467144663\n",
      "Training epoch 78, calculated loss: 0.018017277362998844\n",
      "Training epoch 79, calculated loss: 0.017733581159329898\n",
      "Training epoch 80, calculated loss: 0.017458108236582486\n",
      "Training epoch 81, calculated loss: 0.01718951392964766\n",
      "Training epoch 82, calculated loss: 0.016928625449480556\n",
      "Training epoch 83, calculated loss: 0.016675405525757865\n",
      "Training epoch 84, calculated loss: 0.016429086189926223\n",
      "Training epoch 85, calculated loss: 0.01618959116809155\n",
      "Training epoch 86, calculated loss: 0.01595676142601701\n",
      "Training epoch 87, calculated loss: 0.015729952159804668\n",
      "Training epoch 88, calculated loss: 0.015509170786799671\n",
      "Training epoch 89, calculated loss: 0.015294336819826596\n",
      "Training epoch 90, calculated loss: 0.015084837656319143\n",
      "Training epoch 91, calculated loss: 0.01488068295331584\n",
      "Training epoch 92, calculated loss: 0.01468188662457646\n",
      "Training epoch 93, calculated loss: 0.014487836965727337\n",
      "Training epoch 94, calculated loss: 0.014298556841972902\n",
      "Training epoch 95, calculated loss: 0.014114033257216515\n",
      "Training epoch 96, calculated loss: 0.013933862330291952\n",
      "Training epoch 97, calculated loss: 0.013757952660818827\n",
      "Training epoch 98, calculated loss: 0.013586311462776984\n",
      "Training epoch 99, calculated loss: 0.013418591686161743\n",
      "Training epoch 100, calculated loss: 0.013254655120549677\n",
      "Training epoch 101, calculated loss: 0.013094241226158992\n",
      "Training epoch 102, calculated loss: 0.012935859383418662\n",
      "Training epoch 103, calculated loss: 0.012780974406639294\n",
      "Training epoch 104, calculated loss: 0.012629598004797838\n",
      "Training epoch 105, calculated loss: 0.012481743464966721\n",
      "Training epoch 106, calculated loss: 0.01233706292382228\n",
      "Training epoch 107, calculated loss: 0.012195535774905467\n",
      "Training epoch 108, calculated loss: 0.012057150322344378\n",
      "Training epoch 109, calculated loss: 0.011921752664777286\n",
      "Training epoch 110, calculated loss: 0.011789163542841359\n",
      "Training epoch 111, calculated loss: 0.011659378308228436\n",
      "Training epoch 112, calculated loss: 0.011532175232946262\n",
      "Training epoch 113, calculated loss: 0.011406470947786544\n",
      "Training epoch 114, calculated loss: 0.011284146901674778\n",
      "Training epoch 115, calculated loss: 0.011164460830931193\n",
      "Training epoch 116, calculated loss: 0.011047202711396193\n",
      "Training epoch 117, calculated loss: 0.010932282612650786\n",
      "Training epoch 118, calculated loss: 0.01081969212999524\n",
      "Training epoch 119, calculated loss: 0.010709419476841413\n",
      "Training epoch 120, calculated loss: 0.010601346677047704\n",
      "Training epoch 121, calculated loss: 0.010495470442750773\n",
      "Training epoch 122, calculated loss: 0.010391744245060352\n",
      "Training epoch 123, calculated loss: 0.010289981406125982\n",
      "Training epoch 124, calculated loss: 0.010190133540217984\n",
      "Training epoch 125, calculated loss: 0.010092173643462144\n",
      "Training epoch 126, calculated loss: 0.009996102998083733\n",
      "Training epoch 127, calculated loss: 0.009901715582491664\n",
      "Training epoch 128, calculated loss: 0.009809048980876603\n",
      "Training epoch 129, calculated loss: 0.009718111537217033\n",
      "Training epoch 130, calculated loss: 0.009628785195316165\n",
      "Training epoch 131, calculated loss: 0.00954100641514836\n",
      "Training epoch 132, calculated loss: 0.009454772455100392\n",
      "Training epoch 133, calculated loss: 0.009370125087440949\n",
      "Training epoch 134, calculated loss: 0.00928685776643711\n",
      "Training epoch 135, calculated loss: 0.00920501919657471\n",
      "Training epoch 136, calculated loss: 0.009124608300679878\n",
      "Training epoch 137, calculated loss: 0.00904551596088548\n",
      "Training epoch 138, calculated loss: 0.008967666126322535\n",
      "Training epoch 139, calculated loss: 0.008891110457670657\n",
      "Training epoch 140, calculated loss: 0.008815886412385858\n",
      "Training epoch 141, calculated loss: 0.008741831073211217\n",
      "Training epoch 142, calculated loss: 0.008668968434706504\n",
      "Training epoch 143, calculated loss: 0.008597303466669716\n",
      "Training epoch 144, calculated loss: 0.008526630374527857\n",
      "Training epoch 145, calculated loss: 0.00845658711761369\n",
      "Training epoch 146, calculated loss: 0.008387666059875643\n",
      "Training epoch 147, calculated loss: 0.008319891633441124\n",
      "Training epoch 148, calculated loss: 0.008253149343404757\n",
      "Training epoch 149, calculated loss: 0.008187432705803917\n",
      "Training epoch 150, calculated loss: 0.008122744054130316\n",
      "Training epoch 151, calculated loss: 0.008059096118254354\n",
      "Training epoch 152, calculated loss: 0.007996363605419785\n",
      "Training epoch 153, calculated loss: 0.007934580980792463\n",
      "Training epoch 154, calculated loss: 0.007873757549538946\n",
      "Training epoch 155, calculated loss: 0.007813840746750835\n",
      "Training epoch 156, calculated loss: 0.007754772582793189\n",
      "Training epoch 157, calculated loss: 0.00769656114751518\n",
      "Training epoch 158, calculated loss: 0.00763926809090788\n",
      "Training epoch 159, calculated loss: 0.007582749882393998\n",
      "Training epoch 160, calculated loss: 0.00752704228010497\n",
      "Training epoch 161, calculated loss: 0.007472136360320394\n",
      "Training epoch 162, calculated loss: 0.0074180409219194695\n",
      "Training epoch 163, calculated loss: 0.0073646703593021065\n",
      "Training epoch 164, calculated loss: 0.007312032456710445\n",
      "Training epoch 165, calculated loss: 0.007260293395243072\n",
      "Training epoch 166, calculated loss: 0.0072092880368220865\n",
      "Training epoch 167, calculated loss: 0.007158788239007727\n",
      "Training epoch 168, calculated loss: 0.0071089999969268305\n",
      "Training epoch 169, calculated loss: 0.007060136647488998\n",
      "Training epoch 170, calculated loss: 0.007011550305545944\n",
      "Training epoch 171, calculated loss: 0.006963860604220352\n",
      "Training epoch 172, calculated loss: 0.00691672468428598\n",
      "Training epoch 173, calculated loss: 0.006870216802917115\n",
      "Training epoch 174, calculated loss: 0.006824385313360731\n",
      "Training epoch 175, calculated loss: 0.0067790542674195445\n",
      "Training epoch 176, calculated loss: 0.00673437072539024\n",
      "Training epoch 177, calculated loss: 0.006690323670289527\n",
      "Training epoch 178, calculated loss: 0.006646651421603564\n",
      "Training epoch 179, calculated loss: 0.006603805910746703\n",
      "Training epoch 180, calculated loss: 0.006561237903898346\n",
      "Training epoch 181, calculated loss: 0.006519375186625429\n",
      "Training epoch 182, calculated loss: 0.006478053909802137\n",
      "Training epoch 183, calculated loss: 0.0064370909807912645\n",
      "Training epoch 184, calculated loss: 0.006396632936300833\n",
      "Training epoch 185, calculated loss: 0.006356527016543044\n",
      "Training epoch 186, calculated loss: 0.006316898034667536\n",
      "Training epoch 187, calculated loss: 0.006277668334777653\n",
      "Training epoch 188, calculated loss: 0.006238986432757584\n",
      "Training epoch 189, calculated loss: 0.006200767765833777\n",
      "Training epoch 190, calculated loss: 0.006163015964301123\n",
      "Training epoch 191, calculated loss: 0.006125722670140103\n",
      "Training epoch 192, calculated loss: 0.006088911644092946\n",
      "Training epoch 193, calculated loss: 0.006052502546402619\n",
      "Training epoch 194, calculated loss: 0.00601652114353365\n",
      "Training epoch 195, calculated loss: 0.005980962912637819\n",
      "Training epoch 196, calculated loss: 0.005945845079249853\n",
      "Training epoch 197, calculated loss: 0.00591110069925248\n",
      "Training epoch 198, calculated loss: 0.005876755502067531\n",
      "Training epoch 199, calculated loss: 0.005842807421444797\n",
      "Training epoch 200, calculated loss: 0.005809263327345706\n",
      "Training epoch 201, calculated loss: 0.005776069703539988\n",
      "Training epoch 202, calculated loss: 0.005743248109997052\n",
      "Training epoch 203, calculated loss: 0.005710802142599573\n",
      "Training epoch 204, calculated loss: 0.005678727551982094\n",
      "Training epoch 205, calculated loss: 0.005646982614821442\n",
      "Training epoch 206, calculated loss: 0.005615588271463112\n",
      "Training epoch 207, calculated loss: 0.005584543608288612\n",
      "Training epoch 208, calculated loss: 0.005553843815544225\n",
      "Training epoch 209, calculated loss: 0.005523458059778094\n",
      "Training epoch 210, calculated loss: 0.005493394931134048\n",
      "Training epoch 211, calculated loss: 0.005463664330093355\n",
      "Training epoch 212, calculated loss: 0.005434256638871092\n",
      "Training epoch 213, calculated loss: 0.005405138412986893\n",
      "Training epoch 214, calculated loss: 0.005376329438901857\n",
      "Training epoch 215, calculated loss: 0.005347829411500166\n",
      "Training epoch 216, calculated loss: 0.005319587563245903\n",
      "Training epoch 217, calculated loss: 0.005291627233502662\n",
      "Training epoch 218, calculated loss: 0.005263952811283107\n",
      "Training epoch 219, calculated loss: 0.0052365779986905505\n",
      "Training epoch 220, calculated loss: 0.0052094813491388595\n",
      "Training epoch 221, calculated loss: 0.00518264688068027\n",
      "Training epoch 222, calculated loss: 0.00515608253028438\n",
      "Training epoch 223, calculated loss: 0.005129805051080985\n",
      "Training epoch 224, calculated loss: 0.0051037745892112945\n",
      "Training epoch 225, calculated loss: 0.005077954853861012\n",
      "Training epoch 226, calculated loss: 0.005052324080894849\n",
      "Training epoch 227, calculated loss: 0.005026968661094001\n",
      "Training epoch 228, calculated loss: 0.005001852438977243\n",
      "Training epoch 229, calculated loss: 0.004976978036256941\n",
      "Training epoch 230, calculated loss: 0.004952347929498236\n",
      "Training epoch 231, calculated loss: 0.00492797818840612\n",
      "Training epoch 232, calculated loss: 0.0049038296664125815\n",
      "Training epoch 233, calculated loss: 0.0048799107555260205\n",
      "Training epoch 234, calculated loss: 0.0048562220880356485\n",
      "Training epoch 235, calculated loss: 0.004832781147587814\n",
      "Training epoch 236, calculated loss: 0.004809544834965773\n",
      "Training epoch 237, calculated loss: 0.004786533915169121\n",
      "Training epoch 238, calculated loss: 0.004763744351306298\n",
      "Training epoch 239, calculated loss: 0.004741191328331859\n",
      "Training epoch 240, calculated loss: 0.004718827844653031\n",
      "Training epoch 241, calculated loss: 0.004696672794919415\n",
      "Training epoch 242, calculated loss: 0.004674724766450545\n",
      "Training epoch 243, calculated loss: 0.004652997832868997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 244, calculated loss: 0.0046314507452420465\n",
      "Training epoch 245, calculated loss: 0.004610100759833768\n",
      "Training epoch 246, calculated loss: 0.004588955260026295\n",
      "Training epoch 247, calculated loss: 0.004568069222644431\n",
      "Training epoch 248, calculated loss: 0.004547299560394807\n",
      "Training epoch 249, calculated loss: 0.004526766102449453\n",
      "Training epoch 250, calculated loss: 0.004506368137479146\n",
      "Training epoch 251, calculated loss: 0.004486212382237425\n",
      "Training epoch 252, calculated loss: 0.004466168796161097\n",
      "Training epoch 253, calculated loss: 0.004446346323098141\n",
      "Training epoch 254, calculated loss: 0.004426648655148747\n",
      "Training epoch 255, calculated loss: 0.0044071893143515315\n",
      "Training epoch 256, calculated loss: 0.004387836313688504\n",
      "Training epoch 257, calculated loss: 0.004368677066828967\n",
      "Training epoch 258, calculated loss: 0.004349681210911686\n",
      "Training epoch 259, calculated loss: 0.004330832815628631\n",
      "Training epoch 260, calculated loss: 0.004312149301395976\n",
      "Training epoch 261, calculated loss: 0.0042936285180037925\n",
      "Training epoch 262, calculated loss: 0.004275251500622204\n",
      "Training epoch 263, calculated loss: 0.004257034267729832\n",
      "Training epoch 264, calculated loss: 0.004238987051772273\n",
      "Training epoch 265, calculated loss: 0.004221057641060806\n",
      "Training epoch 266, calculated loss: 0.004203288590231639\n",
      "Training epoch 267, calculated loss: 0.004185693429075573\n",
      "Training epoch 268, calculated loss: 0.004168209108441818\n",
      "Training epoch 269, calculated loss: 0.004150863275029387\n",
      "Training epoch 270, calculated loss: 0.004133696194856671\n",
      "Training epoch 271, calculated loss: 0.004116647970474023\n",
      "Training epoch 272, calculated loss: 0.0040997210921539275\n",
      "Training epoch 273, calculated loss: 0.004082947405965928\n",
      "Training epoch 274, calculated loss: 0.004066327999729533\n",
      "Training epoch 275, calculated loss: 0.004049805339975863\n",
      "Training epoch 276, calculated loss: 0.004033426191679936\n",
      "Training epoch 277, calculated loss: 0.004017192323783257\n",
      "Training epoch 278, calculated loss: 0.004001072699788654\n",
      "Training epoch 279, calculated loss: 0.003985073442407793\n",
      "Training epoch 280, calculated loss: 0.003969218981939323\n",
      "Training epoch 281, calculated loss: 0.003953467855586799\n",
      "Training epoch 282, calculated loss: 0.00393784586945878\n",
      "Training epoch 283, calculated loss: 0.003922360805657231\n",
      "Training epoch 284, calculated loss: 0.003906953136615426\n",
      "Training epoch 285, calculated loss: 0.0038916744389054137\n",
      "Training epoch 286, calculated loss: 0.003876514084583391\n",
      "Training epoch 287, calculated loss: 0.0038614783521183234\n",
      "Training epoch 288, calculated loss: 0.003846537719468345\n",
      "Training epoch 289, calculated loss: 0.0038317243542875332\n",
      "Training epoch 290, calculated loss: 0.0038170116828610324\n",
      "Training epoch 291, calculated loss: 0.0038024239247443584\n",
      "Training epoch 292, calculated loss: 0.0037879321782104593\n",
      "Training epoch 293, calculated loss: 0.003773564923464158\n",
      "Training epoch 294, calculated loss: 0.003759305350797802\n",
      "Training epoch 295, calculated loss: 0.003745132861997369\n",
      "Training epoch 296, calculated loss: 0.003731071925615327\n",
      "Training epoch 297, calculated loss: 0.0037171209745189985\n",
      "Training epoch 298, calculated loss: 0.003703279919659136\n",
      "Training epoch 299, calculated loss: 0.003689520229601016\n",
      "Training epoch 300, calculated loss: 0.003675871756081051\n",
      "Training epoch 301, calculated loss: 0.0036623208593968707\n",
      "Training epoch 302, calculated loss: 0.0036488783719204818\n",
      "Training epoch 303, calculated loss: 0.0036355146312182134\n",
      "Training epoch 304, calculated loss: 0.0036222597492597576\n",
      "Training epoch 305, calculated loss: 0.003609094160376476\n",
      "Training epoch 306, calculated loss: 0.0035960316607239347\n",
      "Training epoch 307, calculated loss: 0.0035830509680456245\n",
      "Training epoch 308, calculated loss: 0.0035701656794139562\n",
      "Training epoch 309, calculated loss: 0.003557382209879375\n",
      "Training epoch 310, calculated loss: 0.003544687285435272\n",
      "Training epoch 311, calculated loss: 0.003532073595788271\n",
      "Training epoch 312, calculated loss: 0.0035195495596116247\n",
      "Training epoch 313, calculated loss: 0.0035071169465417454\n",
      "Training epoch 314, calculated loss: 0.0034947720741195765\n",
      "Training epoch 315, calculated loss: 0.003482523201534946\n",
      "Training epoch 316, calculated loss: 0.0034703470474745043\n",
      "Training epoch 317, calculated loss: 0.003458254654128253\n",
      "Training epoch 318, calculated loss: 0.0034462512968414997\n",
      "Training epoch 319, calculated loss: 0.0034343305867417188\n",
      "Training epoch 320, calculated loss: 0.0034224843094931364\n",
      "Training epoch 321, calculated loss: 0.0034107184375235184\n",
      "Training epoch 322, calculated loss: 0.003399043048066029\n",
      "Training epoch 323, calculated loss: 0.0033874371935609412\n",
      "Training epoch 324, calculated loss: 0.003375907473458422\n",
      "Training epoch 325, calculated loss: 0.0033644582557979044\n",
      "Training epoch 326, calculated loss: 0.0033530913143765212\n",
      "Training epoch 327, calculated loss: 0.003341790927434363\n",
      "Training epoch 328, calculated loss: 0.0033305654983434843\n",
      "Training epoch 329, calculated loss: 0.003319422457674905\n",
      "Training epoch 330, calculated loss: 0.003308348633915066\n",
      "Training epoch 331, calculated loss: 0.0032973436191807808\n",
      "Training epoch 332, calculated loss: 0.0032864118239197813\n",
      "Training epoch 333, calculated loss: 0.003275560958024305\n",
      "Training epoch 334, calculated loss: 0.0032647699694800018\n",
      "Training epoch 335, calculated loss: 0.0032540489668743855\n",
      "Training epoch 336, calculated loss: 0.0032434034602694087\n",
      "Training epoch 337, calculated loss: 0.0032328258481762236\n",
      "Training epoch 338, calculated loss: 0.00322231073155083\n",
      "Training epoch 339, calculated loss: 0.003211862955666954\n",
      "Training epoch 340, calculated loss: 0.003201493054463289\n",
      "Training epoch 341, calculated loss: 0.0031911782303439213\n",
      "Training epoch 342, calculated loss: 0.003180928607306512\n",
      "Training epoch 343, calculated loss: 0.003170748754587999\n",
      "Training epoch 344, calculated loss: 0.003160634429495485\n",
      "Training epoch 345, calculated loss: 0.0031505774116738663\n",
      "Training epoch 346, calculated loss: 0.0031405802855847564\n",
      "Training epoch 347, calculated loss: 0.003130641520388176\n",
      "Training epoch 348, calculated loss: 0.003120755985965063\n",
      "Training epoch 349, calculated loss: 0.003110931266232378\n",
      "Training epoch 350, calculated loss: 0.0031011720099603283\n",
      "Training epoch 351, calculated loss: 0.0030914756102018177\n",
      "Training epoch 352, calculated loss: 0.0030818326992755013\n",
      "Training epoch 353, calculated loss: 0.0030722492832910083\n",
      "Training epoch 354, calculated loss: 0.003062734118900128\n",
      "Training epoch 355, calculated loss: 0.0030532692482256023\n",
      "Training epoch 356, calculated loss: 0.003043860968866627\n",
      "Training epoch 357, calculated loss: 0.0030345138933409624\n",
      "Training epoch 358, calculated loss: 0.00302534430300953\n",
      "Training epoch 359, calculated loss: 0.003016303887774095\n",
      "Training epoch 360, calculated loss: 0.003007317181986988\n",
      "Training epoch 361, calculated loss: 0.002998392595158398\n",
      "Training epoch 362, calculated loss: 0.0029895131516289044\n",
      "Training epoch 363, calculated loss: 0.0029806846874228144\n",
      "Training epoch 364, calculated loss: 0.0029719119517868176\n",
      "Training epoch 365, calculated loss: 0.0029631928322775792\n",
      "Training epoch 366, calculated loss: 0.0029545184599962877\n",
      "Training epoch 367, calculated loss: 0.002945894568298072\n",
      "Training epoch 368, calculated loss: 0.0029373298037199355\n",
      "Training epoch 369, calculated loss: 0.002928806320759397\n",
      "Training epoch 370, calculated loss: 0.002920331216537192\n",
      "Training epoch 371, calculated loss: 0.0029119094289581696\n",
      "Training epoch 372, calculated loss: 0.002903536567493801\n",
      "Training epoch 373, calculated loss: 0.002895206428548133\n",
      "Training epoch 374, calculated loss: 0.002886924113565522\n",
      "Training epoch 375, calculated loss: 0.002878697872996864\n",
      "Training epoch 376, calculated loss: 0.00287051852632629\n",
      "Training epoch 377, calculated loss: 0.0028623787462394483\n",
      "Training epoch 378, calculated loss: 0.0028542878064062183\n",
      "Training epoch 379, calculated loss: 0.002846240213042363\n",
      "Training epoch 380, calculated loss: 0.002838234122669948\n",
      "Training epoch 381, calculated loss: 0.002830285965626206\n",
      "Training epoch 382, calculated loss: 0.0028223778963685233\n",
      "Training epoch 383, calculated loss: 0.0028145048437326794\n",
      "Training epoch 384, calculated loss: 0.0028066754577431726\n",
      "Training epoch 385, calculated loss: 0.0027989003662422535\n",
      "Training epoch 386, calculated loss: 0.0027911685284513744\n",
      "Training epoch 387, calculated loss: 0.00278346779963636\n",
      "Training epoch 388, calculated loss: 0.0027758128917030377\n",
      "Training epoch 389, calculated loss: 0.0027682017046925123\n",
      "Training epoch 390, calculated loss: 0.002760638895679298\n",
      "Training epoch 391, calculated loss: 0.0027531062902309703\n",
      "Training epoch 392, calculated loss: 0.0027456219358530484\n",
      "Training epoch 393, calculated loss: 0.0027381693342066167\n",
      "Training epoch 394, calculated loss: 0.0027307668071660375\n",
      "Training epoch 395, calculated loss: 0.0027234036357667215\n",
      "Training epoch 396, calculated loss: 0.0027160738084124458\n",
      "Training epoch 397, calculated loss: 0.00270877996986485\n",
      "Training epoch 398, calculated loss: 0.002701536909877968\n",
      "Training epoch 399, calculated loss: 0.0026943313810631076\n",
      "Training epoch 400, calculated loss: 0.002687153243888341\n",
      "Training epoch 401, calculated loss: 0.002680013141745974\n",
      "Training epoch 402, calculated loss: 0.002672929761564461\n",
      "Training epoch 403, calculated loss: 0.002665868415385794\n",
      "Training epoch 404, calculated loss: 0.002658840382046159\n",
      "Training epoch 405, calculated loss: 0.002651854555446729\n",
      "Training epoch 406, calculated loss: 0.002644917400189938\n",
      "Training epoch 407, calculated loss: 0.0026379988107841108\n",
      "Training epoch 408, calculated loss: 0.0026311174323717683\n",
      "Training epoch 409, calculated loss: 0.0026242843722793937\n",
      "Training epoch 410, calculated loss: 0.0026174807085720805\n",
      "Training epoch 411, calculated loss: 0.0026107041897249304\n",
      "Training epoch 412, calculated loss: 0.002603968952390342\n",
      "Training epoch 413, calculated loss: 0.002597277565919012\n",
      "Training epoch 414, calculated loss: 0.0025906056628620293\n",
      "Training epoch 415, calculated loss: 0.002583970447892755\n",
      "Training epoch 416, calculated loss: 0.0025773774885774928\n",
      "Training epoch 417, calculated loss: 0.0025708134330765897\n",
      "Training epoch 418, calculated loss: 0.002564276003862657\n",
      "Training epoch 419, calculated loss: 0.002557780396349578\n",
      "Training epoch 420, calculated loss: 0.0025513205210100087\n",
      "Training epoch 421, calculated loss: 0.0025448804590251914\n",
      "Training epoch 422, calculated loss: 0.002538478677305488\n",
      "Training epoch 423, calculated loss: 0.0025321188190887516\n",
      "Training epoch 424, calculated loss: 0.0025257761443633507\n",
      "Training epoch 425, calculated loss: 0.002519467116524647\n",
      "Training epoch 426, calculated loss: 0.002513201431191352\n",
      "Training epoch 427, calculated loss: 0.0025069556850360847\n",
      "Training epoch 428, calculated loss: 0.0025007373940253578\n",
      "Training epoch 429, calculated loss: 0.0024945647725465106\n",
      "Training epoch 430, calculated loss: 0.0024884127785951933\n",
      "Training epoch 431, calculated loss: 0.00248228469947396\n",
      "Training epoch 432, calculated loss: 0.002476201142018268\n",
      "Training epoch 433, calculated loss: 0.002470141288417754\n",
      "Training epoch 434, calculated loss: 0.002464102358917549\n",
      "Training epoch 435, calculated loss: 0.0024581051171849062\n",
      "Training epoch 436, calculated loss: 0.0024521353001270957\n",
      "Training epoch 437, calculated loss: 0.002446183586751748\n",
      "Training epoch 438, calculated loss: 0.002440271690000863\n",
      "Training epoch 439, calculated loss: 0.00243438905057969\n",
      "Training epoch 440, calculated loss: 0.0024285226754842387\n",
      "Training epoch 441, calculated loss: 0.002422695935583261\n",
      "Training epoch 442, calculated loss: 0.002416896162274642\n",
      "Training epoch 443, calculated loss: 0.0024111140624154185\n",
      "Training epoch 444, calculated loss: 0.0024053727797559224\n",
      "Training epoch 445, calculated loss: 0.0023996507099619414\n",
      "Training epoch 446, calculated loss: 0.002393953774980009\n",
      "Training epoch 447, calculated loss: 0.002388294279497521\n",
      "Training epoch 448, calculated loss: 0.0023826486934682934\n",
      "Training epoch 449, calculated loss: 0.002377037977805526\n",
      "Training epoch 450, calculated loss: 0.0023714537079955003\n",
      "Training epoch 451, calculated loss: 0.002365885058112066\n",
      "Training epoch 452, calculated loss: 0.0023603593594945164\n",
      "Training epoch 453, calculated loss: 0.00235484808719801\n",
      "Training epoch 454, calculated loss: 0.002349356576092114\n",
      "Training epoch 455, calculated loss: 0.0023439111636937636\n",
      "Training epoch 456, calculated loss: 0.002338472502719651\n",
      "Training epoch 457, calculated loss: 0.002333062109934417\n",
      "Training epoch 458, calculated loss: 0.0023276850289744772\n",
      "Training epoch 459, calculated loss: 0.0023223221588196917\n",
      "Training epoch 460, calculated loss: 0.002316992521997019\n",
      "Training epoch 461, calculated loss: 0.0023116805418467467\n",
      "Training epoch 462, calculated loss: 0.002306395462286155\n",
      "Training epoch 463, calculated loss: 0.0023011396614225597\n",
      "Training epoch 464, calculated loss: 0.0022958941906256356\n",
      "Training epoch 465, calculated loss: 0.0022906892185789642\n",
      "Training epoch 466, calculated loss: 0.002285497128818713\n",
      "Training epoch 467, calculated loss: 0.0022803249745554782\n",
      "Training epoch 468, calculated loss: 0.002275191405945991\n",
      "Training epoch 469, calculated loss: 0.002270064896937767\n",
      "Training epoch 470, calculated loss: 0.002264970815244466\n",
      "Training epoch 471, calculated loss: 0.002259895406452389\n",
      "Training epoch 472, calculated loss: 0.0022548430942255357\n",
      "Training epoch 473, calculated loss: 0.0022498174925300383\n",
      "Training epoch 474, calculated loss: 0.002244802775056878\n",
      "Training epoch 475, calculated loss: 0.0022398286908701425\n",
      "Training epoch 476, calculated loss: 0.0022348597510054215\n",
      "Training epoch 477, calculated loss: 0.002229917826412184\n",
      "Training epoch 478, calculated loss: 0.002225004343008882\n",
      "Training epoch 479, calculated loss: 0.00222010037372048\n",
      "Training epoch 480, calculated loss: 0.00221523040704323\n",
      "Training epoch 481, calculated loss: 0.0022103706165137907\n",
      "Training epoch 482, calculated loss: 0.0022055431430987173\n",
      "Training epoch 483, calculated loss: 0.0022007254595049863\n",
      "Training epoch 484, calculated loss: 0.0021959349114301307\n",
      "Training epoch 485, calculated loss: 0.0021911675803214705\n",
      "Training epoch 486, calculated loss: 0.0021864111687166362\n",
      "Training epoch 487, calculated loss: 0.0021816871772038973\n",
      "Training epoch 488, calculated loss: 0.0021769735156329535\n",
      "Training epoch 489, calculated loss: 0.002172290376312175\n",
      "Training epoch 490, calculated loss: 0.002167613754447864\n",
      "Training epoch 491, calculated loss: 0.0021629718010292053\n",
      "Training epoch 492, calculated loss: 0.0021583394972474997\n",
      "Training epoch 493, calculated loss: 0.0021537278926917753\n",
      "Training epoch 494, calculated loss: 0.0021491421229243653\n",
      "Training epoch 495, calculated loss: 0.002144566826183989\n",
      "Training epoch 496, calculated loss: 0.0021400184034903584\n",
      "Training epoch 497, calculated loss: 0.0021354796254609973\n",
      "Training epoch 498, calculated loss: 0.0021309754412608592\n",
      "Training epoch 499, calculated loss: 0.002126471249652675\n",
      "Training epoch 500, calculated loss: 0.002122002111267409\n",
      "Training epoch 501, calculated loss: 0.002117541666568374\n",
      "Training epoch 502, calculated loss: 0.002113105821878226\n",
      "Training epoch 503, calculated loss: 0.002108681191958972\n",
      "Training epoch 504, calculated loss: 0.00210428518947187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 505, calculated loss: 0.002099899201243995\n",
      "Training epoch 506, calculated loss: 0.0020955323992591928\n",
      "Training epoch 507, calculated loss: 0.0020911890413125907\n",
      "Training epoch 508, calculated loss: 0.0020868574542448953\n",
      "Training epoch 509, calculated loss: 0.0020825469531432538\n",
      "Training epoch 510, calculated loss: 0.002078251946705064\n",
      "Training epoch 511, calculated loss: 0.0020739800853585622\n",
      "Training epoch 512, calculated loss: 0.0020697157426218912\n",
      "Training epoch 513, calculated loss: 0.0020654800193144634\n",
      "Training epoch 514, calculated loss: 0.002061252570809634\n",
      "Training epoch 515, calculated loss: 0.002057049843966481\n",
      "Training epoch 516, calculated loss: 0.002052854141505929\n",
      "Training epoch 517, calculated loss: 0.0020486900509872753\n",
      "Training epoch 518, calculated loss: 0.0020445267022867163\n",
      "Training epoch 519, calculated loss: 0.0020403925408254498\n",
      "Training epoch 520, calculated loss: 0.002036267571956314\n",
      "Training epoch 521, calculated loss: 0.002032166541673543\n",
      "Training epoch 522, calculated loss: 0.002028069663590601\n",
      "Training epoch 523, calculated loss: 0.0020240054929915553\n",
      "Training epoch 524, calculated loss: 0.0020199424619208274\n",
      "Training epoch 525, calculated loss: 0.0020159060731593363\n",
      "Training epoch 526, calculated loss: 0.0020118781899028226\n",
      "Training epoch 527, calculated loss: 0.002007875028695878\n",
      "Training epoch 528, calculated loss: 0.00200387653378503\n",
      "Training epoch 529, calculated loss: 0.001999904676999214\n",
      "Training epoch 530, calculated loss: 0.0019959413152959215\n",
      "Training epoch 531, calculated loss: 0.0019919974431316404\n",
      "Training epoch 532, calculated loss: 0.001988064971379876\n",
      "Training epoch 533, calculated loss: 0.001984154390356927\n",
      "Training epoch 534, calculated loss: 0.001980252609097586\n",
      "Training epoch 535, calculated loss: 0.0019763681271959092\n",
      "Training epoch 536, calculated loss: 0.0019725025987075767\n",
      "Training epoch 537, calculated loss: 0.001968646273181454\n",
      "Training epoch 538, calculated loss: 0.001964808019350236\n",
      "Training epoch 539, calculated loss: 0.001960984314639241\n",
      "Training epoch 540, calculated loss: 0.0019571778828744025\n",
      "Training epoch 541, calculated loss: 0.0019533773315855685\n",
      "Training epoch 542, calculated loss: 0.001949605478307118\n",
      "Training epoch 543, calculated loss: 0.0019458333323405584\n",
      "Training epoch 544, calculated loss: 0.0019420887328920268\n",
      "Training epoch 545, calculated loss: 0.0019383474626422147\n",
      "Training epoch 546, calculated loss: 0.0019346308480882911\n",
      "Training epoch 547, calculated loss: 0.0019309178716360657\n",
      "Training epoch 548, calculated loss: 0.0019272270776534643\n",
      "Training epoch 549, calculated loss: 0.001923547623950773\n",
      "Training epoch 550, calculated loss: 0.0019198799859391248\n",
      "Training epoch 551, calculated loss: 0.0019162299204411217\n",
      "Training epoch 552, calculated loss: 0.0019125903803280792\n",
      "Training epoch 553, calculated loss: 0.001908969508990515\n",
      "Training epoch 554, calculated loss: 0.0019053521567385361\n",
      "Training epoch 555, calculated loss: 0.0019017645033529724\n",
      "Training epoch 556, calculated loss: 0.0018981743616184813\n",
      "Training epoch 557, calculated loss: 0.001894606178857388\n",
      "Training epoch 558, calculated loss: 0.0018910507354044784\n",
      "Training epoch 559, calculated loss: 0.001887505681523031\n",
      "Training epoch 560, calculated loss: 0.0018839758981687128\n",
      "Training epoch 561, calculated loss: 0.001880457151705332\n",
      "Training epoch 562, calculated loss: 0.0018769579529643104\n",
      "Training epoch 563, calculated loss: 0.0018734600314808455\n",
      "Training epoch 564, calculated loss: 0.0018699883214826562\n",
      "Training epoch 565, calculated loss: 0.0018665199232925634\n",
      "Training epoch 566, calculated loss: 0.0018630665793009876\n",
      "Training epoch 567, calculated loss: 0.0018596288953808481\n",
      "Training epoch 568, calculated loss: 0.001856198591669216\n",
      "Training epoch 569, calculated loss: 0.0018527878629986784\n",
      "Training epoch 570, calculated loss: 0.0018493810513755058\n",
      "Training epoch 571, calculated loss: 0.0018459963932494258\n",
      "Training epoch 572, calculated loss: 0.0018426154713876576\n",
      "Training epoch 573, calculated loss: 0.0018392495774133863\n",
      "Training epoch 574, calculated loss: 0.001835900811712952\n",
      "Training epoch 575, calculated loss: 0.001832553860062587\n",
      "Training epoch 576, calculated loss: 0.0018292306569368095\n",
      "Training epoch 577, calculated loss: 0.0018259120139225303\n",
      "Training epoch 578, calculated loss: 0.0018226070473505048\n",
      "Training epoch 579, calculated loss: 0.0018193138690731487\n",
      "Training epoch 580, calculated loss: 0.0018160316033360472\n",
      "Training epoch 581, calculated loss: 0.0018127668771532814\n",
      "Training epoch 582, calculated loss: 0.0018095034645771435\n",
      "Training epoch 583, calculated loss: 0.0018062619538319357\n",
      "Training epoch 584, calculated loss: 0.0018030267184900141\n",
      "Training epoch 585, calculated loss: 0.0017997990740408417\n",
      "Training epoch 586, calculated loss: 0.0017965951286805205\n",
      "Training epoch 587, calculated loss: 0.0017933903151911808\n",
      "Training epoch 588, calculated loss: 0.0017902020101197367\n",
      "Training epoch 589, calculated loss: 0.0017870264964205806\n",
      "Training epoch 590, calculated loss: 0.0017838562280219467\n",
      "Training epoch 591, calculated loss: 0.0017807059210214603\n",
      "Training epoch 592, calculated loss: 0.001777558892986373\n",
      "Training epoch 593, calculated loss: 0.0017744268488124432\n",
      "Training epoch 594, calculated loss: 0.001771305200514255\n",
      "Training epoch 595, calculated loss: 0.0017681908075515048\n",
      "Training epoch 596, calculated loss: 0.0017650959478113847\n",
      "Training epoch 597, calculated loss: 0.0017620029292714397\n",
      "Training epoch 598, calculated loss: 0.00175892278107113\n",
      "Training epoch 599, calculated loss: 0.00175585984601306\n",
      "Training epoch 600, calculated loss: 0.0017527974544219026\n",
      "Training epoch 601, calculated loss: 0.001749752272616708\n",
      "Training epoch 602, calculated loss: 0.0017467180384471651\n",
      "Training epoch 603, calculated loss: 0.0017436873590495857\n",
      "Training epoch 604, calculated loss: 0.0017406762157317702\n",
      "Training epoch 605, calculated loss: 0.0017376706160259618\n",
      "Training epoch 606, calculated loss: 0.001734672831035136\n",
      "Training epoch 607, calculated loss: 0.0017316914506394661\n",
      "Training epoch 608, calculated loss: 0.0017287160178525982\n",
      "Training epoch 609, calculated loss: 0.0017257513813917695\n",
      "Training epoch 610, calculated loss: 0.0017227976690912085\n",
      "Training epoch 611, calculated loss: 0.0017198528428868287\n",
      "Training epoch 612, calculated loss: 0.0017169205361423114\n",
      "Training epoch 613, calculated loss: 0.001713994718006296\n",
      "Training epoch 614, calculated loss: 0.001711079478203679\n",
      "Training epoch 615, calculated loss: 0.0017081789267655218\n",
      "Training epoch 616, calculated loss: 0.0017052809224642515\n",
      "Training epoch 617, calculated loss: 0.00170239539217483\n",
      "Training epoch 618, calculated loss: 0.0016995246243562637\n",
      "Training epoch 619, calculated loss: 0.0016966548378735847\n",
      "Training epoch 620, calculated loss: 0.0016937992834327104\n",
      "Training epoch 621, calculated loss: 0.0016909562598832906\n",
      "Training epoch 622, calculated loss: 0.0016881152232143281\n",
      "Training epoch 623, calculated loss: 0.001685288881276173\n",
      "Training epoch 624, calculated loss: 0.0016824734943655328\n",
      "Training epoch 625, calculated loss: 0.0016796607733598487\n",
      "Training epoch 626, calculated loss: 0.0016768629061481325\n",
      "Training epoch 627, calculated loss: 0.001674075040462357\n",
      "Training epoch 628, calculated loss: 0.001671290209890548\n",
      "Training epoch 629, calculated loss: 0.0016685201039594432\n",
      "Training epoch 630, calculated loss: 0.00166575963704338\n",
      "Training epoch 631, calculated loss: 0.0016630022803387269\n",
      "Training epoch 632, calculated loss: 0.001660259245462329\n",
      "Training epoch 633, calculated loss: 0.001657526048462171\n",
      "Training epoch 634, calculated loss: 0.0016547957574786561\n",
      "Training epoch 635, calculated loss: 0.0016520791256433285\n",
      "Training epoch 636, calculated loss: 0.0016493730638698126\n",
      "Training epoch 637, calculated loss: 0.0016466694386534902\n",
      "Training epoch 638, calculated loss: 0.0016439785631338717\n",
      "Training epoch 639, calculated loss: 0.0016412994965606692\n",
      "Training epoch 640, calculated loss: 0.001638622145131905\n",
      "Training epoch 641, calculated loss: 0.0016359563996360968\n",
      "Training epoch 642, calculated loss: 0.0016333041833459694\n",
      "Training epoch 643, calculated loss: 0.0016306527214912689\n",
      "Training epoch 644, calculated loss: 0.0016280114993642418\n",
      "Training epoch 645, calculated loss: 0.0016253859839518276\n",
      "Training epoch 646, calculated loss: 0.0016227600350237933\n",
      "Training epoch 647, calculated loss: 0.0016201430255123116\n",
      "Training epoch 648, calculated loss: 0.0016175435069639497\n",
      "Training epoch 649, calculated loss: 0.0016149430433764931\n",
      "Training epoch 650, calculated loss: 0.0016123508668522695\n",
      "Training epoch 651, calculated loss: 0.0016097746766566478\n",
      "Training epoch 652, calculated loss: 0.0016072005851283345\n",
      "Training epoch 653, calculated loss: 0.0016046328961129802\n",
      "Training epoch 654, calculated loss: 0.0016020794681338355\n",
      "Training epoch 655, calculated loss: 0.0015995315927391015\n",
      "Training epoch 656, calculated loss: 0.0015969881123788243\n",
      "Training epoch 657, calculated loss: 0.0015944567786974648\n",
      "Training epoch 658, calculated loss: 0.0015919350177628917\n",
      "Training epoch 659, calculated loss: 0.00158941549888729\n",
      "Training epoch 660, calculated loss: 0.0015869070325496162\n",
      "Training epoch 661, calculated loss: 0.0015844083570993274\n",
      "Training epoch 662, calculated loss: 0.0015819140000754065\n",
      "Training epoch 663, calculated loss: 0.0015794288838343626\n",
      "Training epoch 664, calculated loss: 0.0015769510482195675\n",
      "Training epoch 665, calculated loss: 0.001574482548853959\n",
      "Training epoch 666, calculated loss: 0.0015720204659991337\n",
      "Training epoch 667, calculated loss: 0.0015695637338469268\n",
      "Training epoch 668, calculated loss: 0.0015671194142650529\n",
      "Training epoch 669, calculated loss: 0.001564680855230081\n",
      "Training epoch 670, calculated loss: 0.001562246979125999\n",
      "Training epoch 671, calculated loss: 0.001559822101337452\n",
      "Training epoch 672, calculated loss: 0.0015574090378882622\n",
      "Training epoch 673, calculated loss: 0.00155499768875472\n",
      "Training epoch 674, calculated loss: 0.0015525931437475958\n",
      "Training epoch 675, calculated loss: 0.0015502027486957294\n",
      "Training epoch 676, calculated loss: 0.0015478149862804317\n",
      "Training epoch 677, calculated loss: 0.001545432455455837\n",
      "Training epoch 678, calculated loss: 0.0015430602215215977\n",
      "Training epoch 679, calculated loss: 0.0015406978957778455\n",
      "Training epoch 680, calculated loss: 0.0015383370766804592\n",
      "Training epoch 681, calculated loss: 0.0015359856917119003\n",
      "Training epoch 682, calculated loss: 0.0015336424340624713\n",
      "Training epoch 683, calculated loss: 0.0015313061677561014\n",
      "Training epoch 684, calculated loss: 0.0015289760131292171\n",
      "Training epoch 685, calculated loss: 0.00152665173910046\n",
      "Training epoch 686, calculated loss: 0.00152433704614818\n",
      "Training epoch 687, calculated loss: 0.001522029639481809\n",
      "Training epoch 688, calculated loss: 0.0015197263483152884\n",
      "Training epoch 689, calculated loss: 0.0015174290234448475\n",
      "Training epoch 690, calculated loss: 0.0015151448141229407\n",
      "Training epoch 691, calculated loss: 0.0015128631137357926\n",
      "Training epoch 692, calculated loss: 0.001510586339965636\n",
      "Training epoch 693, calculated loss: 0.0015083183273621207\n",
      "Training epoch 694, calculated loss: 0.0015060607694665242\n",
      "Training epoch 695, calculated loss: 0.0015038046978269794\n",
      "Training epoch 696, calculated loss: 0.0015015568159106065\n",
      "Training epoch 697, calculated loss: 0.0014993148481797375\n",
      "Training epoch 698, calculated loss: 0.0014970829493024722\n",
      "Training epoch 699, calculated loss: 0.00149485526433858\n",
      "Training epoch 700, calculated loss: 0.0014926331437494927\n",
      "Training epoch 701, calculated loss: 0.0014904164986581229\n",
      "Training epoch 702, calculated loss: 0.0014882122789977557\n",
      "Training epoch 703, calculated loss: 0.0014860103310735363\n",
      "Training epoch 704, calculated loss: 0.001483813159117714\n",
      "Training epoch 705, calculated loss: 0.001481624232166733\n",
      "Training epoch 706, calculated loss: 0.0014794444004198732\n",
      "Training epoch 707, calculated loss: 0.0014772676713770243\n",
      "Training epoch 708, calculated loss: 0.0014750978584350248\n",
      "Training epoch 709, calculated loss: 0.001472933759595076\n",
      "Training epoch 710, calculated loss: 0.0014707771053234643\n",
      "Training epoch 711, calculated loss: 0.001468628334018358\n",
      "Training epoch 712, calculated loss: 0.001466483021243015\n",
      "Training epoch 713, calculated loss: 0.0014643428485906548\n",
      "Training epoch 714, calculated loss: 0.0014622114942077274\n",
      "Training epoch 715, calculated loss: 0.0014600881376860015\n",
      "Training epoch 716, calculated loss: 0.0014579664353509762\n",
      "Training epoch 717, calculated loss: 0.0014558526287263525\n",
      "Training epoch 718, calculated loss: 0.0014537442224699278\n",
      "Training epoch 719, calculated loss: 0.001451644088724042\n",
      "Training epoch 720, calculated loss: 0.0014495493067696408\n",
      "Training epoch 721, calculated loss: 0.0014474589439489097\n",
      "Training epoch 722, calculated loss: 0.0014453735663008892\n",
      "Training epoch 723, calculated loss: 0.0014432970577559618\n",
      "Training epoch 724, calculated loss: 0.0014412273778184133\n",
      "Training epoch 725, calculated loss: 0.001439159762753165\n",
      "Training epoch 726, calculated loss: 0.0014370998641062317\n",
      "Training epoch 727, calculated loss: 0.001435044954623957\n",
      "Training epoch 728, calculated loss: 0.0014329974748572712\n",
      "Training epoch 729, calculated loss: 0.0014309563842367\n",
      "Training epoch 730, calculated loss: 0.0014289188127469198\n",
      "Training epoch 731, calculated loss: 0.0014268861377902706\n",
      "Training epoch 732, calculated loss: 0.0014248610954956593\n",
      "Training epoch 733, calculated loss: 0.0014228440553608122\n",
      "Training epoch 734, calculated loss: 0.0014208288047366988\n",
      "Training epoch 735, calculated loss: 0.001418820820539407\n",
      "Training epoch 736, calculated loss: 0.0014168173266837839\n",
      "Training epoch 737, calculated loss: 0.001414818873715594\n",
      "Training epoch 738, calculated loss: 0.0014128311460220263\n",
      "Training epoch 739, calculated loss: 0.001410844319514521\n",
      "Training epoch 740, calculated loss: 0.0014088623610724747\n",
      "Training epoch 741, calculated loss: 0.0014068879648500444\n",
      "Training epoch 742, calculated loss: 0.0014049178569208753\n",
      "Training epoch 743, calculated loss: 0.0014029555926577196\n",
      "Training epoch 744, calculated loss: 0.0014009976394188715\n",
      "Training epoch 745, calculated loss: 0.0013990435779235224\n",
      "Training epoch 746, calculated loss: 0.0013970944045848717\n",
      "Training epoch 747, calculated loss: 0.0013951527796569213\n",
      "Training epoch 748, calculated loss: 0.0013932177764297857\n",
      "Training epoch 749, calculated loss: 0.0013912849475856159\n",
      "Training epoch 750, calculated loss: 0.0013893593670004922\n",
      "Training epoch 751, calculated loss: 0.00138743723935473\n",
      "Training epoch 752, calculated loss: 0.001385520037803102\n",
      "Training epoch 753, calculated loss: 0.0013836122556186016\n",
      "Training epoch 754, calculated loss: 0.0013817070096510372\n",
      "Training epoch 755, calculated loss: 0.0013798054585674967\n",
      "Training epoch 756, calculated loss: 0.001377911476232475\n",
      "Training epoch 757, calculated loss: 0.001376020473507048\n",
      "Training epoch 758, calculated loss: 0.0013741338933576355\n",
      "Training epoch 759, calculated loss: 0.0013722550773708913\n",
      "Training epoch 760, calculated loss: 0.0013703763958367548\n",
      "Training epoch 761, calculated loss: 0.0013685027943379292\n",
      "Training epoch 762, calculated loss: 0.0013666367210038125\n",
      "Training epoch 763, calculated loss: 0.0013647732511688094\n",
      "Training epoch 764, calculated loss: 0.0013629162993272445\n",
      "Training epoch 765, calculated loss: 0.0013610667191573044\n",
      "Training epoch 766, calculated loss: 0.0013592182761230044\n",
      "Training epoch 767, calculated loss: 0.001357375078471514\n",
      "Training epoch 768, calculated loss: 0.001355538818838149\n",
      "Training epoch 769, calculated loss: 0.0013537052915347147\n",
      "Training epoch 770, calculated loss: 0.001351878675649772\n",
      "Training epoch 771, calculated loss: 0.001350058098885998\n",
      "Training epoch 772, calculated loss: 0.0013482392361862576\n",
      "Training epoch 773, calculated loss: 0.0013464257277407835\n",
      "Training epoch 774, calculated loss: 0.0013446185425648467\n",
      "Training epoch 775, calculated loss: 0.001342814236424639\n",
      "Training epoch 776, calculated loss: 0.0013410166461293692\n",
      "Training epoch 777, calculated loss: 0.0013392249615981808\n",
      "Training epoch 778, calculated loss: 0.0013374349696054083\n",
      "Training epoch 779, calculated loss: 0.0013356504598478914\n",
      "Training epoch 780, calculated loss: 0.001333871633134058\n",
      "Training epoch 781, calculated loss: 0.0013320958502198792\n",
      "Training epoch 782, calculated loss: 0.0013303260451536063\n",
      "Training epoch 783, calculated loss: 0.0013285631173099706\n",
      "Training epoch 784, calculated loss: 0.001326801309325381\n",
      "Training epoch 785, calculated loss: 0.0013250451306651722\n",
      "Training epoch 786, calculated loss: 0.0013232939687744848\n",
      "Training epoch 787, calculated loss: 0.0013215460335609056\n",
      "Training epoch 788, calculated loss: 0.0013198038066019782\n",
      "Training epoch 789, calculated loss: 0.001318067529564475\n",
      "Training epoch 790, calculated loss: 0.0013163342479622772\n",
      "Training epoch 791, calculated loss: 0.0013146057497361902\n",
      "Training epoch 792, calculated loss: 0.0013128815846441446\n",
      "Training epoch 793, calculated loss: 0.0013111608426761478\n",
      "Training epoch 794, calculated loss: 0.0013094459802536426\n",
      "Training epoch 795, calculated loss: 0.0013077350939336792\n",
      "Training epoch 796, calculated loss: 0.0013060290743527867\n",
      "Training epoch 797, calculated loss: 0.0013043284293273794\n",
      "Training epoch 798, calculated loss: 0.0013026306133834288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 799, calculated loss: 0.00130093643088106\n",
      "Training epoch 800, calculated loss: 0.001299248314031121\n",
      "Training epoch 801, calculated loss: 0.0012975634574708167\n",
      "Training epoch 802, calculated loss: 0.0012958823514632016\n",
      "Training epoch 803, calculated loss: 0.0012942080732428148\n",
      "Training epoch 804, calculated loss: 0.0012925373090770206\n",
      "Training epoch 805, calculated loss: 0.001290869072079874\n",
      "Training epoch 806, calculated loss: 0.0012892071017473541\n",
      "Training epoch 807, calculated loss: 0.0012875476566187021\n",
      "Training epoch 808, calculated loss: 0.0012858921955581108\n",
      "Training epoch 809, calculated loss: 0.0012842431072689698\n",
      "Training epoch 810, calculated loss: 0.0012825961656643407\n",
      "Training epoch 811, calculated loss: 0.0012809545567812714\n",
      "Training epoch 812, calculated loss: 0.0012793186923304688\n",
      "Training epoch 813, calculated loss: 0.001277684142520034\n",
      "Training epoch 814, calculated loss: 0.001276053995692147\n",
      "Training epoch 815, calculated loss: 0.0012744296729387825\n",
      "Training epoch 816, calculated loss: 0.0012728075118487087\n",
      "Training epoch 817, calculated loss: 0.001271189880821263\n",
      "Training epoch 818, calculated loss: 0.0012695776600682898\n",
      "Training epoch 819, calculated loss: 0.0012679684527559033\n",
      "Training epoch 820, calculated loss: 0.0012663641412741272\n",
      "Training epoch 821, calculated loss: 0.0012647638394521286\n",
      "Training epoch 822, calculated loss: 0.001263165983529368\n",
      "Training epoch 823, calculated loss: 0.0012615729207329993\n",
      "Training epoch 824, calculated loss: 0.0012599844367682618\n",
      "Training epoch 825, calculated loss: 0.0012583985523311786\n",
      "Training epoch 826, calculated loss: 0.0012568175960742819\n",
      "Training epoch 827, calculated loss: 0.0012552407960627022\n",
      "Training epoch 828, calculated loss: 0.0012536676956939785\n",
      "Training epoch 829, calculated loss: 0.001252099259856173\n",
      "Training epoch 830, calculated loss: 0.0012505339696324775\n",
      "Training epoch 831, calculated loss: 0.0012489715780421944\n",
      "Training epoch 832, calculated loss: 0.001247414393809197\n",
      "Training epoch 833, calculated loss: 0.00124586051568862\n",
      "Training epoch 834, calculated loss: 0.0012443096972000374\n",
      "Training epoch 835, calculated loss: 0.0012427642293077706\n",
      "Training epoch 836, calculated loss: 0.0012412216352850412\n",
      "Training epoch 837, calculated loss: 0.0012396823757250327\n",
      "Training epoch 838, calculated loss: 0.0012381497344731865\n",
      "Training epoch 839, calculated loss: 0.0012366182587075262\n",
      "Training epoch 840, calculated loss: 0.0012350901570253724\n",
      "Training epoch 841, calculated loss: 0.0012335677015415149\n",
      "Training epoch 842, calculated loss: 0.0012320472496845471\n",
      "Training epoch 843, calculated loss: 0.0012305303709592048\n",
      "Training epoch 844, calculated loss: 0.0012290191982630462\n",
      "Training epoch 845, calculated loss: 0.0012275096798927427\n",
      "Training epoch 846, calculated loss: 0.001226004072800473\n",
      "Training epoch 847, calculated loss: 0.0012245037099470359\n",
      "Training epoch 848, calculated loss: 0.0012230059108201507\n",
      "Training epoch 849, calculated loss: 0.0012215119037215832\n",
      "Training epoch 850, calculated loss: 0.0012200221945978893\n",
      "Training epoch 851, calculated loss: 0.0012185344902888146\n",
      "Training epoch 852, calculated loss: 0.0012170510300332836\n",
      "Training epoch 853, calculated loss: 0.001215571884070309\n",
      "Training epoch 854, calculated loss: 0.0012140949292568583\n",
      "Training epoch 855, calculated loss: 0.001212622388599314\n",
      "Training epoch 856, calculated loss: 0.0012111536898550893\n",
      "Training epoch 857, calculated loss: 0.0012096873673366477\n",
      "Training epoch 858, calculated loss: 0.0012082256282122661\n",
      "Training epoch 859, calculated loss: 0.0012067672623469188\n",
      "Training epoch 860, calculated loss: 0.0012053123168534149\n",
      "Training epoch 861, calculated loss: 0.0012038614111895716\n",
      "Training epoch 862, calculated loss: 0.0012024132318252125\n",
      "Training epoch 863, calculated loss: 0.001200967792687397\n",
      "Training epoch 864, calculated loss: 0.0011995272846141679\n",
      "Training epoch 865, calculated loss: 0.0011980892049658928\n",
      "Training epoch 866, calculated loss: 0.001196654058537125\n",
      "Training epoch 867, calculated loss: 0.0011952240208593036\n",
      "Training epoch 868, calculated loss: 0.0011937959314622917\n",
      "Training epoch 869, calculated loss: 0.0011923709672332855\n",
      "Training epoch 870, calculated loss: 0.0011909512887529326\n",
      "Training epoch 871, calculated loss: 0.0011895330816312996\n",
      "Training epoch 872, calculated loss: 0.001188118384066319\n",
      "Training epoch 873, calculated loss: 0.001186708724348888\n",
      "Training epoch 874, calculated loss: 0.001185301168473825\n",
      "Training epoch 875, calculated loss: 0.0011838966279819729\n",
      "Training epoch 876, calculated loss: 0.0011824964745429161\n",
      "Training epoch 877, calculated loss: 0.0011810981001697716\n",
      "Training epoch 878, calculated loss: 0.0011797036522562762\n",
      "Training epoch 879, calculated loss: 0.0011783130816064836\n",
      "Training epoch 880, calculated loss: 0.0011769244993455668\n",
      "Training epoch 881, calculated loss: 0.0011755400384716452\n",
      "Training epoch 882, calculated loss: 0.001174158948209526\n",
      "Training epoch 883, calculated loss: 0.001172780054763054\n",
      "Training epoch 884, calculated loss: 0.0011714054770498707\n",
      "Training epoch 885, calculated loss: 0.0011700337661423618\n",
      "Training epoch 886, calculated loss: 0.001168664459710042\n",
      "Training epoch 887, calculated loss: 0.0011672996628883665\n",
      "Training epoch 888, calculated loss: 0.0011659372316457455\n",
      "Training epoch 889, calculated loss: 0.0011645774118891057\n",
      "Training epoch 890, calculated loss: 0.0011632225563441277\n",
      "Training epoch 891, calculated loss: 0.0011618696673513116\n",
      "Training epoch 892, calculated loss: 0.001160519205035693\n",
      "Training epoch 893, calculated loss: 0.0011591736460525915\n",
      "Training epoch 894, calculated loss: 0.0011578294475461473\n",
      "Training epoch 895, calculated loss: 0.0011564884624396179\n",
      "Training epoch 896, calculated loss: 0.001155152026020363\n",
      "Training epoch 897, calculated loss: 0.001153816994488533\n",
      "Training epoch 898, calculated loss: 0.0011524854479406878\n",
      "Training epoch 899, calculated loss: 0.0011511579194269444\n",
      "Training epoch 900, calculated loss: 0.0011498320197841977\n",
      "Training epoch 901, calculated loss: 0.0011485098160216574\n",
      "Training epoch 902, calculated loss: 0.0011471911020439836\n",
      "Training epoch 903, calculated loss: 0.0011458742403087261\n",
      "Training epoch 904, calculated loss: 0.0011445612849968006\n",
      "Training epoch 905, calculated loss: 0.0011432512934048823\n",
      "Training epoch 906, calculated loss: 0.0011419433769121908\n",
      "Training epoch 907, calculated loss: 0.0011406395771178547\n",
      "Training epoch 908, calculated loss: 0.0011393382169611426\n",
      "Training epoch 909, calculated loss: 0.0011380391543354834\n",
      "Training epoch 910, calculated loss: 0.001136744418493256\n",
      "Training epoch 911, calculated loss: 0.0011354516000025173\n",
      "Training epoch 912, calculated loss: 0.0011341613011321807\n",
      "Training epoch 913, calculated loss: 0.0011328755390129207\n",
      "Training epoch 914, calculated loss: 0.0011315912848291058\n",
      "Training epoch 915, calculated loss: 0.0011303100068600221\n",
      "Training epoch 916, calculated loss: 0.001129032891388791\n",
      "Training epoch 917, calculated loss: 0.0011277569752045434\n",
      "Training epoch 918, calculated loss: 0.0011264842778930819\n",
      "Training epoch 919, calculated loss: 0.0011252154536704612\n",
      "Training epoch 920, calculated loss: 0.0011239480679450264\n",
      "Training epoch 921, calculated loss: 0.001122684128226079\n",
      "Training epoch 922, calculated loss: 0.0011214235099913149\n",
      "Training epoch 923, calculated loss: 0.0011201645689770778\n",
      "Training epoch 924, calculated loss: 0.0011189093008417141\n",
      "Training epoch 925, calculated loss: 0.0011176568044259237\n",
      "Training epoch 926, calculated loss: 0.0011164062235481098\n",
      "Training epoch 927, calculated loss: 0.001115159542230431\n",
      "Training epoch 928, calculated loss: 0.0011139150845056252\n",
      "Training epoch 929, calculated loss: 0.0011126727802657484\n",
      "Training epoch 930, calculated loss: 0.001111434602141383\n",
      "Training epoch 931, calculated loss: 0.0011101981010681033\n",
      "Training epoch 932, calculated loss: 0.0011089639912414003\n",
      "Training epoch 933, calculated loss: 0.0011077342340170288\n",
      "Training epoch 934, calculated loss: 0.0011065056086413387\n",
      "Training epoch 935, calculated loss: 0.0011052798069603523\n",
      "Training epoch 936, calculated loss: 0.0011040579996495802\n",
      "Training epoch 937, calculated loss: 0.0011028373660264866\n",
      "Training epoch 938, calculated loss: 0.001101619858524506\n",
      "Training epoch 939, calculated loss: 0.0011004057862328202\n",
      "Training epoch 940, calculated loss: 0.0010991931329261853\n",
      "Training epoch 941, calculated loss: 0.0010979838402032133\n",
      "Training epoch 942, calculated loss: 0.001096777424714189\n",
      "Training epoch 943, calculated loss: 0.0010955726733797308\n",
      "Training epoch 944, calculated loss: 0.0010943715171345145\n",
      "Training epoch 945, calculated loss: 0.0010931726812188841\n",
      "Training epoch 946, calculated loss: 0.0010919757545474508\n",
      "Training epoch 947, calculated loss: 0.0010907826575558297\n",
      "Training epoch 948, calculated loss: 0.0010895913249574223\n",
      "Training epoch 949, calculated loss: 0.0010884021466573595\n",
      "Training epoch 950, calculated loss: 0.0010872170327491193\n",
      "Training epoch 951, calculated loss: 0.0010860331281591863\n",
      "Training epoch 952, calculated loss: 0.0010848517404814697\n",
      "Training epoch 953, calculated loss: 0.0010836742995475226\n",
      "Training epoch 954, calculated loss: 0.001082497867176668\n",
      "Training epoch 955, calculated loss: 0.0010813243455287651\n",
      "Training epoch 956, calculated loss: 0.0010801542016201087\n",
      "Training epoch 957, calculated loss: 0.00107898531903813\n",
      "Training epoch 958, calculated loss: 0.0010778195899231759\n",
      "Training epoch 959, calculated loss: 0.0010766566702983078\n",
      "Training epoch 960, calculated loss: 0.0010754952647989472\n",
      "Training epoch 961, calculated loss: 0.0010743372556887237\n",
      "Training epoch 962, calculated loss: 0.0010731814885139245\n",
      "Training epoch 963, calculated loss: 0.001072027488331337\n",
      "Training epoch 964, calculated loss: 0.0010708771276664152\n",
      "Training epoch 965, calculated loss: 0.0010697284420024611\n",
      "Training epoch 966, calculated loss: 0.0010685817762956249\n",
      "Training epoch 967, calculated loss: 0.0010674389934679292\n",
      "Training epoch 968, calculated loss: 0.0010662973192569305\n",
      "Training epoch 969, calculated loss: 0.001065158051429069\n",
      "Training epoch 970, calculated loss: 0.0010640225102273738\n",
      "Training epoch 971, calculated loss: 0.0010628879125966475\n",
      "Training epoch 972, calculated loss: 0.0010617561141420457\n",
      "Training epoch 973, calculated loss: 0.001060627462424564\n",
      "Training epoch 974, calculated loss: 0.0010595000147828279\n",
      "Training epoch 975, calculated loss: 0.0010583756174859847\n",
      "Training epoch 976, calculated loss: 0.0010572537875437073\n",
      "Training epoch 977, calculated loss: 0.0010561334223133767\n",
      "Training epoch 978, calculated loss: 0.0010550163588507124\n",
      "Training epoch 979, calculated loss: 0.0010539012838087134\n",
      "Training epoch 980, calculated loss: 0.001052787934269022\n",
      "Training epoch 981, calculated loss: 0.001051678138192938\n",
      "Training epoch 982, calculated loss: 0.0010505697519984127\n",
      "Training epoch 983, calculated loss: 0.0010494633522715755\n",
      "Training epoch 984, calculated loss: 0.0010483607579954556\n",
      "Training epoch 985, calculated loss: 0.0010472589954055274\n",
      "Training epoch 986, calculated loss: 0.001046159721067451\n",
      "Training epoch 987, calculated loss: 0.0010450637827522267\n",
      "Training epoch 988, calculated loss: 0.0010439688208789617\n",
      "Training epoch 989, calculated loss: 0.0010428766476733708\n",
      "Training epoch 990, calculated loss: 0.0010417872192383477\n",
      "Training epoch 991, calculated loss: 0.0010406990354758922\n",
      "Training epoch 992, calculated loss: 0.001039613899787026\n",
      "Training epoch 993, calculated loss: 0.0010385309179549724\n",
      "Training epoch 994, calculated loss: 0.0010374494493319726\n",
      "Training epoch 995, calculated loss: 0.001036371288747223\n",
      "Training epoch 996, calculated loss: 0.0010352946913992189\n",
      "Training epoch 997, calculated loss: 0.0010342198759327606\n",
      "Training epoch 998, calculated loss: 0.0010331486286359558\n",
      "Training epoch 999, calculated loss: 0.0010320783542111442\n"
     ]
    }
   ],
   "source": [
    "# Fit new model \n",
    "\n",
    "nn2.fit(X, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAaqUlEQVR4nO3de5hcdZ3n8fenu3MlCUmgcXIhdGBQ12EVsEUu6jjclkEUnWGVCC4CEt3dGRjHWQfGZxd3ZnTY0QdxduZxzSCXZ8XoyEWRZUQmiDyCZkgQEYgM4ZpAIE3CJYZA0t3f/eP8qqvS1Z10V3fV6T7n83qeeqrOqVPn9/vVST7169+5KSIwM7PyaMu7AmZm1loOfjOzknHwm5mVjIPfzKxkHPxmZiXj4DczKxkHv9kYSTpO0qOSfiPpg3nXZzBJP5J01ngva5OXfBy/jZSkJ4FPRMS/5FD2AuCvgVOBWcAzwHeAv42I7a2uz6C6rQJujoivjsO6/hl4d5qcBgSwM01/MyI+NdYyzNzjtwlP0nzgZ8AM4JiImA2cBMwFDmlgfR3jW0MOAh5q5IOD6xIRvx8RsyJiFnAd2Q/brPSoC/0mtMVKwMFv40LSBZLWS9oq6WZJC9N8SfqKpM2SXpb0gKTD0nunSnpY0jZJz0j6s2FW/6fANuDsiHgSICI2RMRFEfGApC5JURuCku6U9In0+uOS7k712Ar8laSXKvVIy3RK2iHpgDR9mqT703L3SHrrMO1+DDgY+EEa6pkmaWH6Dram7+SCmuU/L+l6Sd+U9Arw8VF+zydKelLSX0h6DvhHSftJulVSj6QXJf1A0qKaz/xU0sfT609I+kn6Ll6S9Likkxtc9pC0/LY0RPQ1SdeMpj2WDwe/jZmk44G/AT4MLACeAr6d3j4ZeA/wRrIe+keALem9bwCfTD34w4A7hiniRODGiOgfQzXfCTwOHAD8JXAjsKzm/Q8DP4mIzZKOBK4CPgnsB3wduFnStMErjYhDgKeB96de+evASmAjsBA4A/iipBNqPnY6cD3Z93FdA21ZTDbctQT4L2T/j/8xTR8E7AL2NOx0LPCr1LavkG2HRpZdCdyd3vtr4OzRN8Xy4OC38XAWcFVE3JeC7xLgGEldZCE0G3gz2T6ldRGxKX1uF/AWSXMi4sWIuG+Y9e8HbBrmvZF6NiL+d0T0RsQO4FvsHvwfTfMALgC+HhGrI6IvIq4FXgeO3lshkg4E3gX8eUS8FhH3A1cCH6tZ7GcR8b2I6E91Ga1e4PMRsTMidkRET0TclF6/AnwR+N09fP6xiLgqIvqAa4HFkvYfzbKSDgbeVlOPu4D/10BbLAcOfhsPC8l6+QBExG/IevWLIuIO4O+BfwCel7RC0py06B+S7ax9Kg0pHDPM+reQ/SUxFhsGTd8BzJD0TkkHAYcDN6X3DgI+k4Y3XpL0EnBgaufeLAS2RsS2mnlPAYtqpgfXZbSej4jKDl8k7SPpSklPp+GjO4DhghzguZrXr6bnWaNcdiGwZdAP11jbZS3i4Lfx8CxZWAJZEJH10p8BiIi/i4i3A79DNuTz39L8eyPidLLhl+8B/zTM+v8F+JCk4f69Vo7qmVkz77cGLbPb4Wtp2OifyHr9HwVuqQnrDcAXImJuzWNmRKwcpvxazwLzJc2umbeE9F0MVZcGDP78Z4GlwFERMQc4fozrH4lNwH6SptfMO7AF5do4cPDbaE2RNL3m0UE2RHKupMPTOPgXgdUR8aSkd6Re9RSygH4N6JM0VdJZkvaNiF3AK0DfMGVeDswBrk29cyQtknS5pLdGRA9ZsJ4tqV3SeYzsaJ9vke1zOIvqMA9k4+WfSvVW6lG/b1CYDykiNgD3AH+Tvp+3AufT2Fj+SM0m642/KGk/4H80sSwAIuIxsrH/S9O2fBfwvmaXa+PDwW+jdSuwo+bx+YhYBfx34AaynuAhwJlp+TlkQfoi2ZDHFuDL6b2PAU+m4YlPMczOwYjYSraTcRewWtI2YBXwMrA+LXYB2V8SW8j+srhnbw2JiNVkP0YLgX+umb8mre/vU73XM7qjb5YBXWS9/5uASyPi9lF8frQuB/Yla/s91LSlyZaR7bjfAlxKdl7F6y0q28bAJ3CZ2biQdANwf0T8Vd51sT1zj9/MGiLpKElLJbVJOhU4Dfh+3vWyvfNZf2bWqIVkw3vzyc5buCAiHsi3SjYSHuoxMysZD/WYmZXMpBjq2X///aOrqyvvapiZTSpr1659ISI6B89vWvBLuopsZ8/miKhclOtLwPvJLjP7GHBuRLy0t3V1dXWxZs2aZlXVzKyQJD011PxmDvVcA5wyaN7twGER8Vbg38iu6WJmZi3UtOBPF23aOmjejyKiN03+nOwqg2Zm1kJ57tw9j9adYWhmZkkuwS/pc2SXlh32+iWSlktaI2lNT09P6ypnZlZwLQ9+SeeQ7fQ9K/ZwEkFErIiI7ojo7uys2yltZmYNaunhnJJOAf4c+N2IeHVvy5uZ2fhrWo9f0kqyG2S/SdJGSeeTXe1wNnC7svuZ/p9mlW9mZkNrWo8/IpYNMXtP9/Ycd7v6+rnpF89wxpGLaWtTK4s2M5uwJsWZu41acdfjfOm2RxDwH7t9cyAzMyj4tXq2/Ca7LenLO3blXBMzs4mj0MGvNLrjC5CamVUVO/jzroCZ2QRU6OCvCNzlNzOrKHTwe6jHzKxewYM/S37nvplZVbGDP+8KmJlNQIUOfjMzq1eK4PcYv5lZVbGDv7Jz16P8ZmYDCh38SsnvHr+ZWVWxg997d83M6hQ6+M3MrF6hg98dfjOzesUO/oEzdz3Ib2ZWUejgr3Dum5lVFTr45cEeM7M6hQ7+Cnf4zcyqCh38vjqnmVm9Ygd/3hUwM5uACh38Fb5kg5lZVbGD36fumpnVKXTwV2LfY/xmZlXFDv6Bq3OamVlF04Jf0lWSNkt6sGbefEm3S3o0Pc9rVvlmZja0Zvb4rwFOGTTvYmBVRBwKrErTzeexHjOzAU0L/oi4C9g6aPbpwLXp9bXAB5tVPtRcj7+ZhZiZTTKtHuN/Q0RsAkjPBzSzMJ/AZWZWb8Lu3JW0XNIaSWt6enoaW8c418nMrAhaHfzPS1oAkJ43D7dgRKyIiO6I6O7s7GxZBc3Miq7VwX8zcE56fQ7w/WYWJt9s3cysTjMP51wJ/Ax4k6SNks4HLgNOkvQocFKabhrJN1s3Mxuso1krjohlw7x1QrPKHI5z38ysasLu3DUzs+YoRfB7qMfMrKrQwe+du2Zm9Yod/D6S38ysTqGDf4A7/GZmAwod/L4Pi5lZvWIHf3p2h9/MrKrYwT9wkTZHv5lZRaGD38zM6pUi+N3hNzOrKnTw+3BOM7N6xQ5+32zdzKxOoYO/wkM9ZmZVpQh+MzOrKkXw+1o9ZmZVhQ5+34jFzKxesYM/7wqYmU1AxQ5+J7+ZWZ1CB3+FL9lgZlZV6OD3RdrMzOoVO/g91mNmVqfQwV/hkR4zs6pCB787/GZm9Qod/BU+gcvMrKrQwT+wc9e5b2Y2IJfgl/RpSQ9JelDSSknTm1GO897MrF7Lg1/SIuBCoDsiDgPagTObWaZ/AMzMqvIa6ukAZkjqAGYCzzajEA/xmJnVa3nwR8QzwJeBp4FNwMsR8aPBy0laLmmNpDU9PT1jLHNMHzczK5Q8hnrmAacDS4GFwD6Szh68XESsiIjuiOju7OxsqKzKpRp8yQYzs6o8hnpOBJ6IiJ6I2AXcCBzbzAJ9PL+ZWVUewf80cLSkmcquqXACsK4ZBbmfb2ZWL48x/tXA9cB9wK9SHVY0p6xmrNXMbHLryKPQiLgUuDSPss3Myq7QZ+66w29mVq/QwV/hIR8zs6pCB78P4zQzq1fo4Dczs3oOfjOzkil08Hukx8ysXqGD38zM6hU6+H3nLTOzesUOfue+mVmdQge/mZnVK3Twu8NvZlav0MFf4SEfM7OqQge/A9/MrF6hg9/MzOoVOvh9OKeZWb1iB79z38ysTqGD38zM6jn4zcxKphTB77F+M7OqQge/b8RiZlav4MGfPQvlWxEzswmk0MFf4aEeM7OqQge/497MrN6Igl/SIZKmpdfvlXShpLnNrZqZmTXDSHv8NwB9kn4b+AawFPhW02o1Trxv18ys3kiDvz8ieoEPAVdExKeBBY0WKmmupOsl/VrSOknHNLqukfAPgJlZVccIl9slaRlwDvD+NG/KGMr9KvDDiDhD0lRg5hjWNSzv1DUzqzfSHv+5wDHAFyLiCUlLgW82UqCkOcB7yIaMiIidEfFSI+vam4HDOX00p5nZgBH1+CPiYeBCAEnzgNkRcVmDZR4M9ABXS3obsBa4KCK21y4kaTmwHGDJkiUNFpXxUI+ZWdVIj+q5U9IcSfOBX5KF9uUNltkBHAl8LSKOALYDFw9eKCJWRER3RHR3dnY2VJDz3sys3kiHevaNiFeAPwCujoi3Ayc2WOZGYGNErE7T15P9EDSNfwDMzKpGGvwdkhYAHwZuGUuBEfEcsEHSm9KsE4CHx7LOPRRW+2RmZoz8qJ6/BG4D7o6IeyUdDDw6hnL/GLguHdHzONnO43EXA89OfjOzipHu3P0u8N2a6ceBP2y00Ii4H+hu9PNmZta4ke7cXSzpJkmbJT0v6QZJi5tdubGKapffzMySkY7xXw3cDCwEFgE/SPMmBee+mVnVSIO/MyKujoje9LgGaOwYyxby2L6ZWb2RBv8Lks6W1J4eZwNbmlmx8VAZ6vGduMzMqkYa/OeRHcr5HLAJOIMmHYnTDI59M7OqEQV/RDwdER+IiM6IOCAiPkh2MteE5sA3M6s3ljtw/em41aLJPNJjZlY1luCf8Ne8HBjjz7caZmYTyliCf9LkqXfumplV7fHMXUnbGDrgBcxoSo3GkQ/nNDOrt8fgj4jZrapIU3iox8yszliGeiYPJ7+Z2YBCB7/z3sysXqGDv8Jj/WZmVYUO/vCNWMzM6hQ8+POugZnZxFPo4K/wD4CZWVWhg9+3XjQzq1fo4K9wj9/MrKrQwe/ANzOrV+jgr3D+m5lVFTr4K2P77vmbmVUVO/gd+GZmdQod/FX+BTAzqyhF8Lvnb2ZWlVvwS2qX9AtJt+RVBzOzMsqzx38RsK6ZBQxcq6eZhZiZTTK5BL+kxcD7gCubWc7Ambse6zEzG5BXj/8K4LNA/3ALSFouaY2kNT09PWMqzLFvZlbV8uCXdBqwOSLW7mm5iFgREd0R0d3Z2dlQWe7om5nVy6PHfxzwAUlPAt8Gjpf0zWYW6B8AM7Oqlgd/RFwSEYsjogs4E7gjIs5uRlnnHteVldmMlZuZTVKFPo7/4M5ZHH7g3LyrYWY2oXTkWXhE3Anc2YJyml2EmdmkUegeP4CUdw3MzCaW4gd/3hUwM5tgCh/84KN6zMxqFT74Jfmeu2ZmNYof/LjHb2ZWq/jB70F+M7PdFD74wT1+M7NahQ9+4TF+M7NahQ9+H89pZra74gc/HuoxM6tV+OAXvkibmVmt4ge/h3rMzHZT+OAH3OU3M6tR+OD3UT1mZrsrfvDLO3fNzGoVPvj7+oM1T73I6se35F0VM7MJofDBv3nb6wB8+jv351wTM7OJofDBXzmqp9/DPWZmQAmCvz0lv3fwmpllCh/8Fe7xm5llCh/8lbz3kT1mZpnCB39/6uqHk9/MDChD8KfAd+ybmWVKEPyVZ0e/mRmUIvgrQz05V8TMbIJoefBLOlDSjyWtk/SQpIuaWV5ljN89fjOzTEcOZfYCn4mI+yTNBtZKuj0iHm5GYZWhHue+mVmm5T3+iNgUEfel19uAdcCiZpVXHepx8puZQc5j/JK6gCOA1UO8t1zSGklrenp6Gi6jEvw+gcvMLJNb8EuaBdwA/ElEvDL4/YhYERHdEdHd2dnZcDkDQz0+oNPMDMgp+CVNIQv96yLixmaW5R6/mdnu8jiqR8A3gHURcXmzy+urdvnNzIx8evzHAR8Djpd0f3qc2qzCwidwmZntpuWHc0bETwG1qrzqUI+D38wMynTmbs71MDObKIof/P3Zszv8ZmaZ4ge/E9/MbDcOfjOzkilB8OddAzOziaXwwd/R1rIDiMzMJoXCB/8Bs6flXQUzswml8MH/xt+anXcVzMwmlMIH/xUfORyA+ftMzbkmZmYTQ+GDf+7MqXz82C56+/rzroqZ2YRQ+OCHbAdvnw/vMTMDShL87e1il4PfzAwoSfBPaWvzUI+ZWVKK4G9vE/0B/e71m5mVI/intGcncfU6+M3MyhH87W1ZM72D18ysJME/tSNr5uu9fTnXxMwsf6UI/hlT2gF4bZd38JqZlSP4p2bN3LHLPX4zs3IE/0CP38FvZlaK4J+egt89fjOzkgX/azsd/GZmpQj+Ge7xm5kNKEfwT3Xwm5lVlCL43zBnOgBPvrA955qYmeWvI49CJZ0CfBVoB66MiMuaWd6+M6bwlgVzuPZnT9HWJt6+ZB6HvmE282ZOQfI9ec2sXFoe/JLagX8ATgI2AvdKujkiHm5muV/40GF87qYH+dsfPjIwb/a0DhbPn8n+s6Yyb+ZU5u8zlbkzp7DP1A5mTG1nxpT2gefpU9qZNqWNKW1ttLeJjnZlz22V57bqdHv2LETld6VN2Wtl30F6xj88ZtZyefT4jwLWR8TjAJK+DZwONDX4j1gyj1svejdbt+/klxte4vEXtvPUlu088+IOtr66kw1bX2Xr9p288lpvM6sxpCF/EMhmtqXXg5fJ3qv+sOy2vrr11y9Uv8yQNdvjMkN9pH6ZIcrey3pG8mM4ZLvHoexmy+OHvuUltrjAom/DL37o33PU0vnjus48gn8RsKFmeiPwzsELSVoOLAdYsmTJuBU+f5+p/N6bD+D3hnm/t6+fHbv62LGrj9d2Vl/v2NnHa7199PUFvf1BX3/Q29+fnqP63NdPb3odAUH2DNlloQN2mx9kM/oHzassQ0B/1M+PgIj6i84NnjPEItl697rM3tYzRNmx5+mRlD3UZfTql9l7hYdeT+x1mWYa6vtoepmtLq/FjWz5V5rDNtxnWvu4rzOP4B/q57I+ViJWACsAuru7W/Z1d7S3Mbu9jdnTp7SqSDOzlsrjqJ6NwIE104uBZ3Ooh5lZKeUR/PcCh0paKmkqcCZwcw71MDMrpZYP9UREr6Q/Am4jO5zzqoh4qNX1MDMrq1yO44+IW4Fb8yjbzKzsSnHmrpmZVTn4zcxKxsFvZlYyDn4zs5JRq8+0a4SkHuCpBj++P/DCOFZnMnCby8FtLoextPmgiOgcPHNSBP9YSFoTEd1516OV3OZycJvLoRlt9lCPmVnJOPjNzEqmDMG/Iu8K5MBtLge3uRzGvc2FH+M3M7PdlaHHb2ZmNRz8ZmYlU+jgl3SKpEckrZd0cd71GQ+SDpT0Y0nrJD0k6aI0f76k2yU9mp7npfmS9HfpO3hA0pH5tqBxktol/ULSLWl6qaTVqc3fSZf5RtK0NL0+vd+VZ70bJWmupOsl/Tpt72OKvp0lfTr9u35Q0kpJ04u2nSVdJWmzpAdr5o16u0o6Jy3/qKRzRlOHwgZ/zU3dfx94C7BM0lvyrdW46AU+ExH/Djga+K+pXRcDqyLiUGBVmoas/Yemx3Lga62v8ri5CFhXM/2/gK+kNr8InJ/mnw+8GBG/DXwlLTcZfRX4YUS8GXgbWdsLu50lLQIuBLoj4jCyy7afSfG28zXAKYPmjWq7SpoPXEp229qjgEsrPxYjEhGFfADHALfVTF8CXJJ3vZrQzu8DJwGPAAvSvAXAI+n114FlNcsPLDeZHmR3alsFHA/cQnYLzxeAjsHbm+xeD8ek1x1pOeXdhlG2dw7wxOB6F3k7U70f9/y03W4B/kMRtzPQBTzY6HYFlgFfr5m/23J7exS2x8/QN3VflFNdmiL9aXsEsBp4Q0RsAkjPB6TFivI9XAF8FuhP0/sBL0VEb5qubddAm9P7L6flJ5ODgR7g6jS8daWkfSjwdo6IZ4AvA08Dm8i221qKvZ0rRrtdx7S9ixz8I7qp+2QlaRZwA/AnEfHKnhYdYt6k+h4knQZsjoi1tbOHWDRG8N5k0QEcCXwtIo4AtlP9838ok77NaajidGApsBDYh2yoY7Aibee9Ga6NY2p7kYO/sDd1lzSFLPSvi4gb0+znJS1I7y8ANqf5RfgejgM+IOlJ4Ntkwz1XAHMlVe4iV9uugTan9/cFtraywuNgI7AxIlan6evJfgiKvJ1PBJ6IiJ6I2AXcCBxLsbdzxWi365i2d5GDv5A3dZck4BvAuoi4vOatm4HKnv1zyMb+K/P/Uzo64Gjg5cqflJNFRFwSEYsjootsO94REWcBPwbOSIsNbnPluzgjLT+peoIR8RywQdKb0qwTgIcp8HYmG+I5WtLM9O+80ubCbucao92utwEnS5qX/lI6Oc0bmbx3cjR5B8qpwL8BjwGfy7s+49Smd5H9SfcAcH96nEo2trkKeDQ9z0/Li+zopseAX5EdMZF7O8bQ/vcCt6TXBwP/CqwHvgtMS/Onp+n16f2D8653g209HFiTtvX3gHlF387A/wR+DTwI/F9gWtG2M7CSbB/GLrKe+/mNbFfgvNT29cC5o6mDL9lgZlYyRR7qMTOzITj4zcxKxsFvZlYyDn4zs5Jx8JuZlYyD30pF0m/Sc5ekj47zuv9i0PQ947l+s/Hi4Ley6gJGFfzpiq97slvwR8Sxo6yTWUs4+K2sLgPeLen+dA34dklfknRvuu75JwEkvVfZ/Q++RXYCDZK+J2ltum788jTvMmBGWt91aV7lrwuldT8o6VeSPlKz7jtVveb+demMVbOm6tj7ImaFdDHwZxFxGkAK8Jcj4h2SpgF3S/pRWvYo4LCIeCJNnxcRWyXNAO6VdENEXCzpjyLi8CHK+gOys3DfBuyfPnNXeu8I4HfIrrNyN9l1iX46/s01q3KP3yxzMtk1Ue4nu8z1fmQ3vwD415rQB7hQ0i+Bn5NdKOtQ9uxdwMqI6IuI54GfAO+oWffGiOgnu/xG17i0xmwP3OM3ywj444jY7UJXkt5Ldknk2ukTyW4A8qqkO8muGbO3dQ/n9ZrXffj/pLWAe/xWVtuA2TXTtwH/OV3yGklvTDc+GWxfstv9vSrpzWS3v6zYVfn8IHcBH0n7ETqB95BdVMwsF+5dWFk9APSmIZtryO5v2wXcl3aw9gAfHOJzPwQ+JekBstvg/bzmvRXAA5Lui+yy0RU3kd0y8JdkV1b9bEQ8l344zFrOV+c0MysZD/WYmZWMg9/MrGQc/GZmJePgNzMrGQe/mVnJOPjNzErGwW9mVjL/H5i/zazxMYCQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss \n",
    "\n",
    "nn2.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred = nn2.predict(Xtrain)\n",
    "test_pred = nn2.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 100%\n",
      "Test accuracy is 100%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn2.accuracy(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {}%\".format(nn2.accuracy(ytest, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
