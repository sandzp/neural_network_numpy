{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy-only 2-layer Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from neural_net import NeuralNet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using UCI Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header names\n",
    "\n",
    "headers = ['age', 'sex','chest_pain','resting_blood_pressure',  \n",
    "        'serum_cholestoral', 'fasting_blood_sugar', 'resting_ecg_results',\n",
    "        'max_heart_rate_achieved', 'exercise_induced_angina', 'oldpeak',\"slope of the peak\",\n",
    "        'num_of_major_vessels','thal', 'heart_disease']\n",
    "\n",
    "# Make DF\n",
    "\n",
    "heart_df = pd.read_csv('Data/heart.dat', sep = ' ', names = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 14)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        0\n",
       "sex                        0\n",
       "chest_pain                 0\n",
       "resting_blood_pressure     0\n",
       "serum_cholestoral          0\n",
       "fasting_blood_sugar        0\n",
       "resting_ecg_results        0\n",
       "max_heart_rate_achieved    0\n",
       "exercise_induced_angina    0\n",
       "oldpeak                    0\n",
       "slope of the peak          0\n",
       "num_of_major_vessels       0\n",
       "thal                       0\n",
       "heart_disease              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "heart_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        float64\n",
       "sex                        float64\n",
       "chest_pain                 float64\n",
       "resting_blood_pressure     float64\n",
       "serum_cholestoral          float64\n",
       "fasting_blood_sugar        float64\n",
       "resting_ecg_results        float64\n",
       "max_heart_rate_achieved    float64\n",
       "exercise_induced_angina    float64\n",
       "oldpeak                    float64\n",
       "slope of the peak          float64\n",
       "num_of_major_vessels       float64\n",
       "thal                       float64\n",
       "heart_disease                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "heart_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X = heart_df.drop(columns = ['heart_disease'])\n",
    "\n",
    "# Enumerate target class i.e. labels\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(1,0)\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(2,1)\n",
    "\n",
    "y_label = heart_df['heart_disease'].values.reshape(X.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (216, 13)\n",
      "Shape of test set is (54, 13)\n",
      "Shape of train label is (216, 1)\n",
      "Shape of test labels is (54, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale values\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(Xtrain)\n",
    "Xtrain = sc.transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)\n",
    "\n",
    "print(f\"Shape of train set is {Xtrain.shape}\")\n",
    "print(f\"Shape of test set is {Xtest.shape}\")\n",
    "print(f\"Shape of train label is {ytrain.shape}\")\n",
    "print(f\"Shape of test labels is {ytest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Use it to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class of Neural Net using default parameters\n",
    "\n",
    "nn = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, calculated loss: 1.406818099455712\n",
      "Training epoch 1, calculated loss: 1.2283644538724772\n",
      "Training epoch 2, calculated loss: 1.1027579343396263\n",
      "Training epoch 3, calculated loss: 1.010277834670556\n",
      "Training epoch 4, calculated loss: 0.9375456811690593\n",
      "Training epoch 5, calculated loss: 0.8771342674193834\n",
      "Training epoch 6, calculated loss: 0.8256029193292977\n",
      "Training epoch 7, calculated loss: 0.7809976438171825\n",
      "Training epoch 8, calculated loss: 0.7419983603962279\n",
      "Training epoch 9, calculated loss: 0.707447080573352\n",
      "Training epoch 10, calculated loss: 0.6761308099914313\n",
      "Training epoch 11, calculated loss: 0.6479719529677453\n",
      "Training epoch 12, calculated loss: 0.6225758410329949\n",
      "Training epoch 13, calculated loss: 0.5995873203513066\n",
      "Training epoch 14, calculated loss: 0.5787978131048888\n",
      "Training epoch 15, calculated loss: 0.5599182115487729\n",
      "Training epoch 16, calculated loss: 0.5427118970422696\n",
      "Training epoch 17, calculated loss: 0.5270878681118275\n",
      "Training epoch 18, calculated loss: 0.5127870376789314\n",
      "Training epoch 19, calculated loss: 0.499702605855535\n",
      "Training epoch 20, calculated loss: 0.48738521253819006\n",
      "Training epoch 21, calculated loss: 0.476190151824692\n",
      "Training epoch 22, calculated loss: 0.4660017416103337\n",
      "Training epoch 23, calculated loss: 0.4566221760866736\n",
      "Training epoch 24, calculated loss: 0.44801274903895083\n",
      "Training epoch 25, calculated loss: 0.44006195321689057\n",
      "Training epoch 26, calculated loss: 0.432662096983625\n",
      "Training epoch 27, calculated loss: 0.42579673582802635\n",
      "Training epoch 28, calculated loss: 0.4193526960161447\n",
      "Training epoch 29, calculated loss: 0.41324847065923803\n",
      "Training epoch 30, calculated loss: 0.4074599294185932\n",
      "Training epoch 31, calculated loss: 0.40202222246799607\n",
      "Training epoch 32, calculated loss: 0.3968899842644129\n",
      "Training epoch 33, calculated loss: 0.3920397357291702\n",
      "Training epoch 34, calculated loss: 0.3874509214748642\n",
      "Training epoch 35, calculated loss: 0.3830958350064345\n",
      "Training epoch 36, calculated loss: 0.3789607677725741\n",
      "Training epoch 37, calculated loss: 0.37502934313790387\n",
      "Training epoch 38, calculated loss: 0.37128637914437745\n",
      "Training epoch 39, calculated loss: 0.36771249418561974\n",
      "Training epoch 40, calculated loss: 0.3642833861662177\n",
      "Training epoch 41, calculated loss: 0.3610015600693919\n",
      "Training epoch 42, calculated loss: 0.3578906297976741\n",
      "Training epoch 43, calculated loss: 0.3549156523498778\n",
      "Training epoch 44, calculated loss: 0.3520709025825898\n",
      "Training epoch 45, calculated loss: 0.3493598084388319\n",
      "Training epoch 46, calculated loss: 0.346768845132244\n",
      "Training epoch 47, calculated loss: 0.34431068879480353\n",
      "Training epoch 48, calculated loss: 0.34194282652162544\n",
      "Training epoch 49, calculated loss: 0.3396583732933476\n",
      "Training epoch 50, calculated loss: 0.3374581250738244\n",
      "Training epoch 51, calculated loss: 0.33534760607313313\n",
      "Training epoch 52, calculated loss: 0.33330547606352195\n",
      "Training epoch 53, calculated loss: 0.33133938501962\n",
      "Training epoch 54, calculated loss: 0.32943711682743476\n",
      "Training epoch 55, calculated loss: 0.327561226258419\n",
      "Training epoch 56, calculated loss: 0.32575478917638145\n",
      "Training epoch 57, calculated loss: 0.3240014979718713\n",
      "Training epoch 58, calculated loss: 0.3222911256958769\n",
      "Training epoch 59, calculated loss: 0.320614686718432\n",
      "Training epoch 60, calculated loss: 0.3189950287443454\n",
      "Training epoch 61, calculated loss: 0.31743139477363524\n",
      "Training epoch 62, calculated loss: 0.315898744016534\n",
      "Training epoch 63, calculated loss: 0.3144261330659574\n",
      "Training epoch 64, calculated loss: 0.312979010473956\n",
      "Training epoch 65, calculated loss: 0.3115704557826012\n",
      "Training epoch 66, calculated loss: 0.3102387909138682\n",
      "Training epoch 67, calculated loss: 0.30892274240067386\n",
      "Training epoch 68, calculated loss: 0.30765010587991815\n",
      "Training epoch 69, calculated loss: 0.30639677395985704\n",
      "Training epoch 70, calculated loss: 0.3052113102149649\n",
      "Training epoch 71, calculated loss: 0.3040603570147317\n",
      "Training epoch 72, calculated loss: 0.30293845881919423\n",
      "Training epoch 73, calculated loss: 0.30183492009912677\n",
      "Training epoch 74, calculated loss: 0.3007443230734181\n",
      "Training epoch 75, calculated loss: 0.29967130343718507\n",
      "Training epoch 76, calculated loss: 0.29862341684204396\n",
      "Training epoch 77, calculated loss: 0.2975953115831883\n",
      "Training epoch 78, calculated loss: 0.2965805948342925\n",
      "Training epoch 79, calculated loss: 0.2955858072889425\n",
      "Training epoch 80, calculated loss: 0.2946025425015101\n",
      "Training epoch 81, calculated loss: 0.2936528622103815\n",
      "Training epoch 82, calculated loss: 0.29270959325424567\n",
      "Training epoch 83, calculated loss: 0.2917867375977857\n",
      "Training epoch 84, calculated loss: 0.2908766664032826\n",
      "Training epoch 85, calculated loss: 0.2899858794918911\n",
      "Training epoch 86, calculated loss: 0.2891217726542763\n",
      "Training epoch 87, calculated loss: 0.2882534060741006\n",
      "Training epoch 88, calculated loss: 0.28741232695069635\n",
      "Training epoch 89, calculated loss: 0.28658222439112885\n",
      "Training epoch 90, calculated loss: 0.2857393049629605\n",
      "Training epoch 91, calculated loss: 0.28488377530196873\n",
      "Training epoch 92, calculated loss: 0.2840357881439885\n",
      "Training epoch 93, calculated loss: 0.2831916166264467\n",
      "Training epoch 94, calculated loss: 0.2823680182360841\n",
      "Training epoch 95, calculated loss: 0.28153740084295914\n",
      "Training epoch 96, calculated loss: 0.28071874261248086\n",
      "Training epoch 97, calculated loss: 0.2799062326813389\n",
      "Training epoch 98, calculated loss: 0.2790974743022777\n",
      "Training epoch 99, calculated loss: 0.2783107364057317\n",
      "Training epoch 100, calculated loss: 0.27752267595856\n",
      "Training epoch 101, calculated loss: 0.276746158103033\n",
      "Training epoch 102, calculated loss: 0.27598304715732486\n",
      "Training epoch 103, calculated loss: 0.27526560371594744\n",
      "Training epoch 104, calculated loss: 0.2745564389923945\n",
      "Training epoch 105, calculated loss: 0.27386237629456256\n",
      "Training epoch 106, calculated loss: 0.27319294620008727\n",
      "Training epoch 107, calculated loss: 0.27252215813264585\n",
      "Training epoch 108, calculated loss: 0.2718580260151962\n",
      "Training epoch 109, calculated loss: 0.2711923110045857\n",
      "Training epoch 110, calculated loss: 0.2705296337982729\n",
      "Training epoch 111, calculated loss: 0.2698683935245679\n",
      "Training epoch 112, calculated loss: 0.269213933482101\n",
      "Training epoch 113, calculated loss: 0.2685728735027599\n",
      "Training epoch 114, calculated loss: 0.2679320976157896\n",
      "Training epoch 115, calculated loss: 0.26729387112097147\n",
      "Training epoch 116, calculated loss: 0.26666744357147076\n",
      "Training epoch 117, calculated loss: 0.2660492049364576\n",
      "Training epoch 118, calculated loss: 0.2654278058608459\n",
      "Training epoch 119, calculated loss: 0.26481833920220205\n",
      "Training epoch 120, calculated loss: 0.2642025845392992\n",
      "Training epoch 121, calculated loss: 0.2635980803620852\n",
      "Training epoch 122, calculated loss: 0.2629979179693585\n",
      "Training epoch 123, calculated loss: 0.26240444035825994\n",
      "Training epoch 124, calculated loss: 0.2618081094455079\n",
      "Training epoch 125, calculated loss: 0.26122323147646254\n",
      "Training epoch 126, calculated loss: 0.26063438451572873\n",
      "Training epoch 127, calculated loss: 0.2600492974146751\n",
      "Training epoch 128, calculated loss: 0.25946706587215085\n",
      "Training epoch 129, calculated loss: 0.2589003214821959\n",
      "Training epoch 130, calculated loss: 0.25832673077991153\n",
      "Training epoch 131, calculated loss: 0.25775349782981116\n",
      "Training epoch 132, calculated loss: 0.2571809593347549\n",
      "Training epoch 133, calculated loss: 0.2566217662792042\n",
      "Training epoch 134, calculated loss: 0.25605310226066813\n",
      "Training epoch 135, calculated loss: 0.2554915633487888\n",
      "Training epoch 136, calculated loss: 0.2549334344446226\n",
      "Training epoch 137, calculated loss: 0.25437693001749573\n",
      "Training epoch 138, calculated loss: 0.25382471688608255\n",
      "Training epoch 139, calculated loss: 0.25327245432645695\n",
      "Training epoch 140, calculated loss: 0.2527279179406165\n",
      "Training epoch 141, calculated loss: 0.25217915831365895\n",
      "Training epoch 142, calculated loss: 0.25163781252928474\n",
      "Training epoch 143, calculated loss: 0.2510910290687681\n",
      "Training epoch 144, calculated loss: 0.2505554057293129\n",
      "Training epoch 145, calculated loss: 0.25001024432803864\n",
      "Training epoch 146, calculated loss: 0.24947413070800084\n",
      "Training epoch 147, calculated loss: 0.24894197281573963\n",
      "Training epoch 148, calculated loss: 0.24841204559725721\n",
      "Training epoch 149, calculated loss: 0.2478847909568514\n",
      "Training epoch 150, calculated loss: 0.24735737248299336\n",
      "Training epoch 151, calculated loss: 0.2468379362565074\n",
      "Training epoch 152, calculated loss: 0.2463162713651728\n",
      "Training epoch 153, calculated loss: 0.24580155619808317\n",
      "Training epoch 154, calculated loss: 0.2452840582774365\n",
      "Training epoch 155, calculated loss: 0.24476858805520996\n",
      "Training epoch 156, calculated loss: 0.2442588084173961\n",
      "Training epoch 157, calculated loss: 0.2437435362084511\n",
      "Training epoch 158, calculated loss: 0.24323462312043742\n",
      "Training epoch 159, calculated loss: 0.24272893998407646\n",
      "Training epoch 160, calculated loss: 0.24223230619843128\n",
      "Training epoch 161, calculated loss: 0.24174067957462392\n",
      "Training epoch 162, calculated loss: 0.2412492025963735\n",
      "Training epoch 163, calculated loss: 0.24075542394516838\n",
      "Training epoch 164, calculated loss: 0.24026088887203656\n",
      "Training epoch 165, calculated loss: 0.23977173927195933\n",
      "Training epoch 166, calculated loss: 0.2392809939288868\n",
      "Training epoch 167, calculated loss: 0.23879307544349948\n",
      "Training epoch 168, calculated loss: 0.23831239789946876\n",
      "Training epoch 169, calculated loss: 0.2378248066660743\n",
      "Training epoch 170, calculated loss: 0.23734146228200687\n",
      "Training epoch 171, calculated loss: 0.23685621606243795\n",
      "Training epoch 172, calculated loss: 0.2363680485279514\n",
      "Training epoch 173, calculated loss: 0.23587090742674685\n",
      "Training epoch 174, calculated loss: 0.23537863633274222\n",
      "Training epoch 175, calculated loss: 0.2348903195279528\n",
      "Training epoch 176, calculated loss: 0.2344042567609047\n",
      "Training epoch 177, calculated loss: 0.23391175624870872\n",
      "Training epoch 178, calculated loss: 0.23343017770514024\n",
      "Training epoch 179, calculated loss: 0.23293822165842706\n",
      "Training epoch 180, calculated loss: 0.2324608315998643\n",
      "Training epoch 181, calculated loss: 0.23197788724357066\n",
      "Training epoch 182, calculated loss: 0.23149935858861018\n",
      "Training epoch 183, calculated loss: 0.2310124666397743\n",
      "Training epoch 184, calculated loss: 0.23053584748400333\n",
      "Training epoch 185, calculated loss: 0.23004806467142344\n",
      "Training epoch 186, calculated loss: 0.22957871113285785\n",
      "Training epoch 187, calculated loss: 0.22909778836401165\n",
      "Training epoch 188, calculated loss: 0.22862656328057268\n",
      "Training epoch 189, calculated loss: 0.22814334755455365\n",
      "Training epoch 190, calculated loss: 0.2276709835482682\n",
      "Training epoch 191, calculated loss: 0.22719038218911317\n",
      "Training epoch 192, calculated loss: 0.22672138605294875\n",
      "Training epoch 193, calculated loss: 0.22624451954760144\n",
      "Training epoch 194, calculated loss: 0.22578737541844868\n",
      "Training epoch 195, calculated loss: 0.2253161523571191\n",
      "Training epoch 196, calculated loss: 0.22485424516447283\n",
      "Training epoch 197, calculated loss: 0.22439134919349\n",
      "Training epoch 198, calculated loss: 0.2239280865753238\n",
      "Training epoch 199, calculated loss: 0.22346958764249777\n",
      "Training epoch 200, calculated loss: 0.22301092124771404\n",
      "Training epoch 201, calculated loss: 0.22255614764838536\n",
      "Training epoch 202, calculated loss: 0.22209400264876789\n",
      "Training epoch 203, calculated loss: 0.22164462420329273\n",
      "Training epoch 204, calculated loss: 0.22119825473660013\n",
      "Training epoch 205, calculated loss: 0.2207642169610009\n",
      "Training epoch 206, calculated loss: 0.22033261045718436\n",
      "Training epoch 207, calculated loss: 0.2199035580153558\n",
      "Training epoch 208, calculated loss: 0.2194706450749502\n",
      "Training epoch 209, calculated loss: 0.21904803815622734\n",
      "Training epoch 210, calculated loss: 0.21863185760205067\n",
      "Training epoch 211, calculated loss: 0.21819885067632558\n",
      "Training epoch 212, calculated loss: 0.21778363187425878\n",
      "Training epoch 213, calculated loss: 0.21735914641300724\n",
      "Training epoch 214, calculated loss: 0.2169282705446066\n",
      "Training epoch 215, calculated loss: 0.21651063110765484\n",
      "Training epoch 216, calculated loss: 0.21608748592860538\n",
      "Training epoch 217, calculated loss: 0.21566699955088842\n",
      "Training epoch 218, calculated loss: 0.21526024515912645\n",
      "Training epoch 219, calculated loss: 0.2148374729826649\n",
      "Training epoch 220, calculated loss: 0.21440432542116394\n",
      "Training epoch 221, calculated loss: 0.2139958620223014\n",
      "Training epoch 222, calculated loss: 0.2135679645465027\n",
      "Training epoch 223, calculated loss: 0.21315127653190946\n",
      "Training epoch 224, calculated loss: 0.2127425135582099\n",
      "Training epoch 225, calculated loss: 0.21232183920557945\n",
      "Training epoch 226, calculated loss: 0.2119010342251094\n",
      "Training epoch 227, calculated loss: 0.21148229859970416\n",
      "Training epoch 228, calculated loss: 0.21106058291309945\n",
      "Training epoch 229, calculated loss: 0.21065369209809542\n",
      "Training epoch 230, calculated loss: 0.21023967067439361\n",
      "Training epoch 231, calculated loss: 0.20982902672714915\n",
      "Training epoch 232, calculated loss: 0.20939613306028493\n",
      "Training epoch 233, calculated loss: 0.2089876071558346\n",
      "Training epoch 234, calculated loss: 0.20857927972805704\n",
      "Training epoch 235, calculated loss: 0.20816528682003066\n",
      "Training epoch 236, calculated loss: 0.2077553652163357\n",
      "Training epoch 237, calculated loss: 0.20733396932847467\n",
      "Training epoch 238, calculated loss: 0.20691243935101947\n",
      "Training epoch 239, calculated loss: 0.2064980819185977\n",
      "Training epoch 240, calculated loss: 0.20609397478307614\n",
      "Training epoch 241, calculated loss: 0.20566760276359936\n",
      "Training epoch 242, calculated loss: 0.20525538044164046\n",
      "Training epoch 243, calculated loss: 0.20484343749057887\n",
      "Training epoch 244, calculated loss: 0.20441927750730154\n",
      "Training epoch 245, calculated loss: 0.2040126296924069\n",
      "Training epoch 246, calculated loss: 0.20362076530831855\n",
      "Training epoch 247, calculated loss: 0.20319828652626068\n",
      "Training epoch 248, calculated loss: 0.20277780790284053\n",
      "Training epoch 249, calculated loss: 0.20236571298216968\n",
      "Training epoch 250, calculated loss: 0.20195238099992036\n",
      "Training epoch 251, calculated loss: 0.20156261838124942\n",
      "Training epoch 252, calculated loss: 0.20115952703475037\n",
      "Training epoch 253, calculated loss: 0.2007483563441138\n",
      "Training epoch 254, calculated loss: 0.2003377703110201\n",
      "Training epoch 255, calculated loss: 0.1999290356881397\n",
      "Training epoch 256, calculated loss: 0.19954320203371445\n",
      "Training epoch 257, calculated loss: 0.199182485115241\n",
      "Training epoch 258, calculated loss: 0.19880105043699905\n",
      "Training epoch 259, calculated loss: 0.19840192422729222\n",
      "Training epoch 260, calculated loss: 0.1980207292858923\n",
      "Training epoch 261, calculated loss: 0.19764848083957062\n",
      "Training epoch 262, calculated loss: 0.19726230645401954\n",
      "Training epoch 263, calculated loss: 0.19688050465637108\n",
      "Training epoch 264, calculated loss: 0.19651275770998625\n",
      "Training epoch 265, calculated loss: 0.19612384179538422\n",
      "Training epoch 266, calculated loss: 0.1957630147609834\n",
      "Training epoch 267, calculated loss: 0.1953722569566314\n",
      "Training epoch 268, calculated loss: 0.19498930212044344\n",
      "Training epoch 269, calculated loss: 0.19461641793214018\n",
      "Training epoch 270, calculated loss: 0.1942441575982174\n",
      "Training epoch 271, calculated loss: 0.19386863167293533\n",
      "Training epoch 272, calculated loss: 0.19348478056753204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 273, calculated loss: 0.19311064629426253\n",
      "Training epoch 274, calculated loss: 0.192726628477748\n",
      "Training epoch 275, calculated loss: 0.19235661948829577\n",
      "Training epoch 276, calculated loss: 0.19196502553151357\n",
      "Training epoch 277, calculated loss: 0.1915986397577637\n",
      "Training epoch 278, calculated loss: 0.19122341022500783\n",
      "Training epoch 279, calculated loss: 0.19083085175095132\n",
      "Training epoch 280, calculated loss: 0.19046145055031888\n",
      "Training epoch 281, calculated loss: 0.19009102370011108\n",
      "Training epoch 282, calculated loss: 0.18970648638050977\n",
      "Training epoch 283, calculated loss: 0.18934881202258527\n",
      "Training epoch 284, calculated loss: 0.1889738749204568\n",
      "Training epoch 285, calculated loss: 0.1886218398356439\n",
      "Training epoch 286, calculated loss: 0.18825106361899036\n",
      "Training epoch 287, calculated loss: 0.18786522308620968\n",
      "Training epoch 288, calculated loss: 0.1875090812628159\n",
      "Training epoch 289, calculated loss: 0.1871345473867985\n",
      "Training epoch 290, calculated loss: 0.18676501545272986\n",
      "Training epoch 291, calculated loss: 0.18641840187728403\n",
      "Training epoch 292, calculated loss: 0.18604482687305232\n",
      "Training epoch 293, calculated loss: 0.18568545639933703\n",
      "Training epoch 294, calculated loss: 0.18532975861852727\n",
      "Training epoch 295, calculated loss: 0.18498483777153138\n",
      "Training epoch 296, calculated loss: 0.18461185314995485\n",
      "Training epoch 297, calculated loss: 0.18425013332752715\n",
      "Training epoch 298, calculated loss: 0.18391152812839598\n",
      "Training epoch 299, calculated loss: 0.18355248732093069\n",
      "Training epoch 300, calculated loss: 0.18320601006803303\n",
      "Training epoch 301, calculated loss: 0.1828540115677848\n",
      "Training epoch 302, calculated loss: 0.18251686411492968\n",
      "Training epoch 303, calculated loss: 0.1821732197656081\n",
      "Training epoch 304, calculated loss: 0.1818170142445932\n",
      "Training epoch 305, calculated loss: 0.1814719297134536\n",
      "Training epoch 306, calculated loss: 0.18113902574112584\n",
      "Training epoch 307, calculated loss: 0.1807825447128808\n",
      "Training epoch 308, calculated loss: 0.18045659233714778\n",
      "Training epoch 309, calculated loss: 0.1801078197240961\n",
      "Training epoch 310, calculated loss: 0.17979673876733465\n",
      "Training epoch 311, calculated loss: 0.1794675592851685\n",
      "Training epoch 312, calculated loss: 0.1791372893996969\n",
      "Training epoch 313, calculated loss: 0.1788165049438514\n",
      "Training epoch 314, calculated loss: 0.17849558383987\n",
      "Training epoch 315, calculated loss: 0.17816066147475693\n",
      "Training epoch 316, calculated loss: 0.1778349304644559\n",
      "Training epoch 317, calculated loss: 0.17751311246325255\n",
      "Training epoch 318, calculated loss: 0.17719705052409218\n",
      "Training epoch 319, calculated loss: 0.17686148026632312\n",
      "Training epoch 320, calculated loss: 0.17655129211747075\n",
      "Training epoch 321, calculated loss: 0.17623781820387602\n",
      "Training epoch 322, calculated loss: 0.1759291444132664\n",
      "Training epoch 323, calculated loss: 0.1756326973705929\n",
      "Training epoch 324, calculated loss: 0.17534433945718303\n",
      "Training epoch 325, calculated loss: 0.17505326948737784\n",
      "Training epoch 326, calculated loss: 0.17476336279446636\n",
      "Training epoch 327, calculated loss: 0.17447883345769297\n",
      "Training epoch 328, calculated loss: 0.17419661358090496\n",
      "Training epoch 329, calculated loss: 0.1739487350853175\n",
      "Training epoch 330, calculated loss: 0.1736765846606785\n",
      "Training epoch 331, calculated loss: 0.1734207068236491\n",
      "Training epoch 332, calculated loss: 0.1731866336973259\n",
      "Training epoch 333, calculated loss: 0.17294227082277783\n",
      "Training epoch 334, calculated loss: 0.1726946777873684\n",
      "Training epoch 335, calculated loss: 0.17243647813121277\n",
      "Training epoch 336, calculated loss: 0.1721803136433498\n",
      "Training epoch 337, calculated loss: 0.17195028784810754\n",
      "Training epoch 338, calculated loss: 0.17169346297302743\n",
      "Training epoch 339, calculated loss: 0.17145706532276012\n",
      "Training epoch 340, calculated loss: 0.1711999783518241\n",
      "Training epoch 341, calculated loss: 0.17097264265344975\n",
      "Training epoch 342, calculated loss: 0.17072128054425517\n",
      "Training epoch 343, calculated loss: 0.17048209845709925\n",
      "Training epoch 344, calculated loss: 0.17023203315163624\n",
      "Training epoch 345, calculated loss: 0.1699941945008134\n",
      "Training epoch 346, calculated loss: 0.16974128122812893\n",
      "Training epoch 347, calculated loss: 0.16948323256936773\n",
      "Training epoch 348, calculated loss: 0.16924536060296808\n",
      "Training epoch 349, calculated loss: 0.16898732423520862\n",
      "Training epoch 350, calculated loss: 0.1687340466210543\n",
      "Training epoch 351, calculated loss: 0.16847927733185036\n",
      "Training epoch 352, calculated loss: 0.16825200969880444\n",
      "Training epoch 353, calculated loss: 0.1680184489296365\n",
      "Training epoch 354, calculated loss: 0.16775547357820084\n",
      "Training epoch 355, calculated loss: 0.16751159568829121\n",
      "Training epoch 356, calculated loss: 0.1672623117203635\n",
      "Training epoch 357, calculated loss: 0.1670342009015608\n",
      "Training epoch 358, calculated loss: 0.16680226621584215\n",
      "Training epoch 359, calculated loss: 0.16657145511818808\n",
      "Training epoch 360, calculated loss: 0.16630767040088318\n",
      "Training epoch 361, calculated loss: 0.16607419005800161\n",
      "Training epoch 362, calculated loss: 0.16585328429306176\n",
      "Training epoch 363, calculated loss: 0.16560557466739984\n",
      "Training epoch 364, calculated loss: 0.16536030056040155\n",
      "Training epoch 365, calculated loss: 0.16513041112805482\n",
      "Training epoch 366, calculated loss: 0.1648936309149001\n",
      "Training epoch 367, calculated loss: 0.16466072675561894\n",
      "Training epoch 368, calculated loss: 0.1644214020694981\n",
      "Training epoch 369, calculated loss: 0.16418212751306865\n",
      "Training epoch 370, calculated loss: 0.16394143349632975\n",
      "Training epoch 371, calculated loss: 0.16371446101932746\n",
      "Training epoch 372, calculated loss: 0.16349024136128779\n",
      "Training epoch 373, calculated loss: 0.16324980532824224\n",
      "Training epoch 374, calculated loss: 0.16301616856081388\n",
      "Training epoch 375, calculated loss: 0.16277348779936215\n",
      "Training epoch 376, calculated loss: 0.16255532024251432\n",
      "Training epoch 377, calculated loss: 0.16231595238518823\n",
      "Training epoch 378, calculated loss: 0.1620866097438773\n",
      "Training epoch 379, calculated loss: 0.16184450053069116\n",
      "Training epoch 380, calculated loss: 0.16163600750490625\n",
      "Training epoch 381, calculated loss: 0.16140096811266103\n",
      "Training epoch 382, calculated loss: 0.16116446442739135\n",
      "Training epoch 383, calculated loss: 0.16093534478141389\n",
      "Training epoch 384, calculated loss: 0.1607157263221618\n",
      "Training epoch 385, calculated loss: 0.1604797973286627\n",
      "Training epoch 386, calculated loss: 0.1602524754063829\n",
      "Training epoch 387, calculated loss: 0.16002321456542906\n",
      "Training epoch 388, calculated loss: 0.15980338227215293\n",
      "Training epoch 389, calculated loss: 0.15958713099884503\n",
      "Training epoch 390, calculated loss: 0.15938264923304102\n",
      "Training epoch 391, calculated loss: 0.15914168270242113\n",
      "Training epoch 392, calculated loss: 0.1589254563886855\n",
      "Training epoch 393, calculated loss: 0.15871048884938777\n",
      "Training epoch 394, calculated loss: 0.15849417766186147\n",
      "Training epoch 395, calculated loss: 0.158271723475272\n",
      "Training epoch 396, calculated loss: 0.15804571410781076\n",
      "Training epoch 397, calculated loss: 0.15784562631186577\n",
      "Training epoch 398, calculated loss: 0.1576369728677055\n",
      "Training epoch 399, calculated loss: 0.15742515984689748\n",
      "Training epoch 400, calculated loss: 0.1571858211718213\n",
      "Training epoch 401, calculated loss: 0.15698599481324108\n",
      "Training epoch 402, calculated loss: 0.1567750978752191\n",
      "Training epoch 403, calculated loss: 0.15655796423517826\n",
      "Training epoch 404, calculated loss: 0.1563291245160032\n",
      "Training epoch 405, calculated loss: 0.1561085345953448\n",
      "Training epoch 406, calculated loss: 0.155913303447273\n",
      "Training epoch 407, calculated loss: 0.15569104338576284\n",
      "Training epoch 408, calculated loss: 0.1554782958364362\n",
      "Training epoch 409, calculated loss: 0.15524938642490338\n",
      "Training epoch 410, calculated loss: 0.1550566188530461\n",
      "Training epoch 411, calculated loss: 0.15484971009912843\n",
      "Training epoch 412, calculated loss: 0.15462897713674464\n",
      "Training epoch 413, calculated loss: 0.1543972762265905\n",
      "Training epoch 414, calculated loss: 0.1542018293446751\n",
      "Training epoch 415, calculated loss: 0.15398717441734588\n",
      "Training epoch 416, calculated loss: 0.1537806086744322\n",
      "Training epoch 417, calculated loss: 0.15354170684750246\n",
      "Training epoch 418, calculated loss: 0.15335972380353524\n",
      "Training epoch 419, calculated loss: 0.15314548771175726\n",
      "Training epoch 420, calculated loss: 0.15292860566390828\n",
      "Training epoch 421, calculated loss: 0.15271025318384124\n",
      "Training epoch 422, calculated loss: 0.1525127899603239\n",
      "Training epoch 423, calculated loss: 0.15231247878553822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 424, calculated loss: 0.1521108764328087\n",
      "Training epoch 425, calculated loss: 0.15190066513571876\n",
      "Training epoch 426, calculated loss: 0.15174397750898871\n",
      "Training epoch 427, calculated loss: 0.15154324967660396\n",
      "Training epoch 428, calculated loss: 0.1513364468688277\n",
      "Training epoch 429, calculated loss: 0.1511480397586336\n",
      "Training epoch 430, calculated loss: 0.15095051070204293\n",
      "Training epoch 431, calculated loss: 0.1507677867515939\n",
      "Training epoch 432, calculated loss: 0.1505720638455779\n",
      "Training epoch 433, calculated loss: 0.1503764324283442\n",
      "Training epoch 434, calculated loss: 0.15019125186645826\n",
      "Training epoch 435, calculated loss: 0.14999649628648593\n",
      "Training epoch 436, calculated loss: 0.14981817237114212\n",
      "Training epoch 437, calculated loss: 0.14960372682190795\n",
      "Training epoch 438, calculated loss: 0.14943510557256834\n",
      "Training epoch 439, calculated loss: 0.14923156198711446\n",
      "Training epoch 440, calculated loss: 0.1490323110302897\n",
      "Training epoch 441, calculated loss: 0.14883953673094974\n",
      "Training epoch 442, calculated loss: 0.1486518024178283\n",
      "Training epoch 443, calculated loss: 0.14846139440125317\n",
      "Training epoch 444, calculated loss: 0.14826404317333006\n",
      "Training epoch 445, calculated loss: 0.14807262470931376\n",
      "Training epoch 446, calculated loss: 0.14789467609569903\n",
      "Training epoch 447, calculated loss: 0.14768747847749986\n",
      "Training epoch 448, calculated loss: 0.1474960417560906\n",
      "Training epoch 449, calculated loss: 0.14730509278325882\n",
      "Training epoch 450, calculated loss: 0.1471359462572863\n",
      "Training epoch 451, calculated loss: 0.14693179436827272\n",
      "Training epoch 452, calculated loss: 0.1467399177914218\n",
      "Training epoch 453, calculated loss: 0.14655330684991322\n",
      "Training epoch 454, calculated loss: 0.14637761127055562\n",
      "Training epoch 455, calculated loss: 0.1461791588962648\n",
      "Training epoch 456, calculated loss: 0.14598535187828576\n",
      "Training epoch 457, calculated loss: 0.14580865595688222\n",
      "Training epoch 458, calculated loss: 0.14562088851220337\n",
      "Training epoch 459, calculated loss: 0.14544189874108715\n",
      "Training epoch 460, calculated loss: 0.14524241900311582\n",
      "Training epoch 461, calculated loss: 0.14507407066121558\n",
      "Training epoch 462, calculated loss: 0.14487802313960552\n",
      "Training epoch 463, calculated loss: 0.14469551592094915\n",
      "Training epoch 464, calculated loss: 0.14451702463635938\n",
      "Training epoch 465, calculated loss: 0.14433866494052291\n",
      "Training epoch 466, calculated loss: 0.14415349748542902\n",
      "Training epoch 467, calculated loss: 0.1439582078598358\n",
      "Training epoch 468, calculated loss: 0.14378872473525617\n",
      "Training epoch 469, calculated loss: 0.14360374420805855\n",
      "Training epoch 470, calculated loss: 0.14341147973412957\n",
      "Training epoch 471, calculated loss: 0.14322867211484686\n",
      "Training epoch 472, calculated loss: 0.14305041911098115\n",
      "Training epoch 473, calculated loss: 0.142869843422236\n",
      "Training epoch 474, calculated loss: 0.14268017204880018\n",
      "Training epoch 475, calculated loss: 0.1425192685560983\n",
      "Training epoch 476, calculated loss: 0.14233459141981308\n",
      "Training epoch 477, calculated loss: 0.1421460365936759\n",
      "Training epoch 478, calculated loss: 0.1419797122652719\n",
      "Training epoch 479, calculated loss: 0.14182147289761057\n",
      "Training epoch 480, calculated loss: 0.14163200080777402\n",
      "Training epoch 481, calculated loss: 0.14143658552482288\n",
      "Training epoch 482, calculated loss: 0.14128617243524833\n",
      "Training epoch 483, calculated loss: 0.14111372747242554\n",
      "Training epoch 484, calculated loss: 0.14092477685885882\n",
      "Training epoch 485, calculated loss: 0.1407490590506156\n",
      "Training epoch 486, calculated loss: 0.14058417728075964\n",
      "Training epoch 487, calculated loss: 0.14042727248020903\n",
      "Training epoch 488, calculated loss: 0.1402195545926085\n",
      "Training epoch 489, calculated loss: 0.14006221153078655\n",
      "Training epoch 490, calculated loss: 0.13990036718235757\n",
      "Training epoch 491, calculated loss: 0.13972298261865274\n",
      "Training epoch 492, calculated loss: 0.13955754921744454\n",
      "Training epoch 493, calculated loss: 0.1393711758206643\n",
      "Training epoch 494, calculated loss: 0.139227297141135\n",
      "Training epoch 495, calculated loss: 0.1390445106681748\n",
      "Training epoch 496, calculated loss: 0.1388813906825923\n",
      "Training epoch 497, calculated loss: 0.13870089357746185\n",
      "Training epoch 498, calculated loss: 0.13854056453284114\n",
      "Training epoch 499, calculated loss: 0.13838941320395895\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "nn.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model loss\n",
    "\n",
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 94%\n",
      "Test accuracy is 68%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn.accuracy(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {}%\".format(nn.accuracy(ytest, test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Banknote Authentication Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe\n",
    "\n",
    "banknote_df = pd.read_csv('Data/data_banknote_authentication.txt', sep = ',', names = ['variance', 'skewness', 'kurtosis', 'entropy', 'classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "banknote_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance          0\n",
       "skewness          0\n",
       "kurtosis          0\n",
       "entropy           0\n",
       "classification    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "banknote_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance          float64\n",
       "skewness          float64\n",
       "kurtosis          float64\n",
       "entropy           float64\n",
       "classification      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "banknote_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X2 = banknote_df.drop(columns = ['classification'])\n",
    "\n",
    "y_label2 = banknote_df['classification'].values.reshape(X2.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain2, Xtest2, ytrain2, ytest2 = train_test_split(X2, y_label2, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (1097, 4)\n",
      "Shape of test set is (275, 4)\n",
      "Shape of train label is (1097, 1)\n",
      "Shape of test labels is (275, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale Data\n",
    "\n",
    "sc.fit(Xtrain2)\n",
    "Xtrain2 = sc.transform(Xtrain2)\n",
    "Xtest2 = sc.transform(Xtest2)\n",
    "\n",
    "print(f\"Shape of train set is {Xtrain2.shape}\")\n",
    "print(f\"Shape of test set is {Xtest2.shape}\")\n",
    "print(f\"Shape of train label is {ytrain2.shape}\")\n",
    "print(f\"Shape of test labels is {ytest2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new neural network model with layers corresponding to input dimensions of 4\n",
    "\n",
    "nn2 = NeuralNet(layers=[4, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, calculated loss: 1.5543208674243605\n",
      "Training epoch 1, calculated loss: 1.0206359831199538\n",
      "Training epoch 2, calculated loss: 1.0457455064409347\n",
      "Training epoch 3, calculated loss: 0.68995373571536\n",
      "Training epoch 4, calculated loss: 0.6397701113244398\n",
      "Training epoch 5, calculated loss: 0.541329566993813\n",
      "Training epoch 6, calculated loss: 0.4659455632611285\n",
      "Training epoch 7, calculated loss: 0.3925161037720746\n",
      "Training epoch 8, calculated loss: 0.3346998923982945\n",
      "Training epoch 9, calculated loss: 0.29078823986940117\n",
      "Training epoch 10, calculated loss: 0.2562930989024405\n",
      "Training epoch 11, calculated loss: 0.2282851319826521\n",
      "Training epoch 12, calculated loss: 0.20488885261056808\n",
      "Training epoch 13, calculated loss: 0.1851624968060071\n",
      "Training epoch 14, calculated loss: 0.16851992047006334\n",
      "Training epoch 15, calculated loss: 0.15451366892854068\n",
      "Training epoch 16, calculated loss: 0.14271935539455005\n",
      "Training epoch 17, calculated loss: 0.1327178585577677\n",
      "Training epoch 18, calculated loss: 0.1242114119817114\n",
      "Training epoch 19, calculated loss: 0.11693653948818471\n",
      "Training epoch 20, calculated loss: 0.11065652425393313\n",
      "Training epoch 21, calculated loss: 0.10521455155627073\n",
      "Training epoch 22, calculated loss: 0.10046281052859143\n",
      "Training epoch 23, calculated loss: 0.09627467937115043\n",
      "Training epoch 24, calculated loss: 0.0925565607739147\n",
      "Training epoch 25, calculated loss: 0.08924572292820257\n",
      "Training epoch 26, calculated loss: 0.086269853956357\n",
      "Training epoch 27, calculated loss: 0.08357752772065255\n",
      "Training epoch 28, calculated loss: 0.08113397335597433\n",
      "Training epoch 29, calculated loss: 0.07891180142258644\n",
      "Training epoch 30, calculated loss: 0.07688088421106286\n",
      "Training epoch 31, calculated loss: 0.07500912241677232\n",
      "Training epoch 32, calculated loss: 0.07327944372750338\n",
      "Training epoch 33, calculated loss: 0.0716748154259605\n",
      "Training epoch 34, calculated loss: 0.07018269332773172\n",
      "Training epoch 35, calculated loss: 0.06879070248092817\n",
      "Training epoch 36, calculated loss: 0.0674882418533794\n",
      "Training epoch 37, calculated loss: 0.06626627301947387\n",
      "Training epoch 38, calculated loss: 0.06511619798812292\n",
      "Training epoch 39, calculated loss: 0.06403074115054787\n",
      "Training epoch 40, calculated loss: 0.0630067530724622\n",
      "Training epoch 41, calculated loss: 0.062038377206095965\n",
      "Training epoch 42, calculated loss: 0.06112257026858867\n",
      "Training epoch 43, calculated loss: 0.06025383015988778\n",
      "Training epoch 44, calculated loss: 0.059426603499905736\n",
      "Training epoch 45, calculated loss: 0.058641335949006036\n",
      "Training epoch 46, calculated loss: 0.05788980329925547\n",
      "Training epoch 47, calculated loss: 0.05717066311461107\n",
      "Training epoch 48, calculated loss: 0.05648121899775661\n",
      "Training epoch 49, calculated loss: 0.05581797061107715\n",
      "Training epoch 50, calculated loss: 0.055179673294310526\n",
      "Training epoch 51, calculated loss: 0.0545635054395891\n",
      "Training epoch 52, calculated loss: 0.053971044187427666\n",
      "Training epoch 53, calculated loss: 0.053399121615576375\n",
      "Training epoch 54, calculated loss: 0.05284763421797799\n",
      "Training epoch 55, calculated loss: 0.05231690899776951\n",
      "Training epoch 56, calculated loss: 0.05180354746070126\n",
      "Training epoch 57, calculated loss: 0.051306344978187146\n",
      "Training epoch 58, calculated loss: 0.05082316248311763\n",
      "Training epoch 59, calculated loss: 0.05035270501569593\n",
      "Training epoch 60, calculated loss: 0.04989467120753938\n",
      "Training epoch 61, calculated loss: 0.04944827600727314\n",
      "Training epoch 62, calculated loss: 0.04901420079460849\n",
      "Training epoch 63, calculated loss: 0.0485923925043825\n",
      "Training epoch 64, calculated loss: 0.048180721806699564\n",
      "Training epoch 65, calculated loss: 0.04777998415196984\n",
      "Training epoch 66, calculated loss: 0.04738861501936501\n",
      "Training epoch 67, calculated loss: 0.04700676105602569\n",
      "Training epoch 68, calculated loss: 0.0466338589631813\n",
      "Training epoch 69, calculated loss: 0.04626756950777004\n",
      "Training epoch 70, calculated loss: 0.04590865107738617\n",
      "Training epoch 71, calculated loss: 0.0455568603275545\n",
      "Training epoch 72, calculated loss: 0.045211871372383916\n",
      "Training epoch 73, calculated loss: 0.044873273421436786\n",
      "Training epoch 74, calculated loss: 0.044540955249159755\n",
      "Training epoch 75, calculated loss: 0.04421479609650159\n",
      "Training epoch 76, calculated loss: 0.04389400674958064\n",
      "Training epoch 77, calculated loss: 0.043578543277597526\n",
      "Training epoch 78, calculated loss: 0.04326822900578364\n",
      "Training epoch 79, calculated loss: 0.04296265039535079\n",
      "Training epoch 80, calculated loss: 0.04266276950719431\n",
      "Training epoch 81, calculated loss: 0.04236768834539286\n",
      "Training epoch 82, calculated loss: 0.04207690900328223\n",
      "Training epoch 83, calculated loss: 0.04179032054927925\n",
      "Training epoch 84, calculated loss: 0.041508026293390905\n",
      "Training epoch 85, calculated loss: 0.041229431199347605\n",
      "Training epoch 86, calculated loss: 0.04095482971965161\n",
      "Training epoch 87, calculated loss: 0.04068438729882652\n",
      "Training epoch 88, calculated loss: 0.04041748445608445\n",
      "Training epoch 89, calculated loss: 0.04015382058302206\n",
      "Training epoch 90, calculated loss: 0.03989351476831779\n",
      "Training epoch 91, calculated loss: 0.03963662029594152\n",
      "Training epoch 92, calculated loss: 0.03938327134298444\n",
      "Training epoch 93, calculated loss: 0.039133346475117466\n",
      "Training epoch 94, calculated loss: 0.03888655045579658\n",
      "Training epoch 95, calculated loss: 0.03864279982359351\n",
      "Training epoch 96, calculated loss: 0.03840189121100795\n",
      "Training epoch 97, calculated loss: 0.038163331959375435\n",
      "Training epoch 98, calculated loss: 0.03792738386710379\n",
      "Training epoch 99, calculated loss: 0.03769377383713873\n",
      "Training epoch 100, calculated loss: 0.037462689418297355\n",
      "Training epoch 101, calculated loss: 0.03723403693502435\n",
      "Training epoch 102, calculated loss: 0.03700769225878051\n",
      "Training epoch 103, calculated loss: 0.03678356514891007\n",
      "Training epoch 104, calculated loss: 0.03656173605209449\n",
      "Training epoch 105, calculated loss: 0.03634214315897342\n",
      "Training epoch 106, calculated loss: 0.036124582625798526\n",
      "Training epoch 107, calculated loss: 0.035908923336170705\n",
      "Training epoch 108, calculated loss: 0.0356951344847171\n",
      "Training epoch 109, calculated loss: 0.03548337916352172\n",
      "Training epoch 110, calculated loss: 0.035273534369115435\n",
      "Training epoch 111, calculated loss: 0.0350654350920906\n",
      "Training epoch 112, calculated loss: 0.034859169062589276\n",
      "Training epoch 113, calculated loss: 0.03465477518934033\n",
      "Training epoch 114, calculated loss: 0.034452217303191715\n",
      "Training epoch 115, calculated loss: 0.034251341906204574\n",
      "Training epoch 116, calculated loss: 0.03405209144837716\n",
      "Training epoch 117, calculated loss: 0.03385445319751331\n",
      "Training epoch 118, calculated loss: 0.033658426991012146\n",
      "Training epoch 119, calculated loss: 0.03346397588921645\n",
      "Training epoch 120, calculated loss: 0.03327123804660006\n",
      "Training epoch 121, calculated loss: 0.03308119855493258\n",
      "Training epoch 122, calculated loss: 0.0328944176523672\n",
      "Training epoch 123, calculated loss: 0.032710167420140206\n",
      "Training epoch 124, calculated loss: 0.03253565879959821\n",
      "Training epoch 125, calculated loss: 0.0323654759789022\n",
      "Training epoch 126, calculated loss: 0.032200233753250665\n",
      "Training epoch 127, calculated loss: 0.0320362383428567\n",
      "Training epoch 128, calculated loss: 0.031873527794667206\n",
      "Training epoch 129, calculated loss: 0.03171333682521618\n",
      "Training epoch 130, calculated loss: 0.031556156947719745\n",
      "Training epoch 131, calculated loss: 0.03140015770032732\n",
      "Training epoch 132, calculated loss: 0.031245408408042137\n",
      "Training epoch 133, calculated loss: 0.03109191352121647\n",
      "Training epoch 134, calculated loss: 0.030940204094052154\n",
      "Training epoch 135, calculated loss: 0.030790514266984977\n",
      "Training epoch 136, calculated loss: 0.030642029313758302\n",
      "Training epoch 137, calculated loss: 0.030494727417446726\n",
      "Training epoch 138, calculated loss: 0.03034856127636516\n",
      "Training epoch 139, calculated loss: 0.03020347071039323\n",
      "Training epoch 140, calculated loss: 0.030059420929505778\n",
      "Training epoch 141, calculated loss: 0.029916475764406165\n",
      "Training epoch 142, calculated loss: 0.029775021422206838\n",
      "Training epoch 143, calculated loss: 0.029635051917673818\n",
      "Training epoch 144, calculated loss: 0.02949613339139677\n",
      "Training epoch 145, calculated loss: 0.02935993840226471\n",
      "Training epoch 146, calculated loss: 0.02922833969513686\n",
      "Training epoch 147, calculated loss: 0.029097941234055902\n",
      "Training epoch 148, calculated loss: 0.028969798883189152\n",
      "Training epoch 149, calculated loss: 0.028848024305662412\n",
      "Training epoch 150, calculated loss: 0.028727262587807886\n",
      "Training epoch 151, calculated loss: 0.028607510565560833\n",
      "Training epoch 152, calculated loss: 0.028489185575254688\n",
      "Training epoch 153, calculated loss: 0.028377281942022336\n",
      "Training epoch 154, calculated loss: 0.028269220143985804\n",
      "Training epoch 155, calculated loss: 0.028162084411865917\n",
      "Training epoch 156, calculated loss: 0.028055871156658148\n",
      "Training epoch 157, calculated loss: 0.027950567489597563\n",
      "Training epoch 158, calculated loss: 0.02784614561575677\n",
      "Training epoch 159, calculated loss: 0.027742585946377997\n",
      "Training epoch 160, calculated loss: 0.027639864357811548\n",
      "Training epoch 161, calculated loss: 0.027537967615568898\n",
      "Training epoch 162, calculated loss: 0.027436898940108888\n",
      "Training epoch 163, calculated loss: 0.02733553830780845\n",
      "Training epoch 164, calculated loss: 0.02722675155685308\n",
      "Training epoch 165, calculated loss: 0.027118949229569714\n",
      "Training epoch 166, calculated loss: 0.027012104301842855\n",
      "Training epoch 167, calculated loss: 0.026906031501376302\n",
      "Training epoch 168, calculated loss: 0.026800709318339073\n",
      "Training epoch 169, calculated loss: 0.026696134748914416\n",
      "Training epoch 170, calculated loss: 0.026592336827085995\n",
      "Training epoch 171, calculated loss: 0.026490421784158523\n",
      "Training epoch 172, calculated loss: 0.026390662564046114\n",
      "Training epoch 173, calculated loss: 0.026291641973936186\n",
      "Training epoch 174, calculated loss: 0.026193262835695307\n",
      "Training epoch 175, calculated loss: 0.026095783196075414\n",
      "Training epoch 176, calculated loss: 0.02599903307711625\n",
      "Training epoch 177, calculated loss: 0.02590335043068263\n",
      "Training epoch 178, calculated loss: 0.025809522796089182\n",
      "Training epoch 179, calculated loss: 0.025716387716753292\n",
      "Training epoch 180, calculated loss: 0.02562391062653899\n",
      "Training epoch 181, calculated loss: 0.025535863574221718\n",
      "Training epoch 182, calculated loss: 0.0254517669312273\n",
      "Training epoch 183, calculated loss: 0.025369094884654924\n",
      "Training epoch 184, calculated loss: 0.025287875945016483\n",
      "Training epoch 185, calculated loss: 0.02520919821321161\n",
      "Training epoch 186, calculated loss: 0.025131155061378512\n",
      "Training epoch 187, calculated loss: 0.025053776873631514\n",
      "Training epoch 188, calculated loss: 0.02497699879850671\n",
      "Training epoch 189, calculated loss: 0.024900811382452148\n",
      "Training epoch 190, calculated loss: 0.024825196493763895\n",
      "Training epoch 191, calculated loss: 0.024750203250312897\n",
      "Training epoch 192, calculated loss: 0.024675755312424136\n",
      "Training epoch 193, calculated loss: 0.0246018101911868\n",
      "Training epoch 194, calculated loss: 0.024528412424500098\n",
      "Training epoch 195, calculated loss: 0.024455563550236755\n",
      "Training epoch 196, calculated loss: 0.024383256049459078\n",
      "Training epoch 197, calculated loss: 0.024311482464479794\n",
      "Training epoch 198, calculated loss: 0.02424029131600126\n",
      "Training epoch 199, calculated loss: 0.024169598157041484\n",
      "Training epoch 200, calculated loss: 0.024099418847350257\n",
      "Training epoch 201, calculated loss: 0.024029715734076598\n",
      "Training epoch 202, calculated loss: 0.023960476706554865\n",
      "Training epoch 203, calculated loss: 0.02389175714727621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 204, calculated loss: 0.023823533863734196\n",
      "Training epoch 205, calculated loss: 0.023755818941390325\n",
      "Training epoch 206, calculated loss: 0.02368865603602478\n",
      "Training epoch 207, calculated loss: 0.023621971869010196\n",
      "Training epoch 208, calculated loss: 0.02355576090957149\n",
      "Training epoch 209, calculated loss: 0.023490019750692922\n",
      "Training epoch 210, calculated loss: 0.02342474204123094\n",
      "Training epoch 211, calculated loss: 0.023359920265250397\n",
      "Training epoch 212, calculated loss: 0.02329555815918436\n",
      "Training epoch 213, calculated loss: 0.023231615511551312\n",
      "Training epoch 214, calculated loss: 0.023168167189470932\n",
      "Training epoch 215, calculated loss: 0.023106503740087346\n",
      "Training epoch 216, calculated loss: 0.023045262943421428\n",
      "Training epoch 217, calculated loss: 0.022984457449207463\n",
      "Training epoch 218, calculated loss: 0.022924086456384756\n",
      "Training epoch 219, calculated loss: 0.02286443977695804\n",
      "Training epoch 220, calculated loss: 0.02280572172555671\n",
      "Training epoch 221, calculated loss: 0.0227474077251105\n",
      "Training epoch 222, calculated loss: 0.022689477924454853\n",
      "Training epoch 223, calculated loss: 0.022631933175149947\n",
      "Training epoch 224, calculated loss: 0.02257474729379584\n",
      "Training epoch 225, calculated loss: 0.02251793322395557\n",
      "Training epoch 226, calculated loss: 0.02246149483281463\n",
      "Training epoch 227, calculated loss: 0.022405433208505115\n",
      "Training epoch 228, calculated loss: 0.022349741194258147\n",
      "Training epoch 229, calculated loss: 0.022294395850001023\n",
      "Training epoch 230, calculated loss: 0.02223941082052468\n",
      "Training epoch 231, calculated loss: 0.022185074002067543\n",
      "Training epoch 232, calculated loss: 0.022131642425037668\n",
      "Training epoch 233, calculated loss: 0.022078571450039428\n",
      "Training epoch 234, calculated loss: 0.02202585954728002\n",
      "Training epoch 235, calculated loss: 0.021973566337548836\n",
      "Training epoch 236, calculated loss: 0.021921986342531407\n",
      "Training epoch 237, calculated loss: 0.021870729922543308\n",
      "Training epoch 238, calculated loss: 0.021819795248452657\n",
      "Training epoch 239, calculated loss: 0.02176917289007299\n",
      "Training epoch 240, calculated loss: 0.021718871369279004\n",
      "Training epoch 241, calculated loss: 0.021668887503075624\n",
      "Training epoch 242, calculated loss: 0.021619218162804186\n",
      "Training epoch 243, calculated loss: 0.021569860213154584\n",
      "Training epoch 244, calculated loss: 0.021520809638471017\n",
      "Training epoch 245, calculated loss: 0.021472057204516306\n",
      "Training epoch 246, calculated loss: 0.021423631068818255\n",
      "Training epoch 247, calculated loss: 0.02137550013346932\n",
      "Training epoch 248, calculated loss: 0.02132765548280479\n",
      "Training epoch 249, calculated loss: 0.02128010485405213\n",
      "Training epoch 250, calculated loss: 0.02123284548711692\n",
      "Training epoch 251, calculated loss: 0.021186781607609013\n",
      "Training epoch 252, calculated loss: 0.021141733105287296\n",
      "Training epoch 253, calculated loss: 0.021097234128283016\n",
      "Training epoch 254, calculated loss: 0.02105349638515682\n",
      "Training epoch 255, calculated loss: 0.021010083801861694\n",
      "Training epoch 256, calculated loss: 0.02096707065064579\n",
      "Training epoch 257, calculated loss: 0.020924282671709576\n",
      "Training epoch 258, calculated loss: 0.020881732591795914\n",
      "Training epoch 259, calculated loss: 0.02083941797737827\n",
      "Training epoch 260, calculated loss: 0.020797336249471478\n",
      "Training epoch 261, calculated loss: 0.02075548494001586\n",
      "Training epoch 262, calculated loss: 0.020713861690447574\n",
      "Training epoch 263, calculated loss: 0.02067246471571141\n",
      "Training epoch 264, calculated loss: 0.020631294105844807\n",
      "Training epoch 265, calculated loss: 0.020590343323235658\n",
      "Training epoch 266, calculated loss: 0.02054959718955501\n",
      "Training epoch 267, calculated loss: 0.02050906791176532\n",
      "Training epoch 268, calculated loss: 0.02046875447928981\n",
      "Training epoch 269, calculated loss: 0.02042865518683383\n",
      "Training epoch 270, calculated loss: 0.02038876527021554\n",
      "Training epoch 271, calculated loss: 0.020349080353884982\n",
      "Training epoch 272, calculated loss: 0.020309602096758778\n",
      "Training epoch 273, calculated loss: 0.020270327237427506\n",
      "Training epoch 274, calculated loss: 0.020231249089619385\n",
      "Training epoch 275, calculated loss: 0.020192395220794416\n",
      "Training epoch 276, calculated loss: 0.02015378568914743\n",
      "Training epoch 277, calculated loss: 0.020115372937426216\n",
      "Training epoch 278, calculated loss: 0.02007715507788205\n",
      "Training epoch 279, calculated loss: 0.020039130259555525\n",
      "Training epoch 280, calculated loss: 0.020001296627910566\n",
      "Training epoch 281, calculated loss: 0.019963727902689618\n",
      "Training epoch 282, calculated loss: 0.019926469696129937\n",
      "Training epoch 283, calculated loss: 0.019889399057232634\n",
      "Training epoch 284, calculated loss: 0.01985250867954502\n",
      "Training epoch 285, calculated loss: 0.01981578902672583\n",
      "Training epoch 286, calculated loss: 0.019779244544301487\n",
      "Training epoch 287, calculated loss: 0.01974287469919798\n",
      "Training epoch 288, calculated loss: 0.019706668368943078\n",
      "Training epoch 289, calculated loss: 0.01967061229883547\n",
      "Training epoch 290, calculated loss: 0.01963472790177622\n",
      "Training epoch 291, calculated loss: 0.019599015432734384\n",
      "Training epoch 292, calculated loss: 0.01956352693739724\n",
      "Training epoch 293, calculated loss: 0.01952820511825909\n",
      "Training epoch 294, calculated loss: 0.019493048446728743\n",
      "Training epoch 295, calculated loss: 0.01945805525046133\n",
      "Training epoch 296, calculated loss: 0.01942322399259875\n",
      "Training epoch 297, calculated loss: 0.019388552764505806\n",
      "Training epoch 298, calculated loss: 0.01935403388231458\n",
      "Training epoch 299, calculated loss: 0.01931965884117488\n",
      "Training epoch 300, calculated loss: 0.019285439891646914\n",
      "Training epoch 301, calculated loss: 0.019251376014061048\n",
      "Training epoch 302, calculated loss: 0.019217466591435518\n",
      "Training epoch 303, calculated loss: 0.019183708937534107\n",
      "Training epoch 304, calculated loss: 0.019150101708966392\n",
      "Training epoch 305, calculated loss: 0.019116643422541553\n",
      "Training epoch 306, calculated loss: 0.019083332714025326\n",
      "Training epoch 307, calculated loss: 0.01905016833109527\n",
      "Training epoch 308, calculated loss: 0.019017148767328167\n",
      "Training epoch 309, calculated loss: 0.018984331600369872\n",
      "Training epoch 310, calculated loss: 0.01895173337927134\n",
      "Training epoch 311, calculated loss: 0.018919273520084094\n",
      "Training epoch 312, calculated loss: 0.01888695576066753\n",
      "Training epoch 313, calculated loss: 0.01885478211541755\n",
      "Training epoch 314, calculated loss: 0.018822737788818468\n",
      "Training epoch 315, calculated loss: 0.018790876850025277\n",
      "Training epoch 316, calculated loss: 0.018759243829079793\n",
      "Training epoch 317, calculated loss: 0.01872774015249987\n",
      "Training epoch 318, calculated loss: 0.018696362957911668\n",
      "Training epoch 319, calculated loss: 0.01866509330963509\n",
      "Training epoch 320, calculated loss: 0.018633926944898675\n",
      "Training epoch 321, calculated loss: 0.01860288387934998\n",
      "Training epoch 322, calculated loss: 0.01857196461163533\n",
      "Training epoch 323, calculated loss: 0.018541167970029127\n",
      "Training epoch 324, calculated loss: 0.018510492766506743\n",
      "Training epoch 325, calculated loss: 0.018479937951811003\n",
      "Training epoch 326, calculated loss: 0.018449502414069962\n",
      "Training epoch 327, calculated loss: 0.018419185053160942\n",
      "Training epoch 328, calculated loss: 0.018388984780467292\n",
      "Training epoch 329, calculated loss: 0.018358900518674528\n",
      "Training epoch 330, calculated loss: 0.018328931201589113\n",
      "Training epoch 331, calculated loss: 0.018299075773971402\n",
      "Training epoch 332, calculated loss: 0.018269333191377725\n",
      "Training epoch 333, calculated loss: 0.018239702420008955\n",
      "Training epoch 334, calculated loss: 0.018210183853748407\n",
      "Training epoch 335, calculated loss: 0.018180778734885623\n",
      "Training epoch 336, calculated loss: 0.0181514859466068\n",
      "Training epoch 337, calculated loss: 0.018122346460517375\n",
      "Training epoch 338, calculated loss: 0.018093313408349514\n",
      "Training epoch 339, calculated loss: 0.018064388249859332\n",
      "Training epoch 340, calculated loss: 0.018035575986085354\n",
      "Training epoch 341, calculated loss: 0.01800687472140334\n",
      "Training epoch 342, calculated loss: 0.01797828760862383\n",
      "Training epoch 343, calculated loss: 0.017949801530181893\n",
      "Training epoch 344, calculated loss: 0.01792141572979192\n",
      "Training epoch 345, calculated loss: 0.017893129274587822\n",
      "Training epoch 346, calculated loss: 0.017864941289680997\n",
      "Training epoch 347, calculated loss: 0.01783685088304736\n",
      "Training epoch 348, calculated loss: 0.017808857172134884\n",
      "Training epoch 349, calculated loss: 0.017780958674743066\n",
      "Training epoch 350, calculated loss: 0.017753154363452588\n",
      "Training epoch 351, calculated loss: 0.01772544420130223\n",
      "Training epoch 352, calculated loss: 0.017697827336810353\n",
      "Training epoch 353, calculated loss: 0.017670505139356477\n",
      "Training epoch 354, calculated loss: 0.01764312157620326\n",
      "Training epoch 355, calculated loss: 0.017615794666522796\n",
      "Training epoch 356, calculated loss: 0.01758877334368935\n",
      "Training epoch 357, calculated loss: 0.01756160260899918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 358, calculated loss: 0.017534618154305317\n",
      "Training epoch 359, calculated loss: 0.017507773942776382\n",
      "Training epoch 360, calculated loss: 0.017480852005937925\n",
      "Training epoch 361, calculated loss: 0.017454183100791016\n",
      "Training epoch 362, calculated loss: 0.017427496928137515\n",
      "Training epoch 363, calculated loss: 0.01740082613664551\n",
      "Training epoch 364, calculated loss: 0.017374472107268273\n",
      "Training epoch 365, calculated loss: 0.01734795801760313\n",
      "Training epoch 366, calculated loss: 0.017321616845881228\n",
      "Training epoch 367, calculated loss: 0.01729542228899278\n",
      "Training epoch 368, calculated loss: 0.017269151403135292\n",
      "Training epoch 369, calculated loss: 0.017243126761711802\n",
      "Training epoch 370, calculated loss: 0.01721709001590805\n",
      "Training epoch 371, calculated loss: 0.017191059244768204\n",
      "Training epoch 372, calculated loss: 0.0171653355779045\n",
      "Training epoch 373, calculated loss: 0.017139453513520645\n",
      "Training epoch 374, calculated loss: 0.0171137314795393\n",
      "Training epoch 375, calculated loss: 0.017088155704738508\n",
      "Training epoch 376, calculated loss: 0.017062552776003256\n",
      "Training epoch 377, calculated loss: 0.01703717154459236\n",
      "Training epoch 378, calculated loss: 0.017011786313254278\n",
      "Training epoch 379, calculated loss: 0.016986398382719318\n",
      "Training epoch 380, calculated loss: 0.016961299064915135\n",
      "Training epoch 381, calculated loss: 0.016936067231226582\n",
      "Training epoch 382, calculated loss: 0.01691094674869566\n",
      "Training epoch 383, calculated loss: 0.016886008516901266\n",
      "Training epoch 384, calculated loss: 0.016860971292149196\n",
      "Training epoch 385, calculated loss: 0.01683611417695145\n",
      "Training epoch 386, calculated loss: 0.01681132174293982\n",
      "Training epoch 387, calculated loss: 0.01678649017626591\n",
      "Training epoch 388, calculated loss: 0.016761889207288282\n",
      "Training epoch 389, calculated loss: 0.016737233985821144\n",
      "Training epoch 390, calculated loss: 0.016712593479934926\n",
      "Training epoch 391, calculated loss: 0.016688231134902066\n",
      "Training epoch 392, calculated loss: 0.016663729211307553\n",
      "Training epoch 393, calculated loss: 0.01663935266942703\n",
      "Training epoch 394, calculated loss: 0.016615125763297142\n",
      "Training epoch 395, calculated loss: 0.016590814529033538\n",
      "Training epoch 396, calculated loss: 0.016566666218162246\n",
      "Training epoch 397, calculated loss: 0.016542568404101638\n",
      "Training epoch 398, calculated loss: 0.016518436432256322\n",
      "Training epoch 399, calculated loss: 0.016494509895733853\n",
      "Training epoch 400, calculated loss: 0.016470529463194432\n",
      "Training epoch 401, calculated loss: 0.016446559918497566\n",
      "Training epoch 402, calculated loss: 0.016422838466575714\n",
      "Training epoch 403, calculated loss: 0.016398990347542435\n",
      "Training epoch 404, calculated loss: 0.016375232609171946\n",
      "Training epoch 405, calculated loss: 0.01635164757184817\n",
      "Training epoch 406, calculated loss: 0.01632796486793203\n",
      "Training epoch 407, calculated loss: 0.016304410694098145\n",
      "Training epoch 408, calculated loss: 0.016280942600118104\n",
      "Training epoch 409, calculated loss: 0.016257478639199432\n",
      "Training epoch 410, calculated loss: 0.01623417543191887\n",
      "Training epoch 411, calculated loss: 0.016210849784015102\n",
      "Training epoch 412, calculated loss: 0.016187593674417976\n",
      "Training epoch 413, calculated loss: 0.01616442671750274\n",
      "Training epoch 414, calculated loss: 0.01614138714754772\n",
      "Training epoch 415, calculated loss: 0.01611828421367016\n",
      "Training epoch 416, calculated loss: 0.01609538633272783\n",
      "Training epoch 417, calculated loss: 0.01607245424551852\n",
      "Training epoch 418, calculated loss: 0.016049515297991753\n",
      "Training epoch 419, calculated loss: 0.016026772812110287\n",
      "Training epoch 420, calculated loss: 0.016003974410484165\n",
      "Training epoch 421, calculated loss: 0.01598122937285836\n",
      "Training epoch 422, calculated loss: 0.015958581203497874\n",
      "Training epoch 423, calculated loss: 0.015935996147799513\n",
      "Training epoch 424, calculated loss: 0.015913348680122976\n",
      "Training epoch 425, calculated loss: 0.015891002990864034\n",
      "Training epoch 426, calculated loss: 0.01586883105921396\n",
      "Training epoch 427, calculated loss: 0.015846671267835684\n",
      "Training epoch 428, calculated loss: 0.015824614083511875\n",
      "Training epoch 429, calculated loss: 0.015802601238421792\n",
      "Training epoch 430, calculated loss: 0.015780578292568057\n",
      "Training epoch 431, calculated loss: 0.015758642009637423\n",
      "Training epoch 432, calculated loss: 0.0157368177463153\n",
      "Training epoch 433, calculated loss: 0.01571487717244905\n",
      "Training epoch 434, calculated loss: 0.015693144401486798\n",
      "Training epoch 435, calculated loss: 0.0156714024601336\n",
      "Training epoch 436, calculated loss: 0.01564965718696547\n",
      "Training epoch 437, calculated loss: 0.01562799610545084\n",
      "Training epoch 438, calculated loss: 0.015606401550491589\n",
      "Training epoch 439, calculated loss: 0.01558484269601838\n",
      "Training epoch 440, calculated loss: 0.01556324910988694\n",
      "Training epoch 441, calculated loss: 0.015541846564335906\n",
      "Training epoch 442, calculated loss: 0.015520395000633826\n",
      "Training epoch 443, calculated loss: 0.01549896292420112\n",
      "Training epoch 444, calculated loss: 0.015477738434365033\n",
      "Training epoch 445, calculated loss: 0.015456451643058622\n",
      "Training epoch 446, calculated loss: 0.015435144268066293\n",
      "Training epoch 447, calculated loss: 0.015414030828237238\n",
      "Training epoch 448, calculated loss: 0.015392880378944332\n",
      "Training epoch 449, calculated loss: 0.015371699997255843\n",
      "Training epoch 450, calculated loss: 0.015350777775014758\n",
      "Training epoch 451, calculated loss: 0.01532985346038134\n",
      "Training epoch 452, calculated loss: 0.015308877608725546\n",
      "Training epoch 453, calculated loss: 0.015288066735668472\n",
      "Training epoch 454, calculated loss: 0.015267312552867986\n",
      "Training epoch 455, calculated loss: 0.015246479688444984\n",
      "Training epoch 456, calculated loss: 0.015225740138539464\n",
      "Training epoch 457, calculated loss: 0.015205136577414973\n",
      "Training epoch 458, calculated loss: 0.015184409081631182\n",
      "Training epoch 459, calculated loss: 0.01516372133056533\n",
      "Training epoch 460, calculated loss: 0.015143236011201027\n",
      "Training epoch 461, calculated loss: 0.015122625647630292\n",
      "Training epoch 462, calculated loss: 0.01510203096718133\n",
      "Training epoch 463, calculated loss: 0.01508156553376889\n",
      "Training epoch 464, calculated loss: 0.01506112028061662\n",
      "Training epoch 465, calculated loss: 0.015040623655817115\n",
      "Training epoch 466, calculated loss: 0.01502023340882088\n",
      "Training epoch 467, calculated loss: 0.014999864126617166\n",
      "Training epoch 468, calculated loss: 0.014979518283802541\n",
      "Training epoch 469, calculated loss: 0.014959208809232464\n",
      "Training epoch 470, calculated loss: 0.014938962000623479\n",
      "Training epoch 471, calculated loss: 0.014918670750383299\n",
      "Training epoch 472, calculated loss: 0.014898431477886132\n",
      "Training epoch 473, calculated loss: 0.01487834565419436\n",
      "Training epoch 474, calculated loss: 0.014858189172753956\n",
      "Training epoch 475, calculated loss: 0.01483858637358962\n",
      "Training epoch 476, calculated loss: 0.014819121424711272\n",
      "Training epoch 477, calculated loss: 0.014799598858749997\n",
      "Training epoch 478, calculated loss: 0.014780102467373048\n",
      "Training epoch 479, calculated loss: 0.014760734610105698\n",
      "Training epoch 480, calculated loss: 0.014741333240890721\n",
      "Training epoch 481, calculated loss: 0.014721925628113237\n",
      "Training epoch 482, calculated loss: 0.014702595955525666\n",
      "Training epoch 483, calculated loss: 0.01468331363495849\n",
      "Training epoch 484, calculated loss: 0.014663987657744671\n",
      "Training epoch 485, calculated loss: 0.01464470305266625\n",
      "Training epoch 486, calculated loss: 0.01462553212848643\n",
      "Training epoch 487, calculated loss: 0.014606324989534389\n",
      "Training epoch 488, calculated loss: 0.014587134029664316\n",
      "Training epoch 489, calculated loss: 0.014568042591314017\n",
      "Training epoch 490, calculated loss: 0.014548924830277485\n",
      "Training epoch 491, calculated loss: 0.014529812388646646\n",
      "Training epoch 492, calculated loss: 0.014510774305594302\n",
      "Training epoch 493, calculated loss: 0.014491769521271815\n",
      "Training epoch 494, calculated loss: 0.014472873689609119\n",
      "Training epoch 495, calculated loss: 0.014454969430756892\n",
      "Training epoch 496, calculated loss: 0.014437170641552787\n",
      "Training epoch 497, calculated loss: 0.014419318261127906\n",
      "Training epoch 498, calculated loss: 0.014401495725134454\n",
      "Training epoch 499, calculated loss: 0.014383760990065046\n"
     ]
    }
   ],
   "source": [
    "# Fit new model \n",
    "\n",
    "nn2.fit(Xtrain2, ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss \n",
    "\n",
    "nn2.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred2 = nn2.predict(Xtrain2)\n",
    "test_pred2 = nn2.predict(Xtest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 99%\n",
      "Test accuracy is 97%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn2.accuracy(ytrain2, train_pred2)))\n",
    "print(\"Test accuracy is {}%\".format(nn2.accuracy(ytest2, test_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sonar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe\n",
    "\n",
    "sonar_df = pd.read_csv('Data/sonar.all-data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "sonar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "     ..\n",
       "56    0\n",
       "57    0\n",
       "58    0\n",
       "59    0\n",
       "60    0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "sonar_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     float64\n",
       "1     float64\n",
       "2     float64\n",
       "3     float64\n",
       "4     float64\n",
       "       ...   \n",
       "56    float64\n",
       "57    float64\n",
       "58    float64\n",
       "59    float64\n",
       "60     object\n",
       "Length: 61, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "sonar_df.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
