{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from neural_net import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header names\n",
    "\n",
    "headers = ['age', 'sex','chest_pain','resting_blood_pressure',  \n",
    "        'serum_cholestoral', 'fasting_blood_sugar', 'resting_ecg_results',\n",
    "        'max_heart_rate_achieved', 'exercise_induced_angina', 'oldpeak',\"slope of the peak\",\n",
    "        'num_of_major_vessels','thal', 'heart_disease']\n",
    "\n",
    "# Make DF\n",
    "\n",
    "heart_df = pd.read_csv('heart.dat', sep = ' ', names = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        0\n",
       "sex                        0\n",
       "chest_pain                 0\n",
       "resting_blood_pressure     0\n",
       "serum_cholestoral          0\n",
       "fasting_blood_sugar        0\n",
       "resting_ecg_results        0\n",
       "max_heart_rate_achieved    0\n",
       "exercise_induced_angina    0\n",
       "oldpeak                    0\n",
       "slope of the peak          0\n",
       "num_of_major_vessels       0\n",
       "thal                       0\n",
       "heart_disease              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "heart_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        float64\n",
       "sex                        float64\n",
       "chest_pain                 float64\n",
       "resting_blood_pressure     float64\n",
       "serum_cholestoral          float64\n",
       "fasting_blood_sugar        float64\n",
       "resting_ecg_results        float64\n",
       "max_heart_rate_achieved    float64\n",
       "exercise_induced_angina    float64\n",
       "oldpeak                    float64\n",
       "slope of the peak          float64\n",
       "num_of_major_vessels       float64\n",
       "thal                       float64\n",
       "heart_disease                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "heart_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X = heart_df.drop(columns = ['heart_disease'])\n",
    "\n",
    "# Enumerate target class i.e. labels\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(1,0)\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(2,1)\n",
    "\n",
    "y_label = heart_df['heart_disease'].values.reshape(X.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (216, 13)\n",
      "Shape of test set is (54, 13)\n",
      "Shape of train label is (216, 1)\n",
      "Shape of test labels is (54, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale values\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(Xtrain)\n",
    "Xtrain = sc.transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)\n",
    "\n",
    "print(f\"Shape of train set is {Xtrain.shape}\")\n",
    "print(f\"Shape of test set is {Xtest.shape}\")\n",
    "print(f\"Shape of train label is {ytrain.shape}\")\n",
    "print(f\"Shape of test labels is {ytest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, calculated loss: 1.406818099455712\n",
      "Training epoch 1, calculated loss: 1.2283644538724772\n",
      "Training epoch 2, calculated loss: 1.1027579343396263\n",
      "Training epoch 3, calculated loss: 1.010277834670556\n",
      "Training epoch 4, calculated loss: 0.9375456811690593\n",
      "Training epoch 5, calculated loss: 0.8771342674193834\n",
      "Training epoch 6, calculated loss: 0.8256029193292977\n",
      "Training epoch 7, calculated loss: 0.7809976438171825\n",
      "Training epoch 8, calculated loss: 0.7419983603962279\n",
      "Training epoch 9, calculated loss: 0.707447080573352\n",
      "Training epoch 10, calculated loss: 0.6761308099914313\n",
      "Training epoch 11, calculated loss: 0.6479719529677453\n",
      "Training epoch 12, calculated loss: 0.6225758410329949\n",
      "Training epoch 13, calculated loss: 0.5995873203513066\n",
      "Training epoch 14, calculated loss: 0.5787978131048888\n",
      "Training epoch 15, calculated loss: 0.5599182115487729\n",
      "Training epoch 16, calculated loss: 0.5427118970422696\n",
      "Training epoch 17, calculated loss: 0.5270878681118275\n",
      "Training epoch 18, calculated loss: 0.5127870376789314\n",
      "Training epoch 19, calculated loss: 0.499702605855535\n",
      "Training epoch 20, calculated loss: 0.48738521253819006\n",
      "Training epoch 21, calculated loss: 0.476190151824692\n",
      "Training epoch 22, calculated loss: 0.4660017416103337\n",
      "Training epoch 23, calculated loss: 0.4566221760866736\n",
      "Training epoch 24, calculated loss: 0.44801274903895083\n",
      "Training epoch 25, calculated loss: 0.44006195321689057\n",
      "Training epoch 26, calculated loss: 0.432662096983625\n",
      "Training epoch 27, calculated loss: 0.42579673582802635\n",
      "Training epoch 28, calculated loss: 0.4193526960161447\n",
      "Training epoch 29, calculated loss: 0.41324847065923803\n",
      "Training epoch 30, calculated loss: 0.4074599294185932\n",
      "Training epoch 31, calculated loss: 0.40202222246799607\n",
      "Training epoch 32, calculated loss: 0.3968899842644129\n",
      "Training epoch 33, calculated loss: 0.3920397357291702\n",
      "Training epoch 34, calculated loss: 0.3874509214748642\n",
      "Training epoch 35, calculated loss: 0.3830958350064345\n",
      "Training epoch 36, calculated loss: 0.3789607677725741\n",
      "Training epoch 37, calculated loss: 0.37502934313790387\n",
      "Training epoch 38, calculated loss: 0.37128637914437745\n",
      "Training epoch 39, calculated loss: 0.36771249418561974\n",
      "Training epoch 40, calculated loss: 0.3642833861662177\n",
      "Training epoch 41, calculated loss: 0.3610015600693919\n",
      "Training epoch 42, calculated loss: 0.3578906297976741\n",
      "Training epoch 43, calculated loss: 0.3549156523498778\n",
      "Training epoch 44, calculated loss: 0.3520709025825898\n",
      "Training epoch 45, calculated loss: 0.3493598084388319\n",
      "Training epoch 46, calculated loss: 0.346768845132244\n",
      "Training epoch 47, calculated loss: 0.34431068879480353\n",
      "Training epoch 48, calculated loss: 0.34194282652162544\n",
      "Training epoch 49, calculated loss: 0.3396583732933476\n",
      "Training epoch 50, calculated loss: 0.3374581250738244\n",
      "Training epoch 51, calculated loss: 0.33534760607313313\n",
      "Training epoch 52, calculated loss: 0.33330547606352195\n",
      "Training epoch 53, calculated loss: 0.33133938501962\n",
      "Training epoch 54, calculated loss: 0.32943711682743476\n",
      "Training epoch 55, calculated loss: 0.327561226258419\n",
      "Training epoch 56, calculated loss: 0.32575478917638145\n",
      "Training epoch 57, calculated loss: 0.3240014979718713\n",
      "Training epoch 58, calculated loss: 0.3222911256958769\n",
      "Training epoch 59, calculated loss: 0.320614686718432\n",
      "Training epoch 60, calculated loss: 0.3189950287443454\n",
      "Training epoch 61, calculated loss: 0.31743139477363524\n",
      "Training epoch 62, calculated loss: 0.315898744016534\n",
      "Training epoch 63, calculated loss: 0.3144261330659574\n",
      "Training epoch 64, calculated loss: 0.312979010473956\n",
      "Training epoch 65, calculated loss: 0.3115704557826012\n",
      "Training epoch 66, calculated loss: 0.3102387909138682\n",
      "Training epoch 67, calculated loss: 0.30892274240067386\n",
      "Training epoch 68, calculated loss: 0.30765010587991815\n",
      "Training epoch 69, calculated loss: 0.30639677395985704\n",
      "Training epoch 70, calculated loss: 0.3052113102149649\n",
      "Training epoch 71, calculated loss: 0.3040603570147317\n",
      "Training epoch 72, calculated loss: 0.30293845881919423\n",
      "Training epoch 73, calculated loss: 0.30183492009912677\n",
      "Training epoch 74, calculated loss: 0.3007443230734181\n",
      "Training epoch 75, calculated loss: 0.29967130343718507\n",
      "Training epoch 76, calculated loss: 0.29862341684204396\n",
      "Training epoch 77, calculated loss: 0.2975953115831883\n",
      "Training epoch 78, calculated loss: 0.2965805948342925\n",
      "Training epoch 79, calculated loss: 0.2955858072889425\n",
      "Training epoch 80, calculated loss: 0.2946025425015101\n",
      "Training epoch 81, calculated loss: 0.2936528622103815\n",
      "Training epoch 82, calculated loss: 0.29270959325424567\n",
      "Training epoch 83, calculated loss: 0.2917867375977857\n",
      "Training epoch 84, calculated loss: 0.2908766664032826\n",
      "Training epoch 85, calculated loss: 0.2899858794918911\n",
      "Training epoch 86, calculated loss: 0.2891217726542763\n",
      "Training epoch 87, calculated loss: 0.2882534060741006\n",
      "Training epoch 88, calculated loss: 0.28741232695069635\n",
      "Training epoch 89, calculated loss: 0.28658222439112885\n",
      "Training epoch 90, calculated loss: 0.2857393049629605\n",
      "Training epoch 91, calculated loss: 0.28488377530196873\n",
      "Training epoch 92, calculated loss: 0.2840357881439885\n",
      "Training epoch 93, calculated loss: 0.2831916166264467\n",
      "Training epoch 94, calculated loss: 0.2823680182360841\n",
      "Training epoch 95, calculated loss: 0.28153740084295914\n",
      "Training epoch 96, calculated loss: 0.28071874261248086\n",
      "Training epoch 97, calculated loss: 0.2799062326813389\n",
      "Training epoch 98, calculated loss: 0.2790974743022777\n",
      "Training epoch 99, calculated loss: 0.2783107364057317\n",
      "Training epoch 100, calculated loss: 0.27752267595856\n",
      "Training epoch 101, calculated loss: 0.276746158103033\n",
      "Training epoch 102, calculated loss: 0.27598304715732486\n",
      "Training epoch 103, calculated loss: 0.27526560371594744\n",
      "Training epoch 104, calculated loss: 0.2745564389923945\n",
      "Training epoch 105, calculated loss: 0.27386237629456256\n",
      "Training epoch 106, calculated loss: 0.27319294620008727\n",
      "Training epoch 107, calculated loss: 0.27252215813264585\n",
      "Training epoch 108, calculated loss: 0.2718580260151962\n",
      "Training epoch 109, calculated loss: 0.2711923110045857\n",
      "Training epoch 110, calculated loss: 0.2705296337982729\n",
      "Training epoch 111, calculated loss: 0.2698683935245679\n",
      "Training epoch 112, calculated loss: 0.269213933482101\n",
      "Training epoch 113, calculated loss: 0.2685728735027599\n",
      "Training epoch 114, calculated loss: 0.2679320976157896\n",
      "Training epoch 115, calculated loss: 0.26729387112097147\n",
      "Training epoch 116, calculated loss: 0.26666744357147076\n",
      "Training epoch 117, calculated loss: 0.2660492049364576\n",
      "Training epoch 118, calculated loss: 0.2654278058608459\n",
      "Training epoch 119, calculated loss: 0.26481833920220205\n",
      "Training epoch 120, calculated loss: 0.2642025845392992\n",
      "Training epoch 121, calculated loss: 0.2635980803620852\n",
      "Training epoch 122, calculated loss: 0.2629979179693585\n",
      "Training epoch 123, calculated loss: 0.26240444035825994\n",
      "Training epoch 124, calculated loss: 0.2618081094455079\n",
      "Training epoch 125, calculated loss: 0.26122323147646254\n",
      "Training epoch 126, calculated loss: 0.26063438451572873\n",
      "Training epoch 127, calculated loss: 0.2600492974146751\n",
      "Training epoch 128, calculated loss: 0.25946706587215085\n",
      "Training epoch 129, calculated loss: 0.2589003214821959\n",
      "Training epoch 130, calculated loss: 0.25832673077991153\n",
      "Training epoch 131, calculated loss: 0.25775349782981116\n",
      "Training epoch 132, calculated loss: 0.2571809593347549\n",
      "Training epoch 133, calculated loss: 0.2566217662792042\n",
      "Training epoch 134, calculated loss: 0.25605310226066813\n",
      "Training epoch 135, calculated loss: 0.2554915633487888\n",
      "Training epoch 136, calculated loss: 0.2549334344446226\n",
      "Training epoch 137, calculated loss: 0.25437693001749573\n",
      "Training epoch 138, calculated loss: 0.25382471688608255\n",
      "Training epoch 139, calculated loss: 0.25327245432645695\n",
      "Training epoch 140, calculated loss: 0.2527279179406165\n",
      "Training epoch 141, calculated loss: 0.25217915831365895\n",
      "Training epoch 142, calculated loss: 0.25163781252928474\n",
      "Training epoch 143, calculated loss: 0.2510910290687681\n",
      "Training epoch 144, calculated loss: 0.2505554057293129\n",
      "Training epoch 145, calculated loss: 0.25001024432803864\n",
      "Training epoch 146, calculated loss: 0.24947413070800084\n",
      "Training epoch 147, calculated loss: 0.24894197281573963\n",
      "Training epoch 148, calculated loss: 0.24841204559725721\n",
      "Training epoch 149, calculated loss: 0.2478847909568514\n",
      "Training epoch 150, calculated loss: 0.24735737248299336\n",
      "Training epoch 151, calculated loss: 0.2468379362565074\n",
      "Training epoch 152, calculated loss: 0.2463162713651728\n",
      "Training epoch 153, calculated loss: 0.24580155619808317\n",
      "Training epoch 154, calculated loss: 0.2452840582774365\n",
      "Training epoch 155, calculated loss: 0.24476858805520996\n",
      "Training epoch 156, calculated loss: 0.2442588084173961\n",
      "Training epoch 157, calculated loss: 0.2437435362084511\n",
      "Training epoch 158, calculated loss: 0.24323462312043742\n",
      "Training epoch 159, calculated loss: 0.24272893998407646\n",
      "Training epoch 160, calculated loss: 0.24223230619843128\n",
      "Training epoch 161, calculated loss: 0.24174067957462392\n",
      "Training epoch 162, calculated loss: 0.2412492025963735\n",
      "Training epoch 163, calculated loss: 0.24075542394516838\n",
      "Training epoch 164, calculated loss: 0.24026088887203656\n",
      "Training epoch 165, calculated loss: 0.23977173927195933\n",
      "Training epoch 166, calculated loss: 0.2392809939288868\n",
      "Training epoch 167, calculated loss: 0.23879307544349948\n",
      "Training epoch 168, calculated loss: 0.23831239789946876\n",
      "Training epoch 169, calculated loss: 0.2378248066660743\n",
      "Training epoch 170, calculated loss: 0.23734146228200687\n",
      "Training epoch 171, calculated loss: 0.23685621606243795\n",
      "Training epoch 172, calculated loss: 0.2363680485279514\n",
      "Training epoch 173, calculated loss: 0.23587090742674685\n",
      "Training epoch 174, calculated loss: 0.23537863633274222\n",
      "Training epoch 175, calculated loss: 0.2348903195279528\n",
      "Training epoch 176, calculated loss: 0.2344042567609047\n",
      "Training epoch 177, calculated loss: 0.23391175624870872\n",
      "Training epoch 178, calculated loss: 0.23343017770514024\n",
      "Training epoch 179, calculated loss: 0.23293822165842706\n",
      "Training epoch 180, calculated loss: 0.2324608315998643\n",
      "Training epoch 181, calculated loss: 0.23197788724357066\n",
      "Training epoch 182, calculated loss: 0.23149935858861018\n",
      "Training epoch 183, calculated loss: 0.2310124666397743\n",
      "Training epoch 184, calculated loss: 0.23053584748400333\n",
      "Training epoch 185, calculated loss: 0.23004806467142344\n",
      "Training epoch 186, calculated loss: 0.22957871113285785\n",
      "Training epoch 187, calculated loss: 0.22909778836401165\n",
      "Training epoch 188, calculated loss: 0.22862656328057268\n",
      "Training epoch 189, calculated loss: 0.22814334755455365\n",
      "Training epoch 190, calculated loss: 0.2276709835482682\n",
      "Training epoch 191, calculated loss: 0.22719038218911317\n",
      "Training epoch 192, calculated loss: 0.22672138605294875\n",
      "Training epoch 193, calculated loss: 0.22624451954760144\n",
      "Training epoch 194, calculated loss: 0.22578737541844868\n",
      "Training epoch 195, calculated loss: 0.2253161523571191\n",
      "Training epoch 196, calculated loss: 0.22485424516447283\n",
      "Training epoch 197, calculated loss: 0.22439134919349\n",
      "Training epoch 198, calculated loss: 0.2239280865753238\n",
      "Training epoch 199, calculated loss: 0.22346958764249777\n",
      "Training epoch 200, calculated loss: 0.22301092124771404\n",
      "Training epoch 201, calculated loss: 0.22255614764838536\n",
      "Training epoch 202, calculated loss: 0.22209400264876789\n",
      "Training epoch 203, calculated loss: 0.22164462420329273\n",
      "Training epoch 204, calculated loss: 0.22119825473660013\n",
      "Training epoch 205, calculated loss: 0.2207642169610009\n",
      "Training epoch 206, calculated loss: 0.22033261045718436\n",
      "Training epoch 207, calculated loss: 0.2199035580153558\n",
      "Training epoch 208, calculated loss: 0.2194706450749502\n",
      "Training epoch 209, calculated loss: 0.21904803815622734\n",
      "Training epoch 210, calculated loss: 0.21863185760205067\n",
      "Training epoch 211, calculated loss: 0.21819885067632558\n",
      "Training epoch 212, calculated loss: 0.21778363187425878\n",
      "Training epoch 213, calculated loss: 0.21735914641300724\n",
      "Training epoch 214, calculated loss: 0.2169282705446066\n",
      "Training epoch 215, calculated loss: 0.21651063110765484\n",
      "Training epoch 216, calculated loss: 0.21608748592860538\n",
      "Training epoch 217, calculated loss: 0.21566699955088842\n",
      "Training epoch 218, calculated loss: 0.21526024515912645\n",
      "Training epoch 219, calculated loss: 0.2148374729826649\n",
      "Training epoch 220, calculated loss: 0.21440432542116394\n",
      "Training epoch 221, calculated loss: 0.2139958620223014\n",
      "Training epoch 222, calculated loss: 0.2135679645465027\n",
      "Training epoch 223, calculated loss: 0.21315127653190946\n",
      "Training epoch 224, calculated loss: 0.2127425135582099\n",
      "Training epoch 225, calculated loss: 0.21232183920557945\n",
      "Training epoch 226, calculated loss: 0.2119010342251094\n",
      "Training epoch 227, calculated loss: 0.21148229859970416\n",
      "Training epoch 228, calculated loss: 0.21106058291309945\n",
      "Training epoch 229, calculated loss: 0.21065369209809542\n",
      "Training epoch 230, calculated loss: 0.21023967067439361\n",
      "Training epoch 231, calculated loss: 0.20982902672714915\n",
      "Training epoch 232, calculated loss: 0.20939613306028493\n",
      "Training epoch 233, calculated loss: 0.2089876071558346\n",
      "Training epoch 234, calculated loss: 0.20857927972805704\n",
      "Training epoch 235, calculated loss: 0.20816528682003066\n",
      "Training epoch 236, calculated loss: 0.2077553652163357\n",
      "Training epoch 237, calculated loss: 0.20733396932847467\n",
      "Training epoch 238, calculated loss: 0.20691243935101947\n",
      "Training epoch 239, calculated loss: 0.2064980819185977\n",
      "Training epoch 240, calculated loss: 0.20609397478307614\n",
      "Training epoch 241, calculated loss: 0.20566760276359936\n",
      "Training epoch 242, calculated loss: 0.20525538044164046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 243, calculated loss: 0.20484343749057887\n",
      "Training epoch 244, calculated loss: 0.20441927750730154\n",
      "Training epoch 245, calculated loss: 0.2040126296924069\n",
      "Training epoch 246, calculated loss: 0.20362076530831855\n",
      "Training epoch 247, calculated loss: 0.20319828652626068\n",
      "Training epoch 248, calculated loss: 0.20277780790284053\n",
      "Training epoch 249, calculated loss: 0.20236571298216968\n",
      "Training epoch 250, calculated loss: 0.20195238099992036\n",
      "Training epoch 251, calculated loss: 0.20156261838124942\n",
      "Training epoch 252, calculated loss: 0.20115952703475037\n",
      "Training epoch 253, calculated loss: 0.2007483563441138\n",
      "Training epoch 254, calculated loss: 0.2003377703110201\n",
      "Training epoch 255, calculated loss: 0.1999290356881397\n",
      "Training epoch 256, calculated loss: 0.19954320203371445\n",
      "Training epoch 257, calculated loss: 0.199182485115241\n",
      "Training epoch 258, calculated loss: 0.19880105043699905\n",
      "Training epoch 259, calculated loss: 0.19840192422729222\n",
      "Training epoch 260, calculated loss: 0.1980207292858923\n",
      "Training epoch 261, calculated loss: 0.19764848083957062\n",
      "Training epoch 262, calculated loss: 0.19726230645401954\n",
      "Training epoch 263, calculated loss: 0.19688050465637108\n",
      "Training epoch 264, calculated loss: 0.19651275770998625\n",
      "Training epoch 265, calculated loss: 0.19612384179538422\n",
      "Training epoch 266, calculated loss: 0.1957630147609834\n",
      "Training epoch 267, calculated loss: 0.1953722569566314\n",
      "Training epoch 268, calculated loss: 0.19498930212044344\n",
      "Training epoch 269, calculated loss: 0.19461641793214018\n",
      "Training epoch 270, calculated loss: 0.1942441575982174\n",
      "Training epoch 271, calculated loss: 0.19386863167293533\n",
      "Training epoch 272, calculated loss: 0.19348478056753204\n",
      "Training epoch 273, calculated loss: 0.19311064629426253\n",
      "Training epoch 274, calculated loss: 0.192726628477748\n",
      "Training epoch 275, calculated loss: 0.19235661948829577\n",
      "Training epoch 276, calculated loss: 0.19196502553151357\n",
      "Training epoch 277, calculated loss: 0.1915986397577637\n",
      "Training epoch 278, calculated loss: 0.19122341022500783\n",
      "Training epoch 279, calculated loss: 0.19083085175095132\n",
      "Training epoch 280, calculated loss: 0.19046145055031888\n",
      "Training epoch 281, calculated loss: 0.19009102370011108\n",
      "Training epoch 282, calculated loss: 0.18970648638050977\n",
      "Training epoch 283, calculated loss: 0.18934881202258527\n",
      "Training epoch 284, calculated loss: 0.1889738749204568\n",
      "Training epoch 285, calculated loss: 0.1886218398356439\n",
      "Training epoch 286, calculated loss: 0.18825106361899036\n",
      "Training epoch 287, calculated loss: 0.18786522308620968\n",
      "Training epoch 288, calculated loss: 0.1875090812628159\n",
      "Training epoch 289, calculated loss: 0.1871345473867985\n",
      "Training epoch 290, calculated loss: 0.18676501545272986\n",
      "Training epoch 291, calculated loss: 0.18641840187728403\n",
      "Training epoch 292, calculated loss: 0.18604482687305232\n",
      "Training epoch 293, calculated loss: 0.18568545639933703\n",
      "Training epoch 294, calculated loss: 0.18532975861852727\n",
      "Training epoch 295, calculated loss: 0.18498483777153138\n",
      "Training epoch 296, calculated loss: 0.18461185314995485\n",
      "Training epoch 297, calculated loss: 0.18425013332752715\n",
      "Training epoch 298, calculated loss: 0.18391152812839598\n",
      "Training epoch 299, calculated loss: 0.18355248732093069\n",
      "Training epoch 300, calculated loss: 0.18320601006803303\n",
      "Training epoch 301, calculated loss: 0.1828540115677848\n",
      "Training epoch 302, calculated loss: 0.18251686411492968\n",
      "Training epoch 303, calculated loss: 0.1821732197656081\n",
      "Training epoch 304, calculated loss: 0.1818170142445932\n",
      "Training epoch 305, calculated loss: 0.1814719297134536\n",
      "Training epoch 306, calculated loss: 0.18113902574112584\n",
      "Training epoch 307, calculated loss: 0.1807825447128808\n",
      "Training epoch 308, calculated loss: 0.18045659233714778\n",
      "Training epoch 309, calculated loss: 0.1801078197240961\n",
      "Training epoch 310, calculated loss: 0.17979673876733465\n",
      "Training epoch 311, calculated loss: 0.1794675592851685\n",
      "Training epoch 312, calculated loss: 0.1791372893996969\n",
      "Training epoch 313, calculated loss: 0.1788165049438514\n",
      "Training epoch 314, calculated loss: 0.17849558383987\n",
      "Training epoch 315, calculated loss: 0.17816066147475693\n",
      "Training epoch 316, calculated loss: 0.1778349304644559\n",
      "Training epoch 317, calculated loss: 0.17751311246325255\n",
      "Training epoch 318, calculated loss: 0.17719705052409218\n",
      "Training epoch 319, calculated loss: 0.17686148026632312\n",
      "Training epoch 320, calculated loss: 0.17655129211747075\n",
      "Training epoch 321, calculated loss: 0.17623781820387602\n",
      "Training epoch 322, calculated loss: 0.1759291444132664\n",
      "Training epoch 323, calculated loss: 0.1756326973705929\n",
      "Training epoch 324, calculated loss: 0.17534433945718303\n",
      "Training epoch 325, calculated loss: 0.17505326948737784\n",
      "Training epoch 326, calculated loss: 0.17476336279446636\n",
      "Training epoch 327, calculated loss: 0.17447883345769297\n",
      "Training epoch 328, calculated loss: 0.17419661358090496\n",
      "Training epoch 329, calculated loss: 0.1739487350853175\n",
      "Training epoch 330, calculated loss: 0.1736765846606785\n",
      "Training epoch 331, calculated loss: 0.1734207068236491\n",
      "Training epoch 332, calculated loss: 0.1731866336973259\n",
      "Training epoch 333, calculated loss: 0.17294227082277783\n",
      "Training epoch 334, calculated loss: 0.1726946777873684\n",
      "Training epoch 335, calculated loss: 0.17243647813121277\n",
      "Training epoch 336, calculated loss: 0.1721803136433498\n",
      "Training epoch 337, calculated loss: 0.17195028784810754\n",
      "Training epoch 338, calculated loss: 0.17169346297302743\n",
      "Training epoch 339, calculated loss: 0.17145706532276012\n",
      "Training epoch 340, calculated loss: 0.1711999783518241\n",
      "Training epoch 341, calculated loss: 0.17097264265344975\n",
      "Training epoch 342, calculated loss: 0.17072128054425517\n",
      "Training epoch 343, calculated loss: 0.17048209845709925\n",
      "Training epoch 344, calculated loss: 0.17023203315163624\n",
      "Training epoch 345, calculated loss: 0.1699941945008134\n",
      "Training epoch 346, calculated loss: 0.16974128122812893\n",
      "Training epoch 347, calculated loss: 0.16948323256936773\n",
      "Training epoch 348, calculated loss: 0.16924536060296808\n",
      "Training epoch 349, calculated loss: 0.16898732423520862\n",
      "Training epoch 350, calculated loss: 0.1687340466210543\n",
      "Training epoch 351, calculated loss: 0.16847927733185036\n",
      "Training epoch 352, calculated loss: 0.16825200969880444\n",
      "Training epoch 353, calculated loss: 0.1680184489296365\n",
      "Training epoch 354, calculated loss: 0.16775547357820084\n",
      "Training epoch 355, calculated loss: 0.16751159568829121\n",
      "Training epoch 356, calculated loss: 0.1672623117203635\n",
      "Training epoch 357, calculated loss: 0.1670342009015608\n",
      "Training epoch 358, calculated loss: 0.16680226621584215\n",
      "Training epoch 359, calculated loss: 0.16657145511818808\n",
      "Training epoch 360, calculated loss: 0.16630767040088318\n",
      "Training epoch 361, calculated loss: 0.16607419005800161\n",
      "Training epoch 362, calculated loss: 0.16585328429306176\n",
      "Training epoch 363, calculated loss: 0.16560557466739984\n",
      "Training epoch 364, calculated loss: 0.16536030056040155\n",
      "Training epoch 365, calculated loss: 0.16513041112805482\n",
      "Training epoch 366, calculated loss: 0.1648936309149001\n",
      "Training epoch 367, calculated loss: 0.16466072675561894\n",
      "Training epoch 368, calculated loss: 0.1644214020694981\n",
      "Training epoch 369, calculated loss: 0.16418212751306865\n",
      "Training epoch 370, calculated loss: 0.16394143349632975\n",
      "Training epoch 371, calculated loss: 0.16371446101932746\n",
      "Training epoch 372, calculated loss: 0.16349024136128779\n",
      "Training epoch 373, calculated loss: 0.16324980532824224\n",
      "Training epoch 374, calculated loss: 0.16301616856081388\n",
      "Training epoch 375, calculated loss: 0.16277348779936215\n",
      "Training epoch 376, calculated loss: 0.16255532024251432\n",
      "Training epoch 377, calculated loss: 0.16231595238518823\n",
      "Training epoch 378, calculated loss: 0.1620866097438773\n",
      "Training epoch 379, calculated loss: 0.16184450053069116\n",
      "Training epoch 380, calculated loss: 0.16163600750490625\n",
      "Training epoch 381, calculated loss: 0.16140096811266103\n",
      "Training epoch 382, calculated loss: 0.16116446442739135\n",
      "Training epoch 383, calculated loss: 0.16093534478141389\n",
      "Training epoch 384, calculated loss: 0.1607157263221618\n",
      "Training epoch 385, calculated loss: 0.1604797973286627\n",
      "Training epoch 386, calculated loss: 0.1602524754063829\n",
      "Training epoch 387, calculated loss: 0.16002321456542906\n",
      "Training epoch 388, calculated loss: 0.15980338227215293\n",
      "Training epoch 389, calculated loss: 0.15958713099884503\n",
      "Training epoch 390, calculated loss: 0.15938264923304102\n",
      "Training epoch 391, calculated loss: 0.15914168270242113\n",
      "Training epoch 392, calculated loss: 0.1589254563886855\n",
      "Training epoch 393, calculated loss: 0.15871048884938777\n",
      "Training epoch 394, calculated loss: 0.15849417766186147\n",
      "Training epoch 395, calculated loss: 0.158271723475272\n",
      "Training epoch 396, calculated loss: 0.15804571410781076\n",
      "Training epoch 397, calculated loss: 0.15784562631186577\n",
      "Training epoch 398, calculated loss: 0.1576369728677055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 399, calculated loss: 0.15742515984689748\n",
      "Training epoch 400, calculated loss: 0.1571858211718213\n",
      "Training epoch 401, calculated loss: 0.15698599481324108\n",
      "Training epoch 402, calculated loss: 0.1567750978752191\n",
      "Training epoch 403, calculated loss: 0.15655796423517826\n",
      "Training epoch 404, calculated loss: 0.1563291245160032\n",
      "Training epoch 405, calculated loss: 0.1561085345953448\n",
      "Training epoch 406, calculated loss: 0.155913303447273\n",
      "Training epoch 407, calculated loss: 0.15569104338576284\n",
      "Training epoch 408, calculated loss: 0.1554782958364362\n",
      "Training epoch 409, calculated loss: 0.15524938642490338\n",
      "Training epoch 410, calculated loss: 0.1550566188530461\n",
      "Training epoch 411, calculated loss: 0.15484971009912843\n",
      "Training epoch 412, calculated loss: 0.15462897713674464\n",
      "Training epoch 413, calculated loss: 0.1543972762265905\n",
      "Training epoch 414, calculated loss: 0.1542018293446751\n",
      "Training epoch 415, calculated loss: 0.15398717441734588\n",
      "Training epoch 416, calculated loss: 0.1537806086744322\n",
      "Training epoch 417, calculated loss: 0.15354170684750246\n",
      "Training epoch 418, calculated loss: 0.15335972380353524\n",
      "Training epoch 419, calculated loss: 0.15314548771175726\n",
      "Training epoch 420, calculated loss: 0.15292860566390828\n",
      "Training epoch 421, calculated loss: 0.15271025318384124\n",
      "Training epoch 422, calculated loss: 0.1525127899603239\n",
      "Training epoch 423, calculated loss: 0.15231247878553822\n",
      "Training epoch 424, calculated loss: 0.1521108764328087\n",
      "Training epoch 425, calculated loss: 0.15190066513571876\n",
      "Training epoch 426, calculated loss: 0.15174397750898871\n",
      "Training epoch 427, calculated loss: 0.15154324967660396\n",
      "Training epoch 428, calculated loss: 0.1513364468688277\n",
      "Training epoch 429, calculated loss: 0.1511480397586336\n",
      "Training epoch 430, calculated loss: 0.15095051070204293\n",
      "Training epoch 431, calculated loss: 0.1507677867515939\n",
      "Training epoch 432, calculated loss: 0.1505720638455779\n",
      "Training epoch 433, calculated loss: 0.1503764324283442\n",
      "Training epoch 434, calculated loss: 0.15019125186645826\n",
      "Training epoch 435, calculated loss: 0.14999649628648593\n",
      "Training epoch 436, calculated loss: 0.14981817237114212\n",
      "Training epoch 437, calculated loss: 0.14960372682190795\n",
      "Training epoch 438, calculated loss: 0.14943510557256834\n",
      "Training epoch 439, calculated loss: 0.14923156198711446\n",
      "Training epoch 440, calculated loss: 0.1490323110302897\n",
      "Training epoch 441, calculated loss: 0.14883953673094974\n",
      "Training epoch 442, calculated loss: 0.1486518024178283\n",
      "Training epoch 443, calculated loss: 0.14846139440125317\n",
      "Training epoch 444, calculated loss: 0.14826404317333006\n",
      "Training epoch 445, calculated loss: 0.14807262470931376\n",
      "Training epoch 446, calculated loss: 0.14789467609569903\n",
      "Training epoch 447, calculated loss: 0.14768747847749986\n",
      "Training epoch 448, calculated loss: 0.1474960417560906\n",
      "Training epoch 449, calculated loss: 0.14730509278325882\n",
      "Training epoch 450, calculated loss: 0.1471359462572863\n",
      "Training epoch 451, calculated loss: 0.14693179436827272\n",
      "Training epoch 452, calculated loss: 0.1467399177914218\n",
      "Training epoch 453, calculated loss: 0.14655330684991322\n",
      "Training epoch 454, calculated loss: 0.14637761127055562\n",
      "Training epoch 455, calculated loss: 0.1461791588962648\n",
      "Training epoch 456, calculated loss: 0.14598535187828576\n",
      "Training epoch 457, calculated loss: 0.14580865595688222\n",
      "Training epoch 458, calculated loss: 0.14562088851220337\n",
      "Training epoch 459, calculated loss: 0.14544189874108715\n",
      "Training epoch 460, calculated loss: 0.14524241900311582\n",
      "Training epoch 461, calculated loss: 0.14507407066121558\n",
      "Training epoch 462, calculated loss: 0.14487802313960552\n",
      "Training epoch 463, calculated loss: 0.14469551592094915\n",
      "Training epoch 464, calculated loss: 0.14451702463635938\n",
      "Training epoch 465, calculated loss: 0.14433866494052291\n",
      "Training epoch 466, calculated loss: 0.14415349748542902\n",
      "Training epoch 467, calculated loss: 0.1439582078598358\n",
      "Training epoch 468, calculated loss: 0.14378872473525617\n",
      "Training epoch 469, calculated loss: 0.14360374420805855\n",
      "Training epoch 470, calculated loss: 0.14341147973412957\n",
      "Training epoch 471, calculated loss: 0.14322867211484686\n",
      "Training epoch 472, calculated loss: 0.14305041911098115\n",
      "Training epoch 473, calculated loss: 0.142869843422236\n",
      "Training epoch 474, calculated loss: 0.14268017204880018\n",
      "Training epoch 475, calculated loss: 0.1425192685560983\n",
      "Training epoch 476, calculated loss: 0.14233459141981308\n",
      "Training epoch 477, calculated loss: 0.1421460365936759\n",
      "Training epoch 478, calculated loss: 0.1419797122652719\n",
      "Training epoch 479, calculated loss: 0.14182147289761057\n",
      "Training epoch 480, calculated loss: 0.14163200080777402\n",
      "Training epoch 481, calculated loss: 0.14143658552482288\n",
      "Training epoch 482, calculated loss: 0.14128617243524833\n",
      "Training epoch 483, calculated loss: 0.14111372747242554\n",
      "Training epoch 484, calculated loss: 0.14092477685885882\n",
      "Training epoch 485, calculated loss: 0.1407490590506156\n",
      "Training epoch 486, calculated loss: 0.14058417728075964\n",
      "Training epoch 487, calculated loss: 0.14042727248020903\n",
      "Training epoch 488, calculated loss: 0.1402195545926085\n",
      "Training epoch 489, calculated loss: 0.14006221153078655\n",
      "Training epoch 490, calculated loss: 0.13990036718235757\n",
      "Training epoch 491, calculated loss: 0.13972298261865274\n",
      "Training epoch 492, calculated loss: 0.13955754921744454\n",
      "Training epoch 493, calculated loss: 0.1393711758206643\n",
      "Training epoch 494, calculated loss: 0.139227297141135\n",
      "Training epoch 495, calculated loss: 0.1390445106681748\n",
      "Training epoch 496, calculated loss: 0.1388813906825923\n",
      "Training epoch 497, calculated loss: 0.13870089357746185\n",
      "Training epoch 498, calculated loss: 0.13854056453284114\n",
      "Training epoch 499, calculated loss: 0.13838941320395895\n"
     ]
    }
   ],
   "source": [
    "nn.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 94\n",
      "Test accuracy is 68\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy is {}\".format(nn.accuracy(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {}\".format(nn.accuracy(ytest, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
