{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy-only 2-layer Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from neural_net import NeuralNet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using UCI Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header names\n",
    "\n",
    "headers = ['age', 'sex','chest_pain','resting_blood_pressure',  \n",
    "        'serum_cholestoral', 'fasting_blood_sugar', 'resting_ecg_results',\n",
    "        'max_heart_rate_achieved', 'exercise_induced_angina', 'oldpeak',\"slope of the peak\",\n",
    "        'num_of_major_vessels','thal', 'heart_disease']\n",
    "\n",
    "# Make DF\n",
    "\n",
    "heart_df = pd.read_csv('Data/heart.dat', sep = ' ', names = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        0\n",
       "sex                        0\n",
       "chest_pain                 0\n",
       "resting_blood_pressure     0\n",
       "serum_cholestoral          0\n",
       "fasting_blood_sugar        0\n",
       "resting_ecg_results        0\n",
       "max_heart_rate_achieved    0\n",
       "exercise_induced_angina    0\n",
       "oldpeak                    0\n",
       "slope of the peak          0\n",
       "num_of_major_vessels       0\n",
       "thal                       0\n",
       "heart_disease              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "heart_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        float64\n",
       "sex                        float64\n",
       "chest_pain                 float64\n",
       "resting_blood_pressure     float64\n",
       "serum_cholestoral          float64\n",
       "fasting_blood_sugar        float64\n",
       "resting_ecg_results        float64\n",
       "max_heart_rate_achieved    float64\n",
       "exercise_induced_angina    float64\n",
       "oldpeak                    float64\n",
       "slope of the peak          float64\n",
       "num_of_major_vessels       float64\n",
       "thal                       float64\n",
       "heart_disease                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "heart_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X = heart_df.drop(columns = ['heart_disease'])\n",
    "\n",
    "# Enumerate target class i.e. labels\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(1,0)\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(2,1)\n",
    "\n",
    "y_label = heart_df['heart_disease'].values.reshape(X.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (216, 13)\n",
      "Shape of test set is (54, 13)\n",
      "Shape of train label is (216, 1)\n",
      "Shape of test labels is (54, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale values\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(Xtrain)\n",
    "Xtrain = sc.transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)\n",
    "\n",
    "print(f\"Shape of train set is {Xtrain.shape}\")\n",
    "print(f\"Shape of test set is {Xtest.shape}\")\n",
    "print(f\"Shape of train label is {ytrain.shape}\")\n",
    "print(f\"Shape of test labels is {ytest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Use it to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class of Neural Net using default parameters\n",
    "\n",
    "nn = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1, calculated loss: 1.406818099455712\n",
      "Training epoch 2, calculated loss: 1.2283644538724772\n",
      "Training epoch 3, calculated loss: 1.1027579343396263\n",
      "Training epoch 4, calculated loss: 1.010277834670556\n",
      "Training epoch 5, calculated loss: 0.9375456811690593\n",
      "Training epoch 6, calculated loss: 0.8771342674193834\n",
      "Training epoch 7, calculated loss: 0.8256029193292977\n",
      "Training epoch 8, calculated loss: 0.7809976438171825\n",
      "Training epoch 9, calculated loss: 0.7419983603962279\n",
      "Training epoch 10, calculated loss: 0.707447080573352\n",
      "Training epoch 11, calculated loss: 0.6761308099914313\n",
      "Training epoch 12, calculated loss: 0.6479719529677453\n",
      "Training epoch 13, calculated loss: 0.6225758410329949\n",
      "Training epoch 14, calculated loss: 0.5995873203513066\n",
      "Training epoch 15, calculated loss: 0.5787978131048888\n",
      "Training epoch 16, calculated loss: 0.5599182115487729\n",
      "Training epoch 17, calculated loss: 0.5427118970422696\n",
      "Training epoch 18, calculated loss: 0.5270878681118275\n",
      "Training epoch 19, calculated loss: 0.5127870376789314\n",
      "Training epoch 20, calculated loss: 0.499702605855535\n",
      "Training epoch 21, calculated loss: 0.48738521253819006\n",
      "Training epoch 22, calculated loss: 0.476190151824692\n",
      "Training epoch 23, calculated loss: 0.4660017416103337\n",
      "Training epoch 24, calculated loss: 0.4566221760866736\n",
      "Training epoch 25, calculated loss: 0.44801274903895083\n",
      "Training epoch 26, calculated loss: 0.44006195321689057\n",
      "Training epoch 27, calculated loss: 0.432662096983625\n",
      "Training epoch 28, calculated loss: 0.42579673582802635\n",
      "Training epoch 29, calculated loss: 0.4193526960161447\n",
      "Training epoch 30, calculated loss: 0.41324847065923803\n",
      "Training epoch 31, calculated loss: 0.4074599294185932\n",
      "Training epoch 32, calculated loss: 0.40202222246799607\n",
      "Training epoch 33, calculated loss: 0.3968899842644129\n",
      "Training epoch 34, calculated loss: 0.3920397357291702\n",
      "Training epoch 35, calculated loss: 0.3874509214748642\n",
      "Training epoch 36, calculated loss: 0.3830958350064345\n",
      "Training epoch 37, calculated loss: 0.3789607677725741\n",
      "Training epoch 38, calculated loss: 0.37502934313790387\n",
      "Training epoch 39, calculated loss: 0.37128637914437745\n",
      "Training epoch 40, calculated loss: 0.36771249418561974\n",
      "Training epoch 41, calculated loss: 0.3642833861662177\n",
      "Training epoch 42, calculated loss: 0.3610015600693919\n",
      "Training epoch 43, calculated loss: 0.3578906297976741\n",
      "Training epoch 44, calculated loss: 0.3549156523498778\n",
      "Training epoch 45, calculated loss: 0.3520709025825898\n",
      "Training epoch 46, calculated loss: 0.3493598084388319\n",
      "Training epoch 47, calculated loss: 0.346768845132244\n",
      "Training epoch 48, calculated loss: 0.34431068879480353\n",
      "Training epoch 49, calculated loss: 0.34194282652162544\n",
      "Training epoch 50, calculated loss: 0.3396583732933476\n",
      "Training epoch 51, calculated loss: 0.3374581250738244\n",
      "Training epoch 52, calculated loss: 0.33534760607313313\n",
      "Training epoch 53, calculated loss: 0.33330547606352195\n",
      "Training epoch 54, calculated loss: 0.33133938501962\n",
      "Training epoch 55, calculated loss: 0.32943711682743476\n",
      "Training epoch 56, calculated loss: 0.327561226258419\n",
      "Training epoch 57, calculated loss: 0.32575478917638145\n",
      "Training epoch 58, calculated loss: 0.3240014979718713\n",
      "Training epoch 59, calculated loss: 0.3222911256958769\n",
      "Training epoch 60, calculated loss: 0.320614686718432\n",
      "Training epoch 61, calculated loss: 0.3189950287443454\n",
      "Training epoch 62, calculated loss: 0.31743139477363524\n",
      "Training epoch 63, calculated loss: 0.315898744016534\n",
      "Training epoch 64, calculated loss: 0.3144261330659574\n",
      "Training epoch 65, calculated loss: 0.312979010473956\n",
      "Training epoch 66, calculated loss: 0.3115704557826012\n",
      "Training epoch 67, calculated loss: 0.3102387909138682\n",
      "Training epoch 68, calculated loss: 0.30892274240067386\n",
      "Training epoch 69, calculated loss: 0.30765010587991815\n",
      "Training epoch 70, calculated loss: 0.30639677395985704\n",
      "Training epoch 71, calculated loss: 0.3052113102149649\n",
      "Training epoch 72, calculated loss: 0.3040603570147317\n",
      "Training epoch 73, calculated loss: 0.30293845881919423\n",
      "Training epoch 74, calculated loss: 0.30183492009912677\n",
      "Training epoch 75, calculated loss: 0.3007443230734181\n",
      "Training epoch 76, calculated loss: 0.29967130343718507\n",
      "Training epoch 77, calculated loss: 0.29862341684204396\n",
      "Training epoch 78, calculated loss: 0.2975953115831883\n",
      "Training epoch 79, calculated loss: 0.2965805948342925\n",
      "Training epoch 80, calculated loss: 0.2955858072889425\n",
      "Training epoch 81, calculated loss: 0.2946025425015101\n",
      "Training epoch 82, calculated loss: 0.2936528622103815\n",
      "Training epoch 83, calculated loss: 0.29270959325424567\n",
      "Training epoch 84, calculated loss: 0.2917867375977857\n",
      "Training epoch 85, calculated loss: 0.2908766664032826\n",
      "Training epoch 86, calculated loss: 0.2899858794918911\n",
      "Training epoch 87, calculated loss: 0.2891217726542763\n",
      "Training epoch 88, calculated loss: 0.2882534060741006\n",
      "Training epoch 89, calculated loss: 0.28741232695069635\n",
      "Training epoch 90, calculated loss: 0.28658222439112885\n",
      "Training epoch 91, calculated loss: 0.2857393049629605\n",
      "Training epoch 92, calculated loss: 0.28488377530196873\n",
      "Training epoch 93, calculated loss: 0.2840357881439885\n",
      "Training epoch 94, calculated loss: 0.2831916166264467\n",
      "Training epoch 95, calculated loss: 0.2823680182360841\n",
      "Training epoch 96, calculated loss: 0.28153740084295914\n",
      "Training epoch 97, calculated loss: 0.28071874261248086\n",
      "Training epoch 98, calculated loss: 0.2799062326813389\n",
      "Training epoch 99, calculated loss: 0.2790974743022777\n",
      "Training epoch 100, calculated loss: 0.2783107364057317\n",
      "Training epoch 101, calculated loss: 0.27752267595856\n",
      "Training epoch 102, calculated loss: 0.276746158103033\n",
      "Training epoch 103, calculated loss: 0.27598304715732486\n",
      "Training epoch 104, calculated loss: 0.27526560371594744\n",
      "Training epoch 105, calculated loss: 0.2745564389923945\n",
      "Training epoch 106, calculated loss: 0.27386237629456256\n",
      "Training epoch 107, calculated loss: 0.27319294620008727\n",
      "Training epoch 108, calculated loss: 0.27252215813264585\n",
      "Training epoch 109, calculated loss: 0.2718580260151962\n",
      "Training epoch 110, calculated loss: 0.2711923110045857\n",
      "Training epoch 111, calculated loss: 0.2705296337982729\n",
      "Training epoch 112, calculated loss: 0.2698683935245679\n",
      "Training epoch 113, calculated loss: 0.269213933482101\n",
      "Training epoch 114, calculated loss: 0.2685728735027599\n",
      "Training epoch 115, calculated loss: 0.2679320976157896\n",
      "Training epoch 116, calculated loss: 0.26729387112097147\n",
      "Training epoch 117, calculated loss: 0.26666744357147076\n",
      "Training epoch 118, calculated loss: 0.2660492049364576\n",
      "Training epoch 119, calculated loss: 0.2654278058608459\n",
      "Training epoch 120, calculated loss: 0.26481833920220205\n",
      "Training epoch 121, calculated loss: 0.2642025845392992\n",
      "Training epoch 122, calculated loss: 0.2635980803620852\n",
      "Training epoch 123, calculated loss: 0.2629979179693585\n",
      "Training epoch 124, calculated loss: 0.26240444035825994\n",
      "Training epoch 125, calculated loss: 0.2618081094455079\n",
      "Training epoch 126, calculated loss: 0.26122323147646254\n",
      "Training epoch 127, calculated loss: 0.26063438451572873\n",
      "Training epoch 128, calculated loss: 0.2600492974146751\n",
      "Training epoch 129, calculated loss: 0.25946706587215085\n",
      "Training epoch 130, calculated loss: 0.2589003214821959\n",
      "Training epoch 131, calculated loss: 0.25832673077991153\n",
      "Training epoch 132, calculated loss: 0.25775349782981116\n",
      "Training epoch 133, calculated loss: 0.2571809593347549\n",
      "Training epoch 134, calculated loss: 0.2566217662792042\n",
      "Training epoch 135, calculated loss: 0.25605310226066813\n",
      "Training epoch 136, calculated loss: 0.2554915633487888\n",
      "Training epoch 137, calculated loss: 0.2549334344446226\n",
      "Training epoch 138, calculated loss: 0.25437693001749573\n",
      "Training epoch 139, calculated loss: 0.25382471688608255\n",
      "Training epoch 140, calculated loss: 0.25327245432645695\n",
      "Training epoch 141, calculated loss: 0.2527279179406165\n",
      "Training epoch 142, calculated loss: 0.25217915831365895\n",
      "Training epoch 143, calculated loss: 0.25163781252928474\n",
      "Training epoch 144, calculated loss: 0.2510910290687681\n",
      "Training epoch 145, calculated loss: 0.2505554057293129\n",
      "Training epoch 146, calculated loss: 0.25001024432803864\n",
      "Training epoch 147, calculated loss: 0.24947413070800084\n",
      "Training epoch 148, calculated loss: 0.24894197281573963\n",
      "Training epoch 149, calculated loss: 0.24841204559725721\n",
      "Training epoch 150, calculated loss: 0.2478847909568514\n",
      "Training epoch 151, calculated loss: 0.24735737248299336\n",
      "Training epoch 152, calculated loss: 0.2468379362565074\n",
      "Training epoch 153, calculated loss: 0.2463162713651728\n",
      "Training epoch 154, calculated loss: 0.24580155619808317\n",
      "Training epoch 155, calculated loss: 0.2452840582774365\n",
      "Training epoch 156, calculated loss: 0.24476858805520996\n",
      "Training epoch 157, calculated loss: 0.2442588084173961\n",
      "Training epoch 158, calculated loss: 0.2437435362084511\n",
      "Training epoch 159, calculated loss: 0.24323462312043742\n",
      "Training epoch 160, calculated loss: 0.24272893998407646\n",
      "Training epoch 161, calculated loss: 0.24223230619843128\n",
      "Training epoch 162, calculated loss: 0.24174067957462392\n",
      "Training epoch 163, calculated loss: 0.2412492025963735\n",
      "Training epoch 164, calculated loss: 0.24075542394516838\n",
      "Training epoch 165, calculated loss: 0.24026088887203656\n",
      "Training epoch 166, calculated loss: 0.23977173927195933\n",
      "Training epoch 167, calculated loss: 0.2392809939288868\n",
      "Training epoch 168, calculated loss: 0.23879307544349948\n",
      "Training epoch 169, calculated loss: 0.23831239789946876\n",
      "Training epoch 170, calculated loss: 0.2378248066660743\n",
      "Training epoch 171, calculated loss: 0.23734146228200687\n",
      "Training epoch 172, calculated loss: 0.23685621606243795\n",
      "Training epoch 173, calculated loss: 0.2363680485279514\n",
      "Training epoch 174, calculated loss: 0.23587090742674685\n",
      "Training epoch 175, calculated loss: 0.23537863633274222\n",
      "Training epoch 176, calculated loss: 0.2348903195279528\n",
      "Training epoch 177, calculated loss: 0.2344042567609047\n",
      "Training epoch 178, calculated loss: 0.23391175624870872\n",
      "Training epoch 179, calculated loss: 0.23343017770514024\n",
      "Training epoch 180, calculated loss: 0.23293822165842706\n",
      "Training epoch 181, calculated loss: 0.2324608315998643\n",
      "Training epoch 182, calculated loss: 0.23197788724357066\n",
      "Training epoch 183, calculated loss: 0.23149935858861018\n",
      "Training epoch 184, calculated loss: 0.2310124666397743\n",
      "Training epoch 185, calculated loss: 0.23053584748400333\n",
      "Training epoch 186, calculated loss: 0.23004806467142344\n",
      "Training epoch 187, calculated loss: 0.22957871113285785\n",
      "Training epoch 188, calculated loss: 0.22909778836401165\n",
      "Training epoch 189, calculated loss: 0.22862656328057268\n",
      "Training epoch 190, calculated loss: 0.22814334755455365\n",
      "Training epoch 191, calculated loss: 0.2276709835482682\n",
      "Training epoch 192, calculated loss: 0.22719038218911317\n",
      "Training epoch 193, calculated loss: 0.22672138605294875\n",
      "Training epoch 194, calculated loss: 0.22624451954760144\n",
      "Training epoch 195, calculated loss: 0.22578737541844868\n",
      "Training epoch 196, calculated loss: 0.2253161523571191\n",
      "Training epoch 197, calculated loss: 0.22485424516447283\n",
      "Training epoch 198, calculated loss: 0.22439134919349\n",
      "Training epoch 199, calculated loss: 0.2239280865753238\n",
      "Training epoch 200, calculated loss: 0.22346958764249777\n",
      "Training epoch 201, calculated loss: 0.22301092124771404\n",
      "Training epoch 202, calculated loss: 0.22255614764838536\n",
      "Training epoch 203, calculated loss: 0.22209400264876789\n",
      "Training epoch 204, calculated loss: 0.22164462420329273\n",
      "Training epoch 205, calculated loss: 0.22119825473660013\n",
      "Training epoch 206, calculated loss: 0.2207642169610009\n",
      "Training epoch 207, calculated loss: 0.22033261045718436\n",
      "Training epoch 208, calculated loss: 0.2199035580153558\n",
      "Training epoch 209, calculated loss: 0.2194706450749502\n",
      "Training epoch 210, calculated loss: 0.21904803815622734\n",
      "Training epoch 211, calculated loss: 0.21863185760205067\n",
      "Training epoch 212, calculated loss: 0.21819885067632558\n",
      "Training epoch 213, calculated loss: 0.21778363187425878\n",
      "Training epoch 214, calculated loss: 0.21735914641300724\n",
      "Training epoch 215, calculated loss: 0.2169282705446066\n",
      "Training epoch 216, calculated loss: 0.21651063110765484\n",
      "Training epoch 217, calculated loss: 0.21608748592860538\n",
      "Training epoch 218, calculated loss: 0.21566699955088842\n",
      "Training epoch 219, calculated loss: 0.21526024515912645\n",
      "Training epoch 220, calculated loss: 0.2148374729826649\n",
      "Training epoch 221, calculated loss: 0.21440432542116394\n",
      "Training epoch 222, calculated loss: 0.2139958620223014\n",
      "Training epoch 223, calculated loss: 0.2135679645465027\n",
      "Training epoch 224, calculated loss: 0.21315127653190946\n",
      "Training epoch 225, calculated loss: 0.2127425135582099\n",
      "Training epoch 226, calculated loss: 0.21232183920557945\n",
      "Training epoch 227, calculated loss: 0.2119010342251094\n",
      "Training epoch 228, calculated loss: 0.21148229859970416\n",
      "Training epoch 229, calculated loss: 0.21106058291309945\n",
      "Training epoch 230, calculated loss: 0.21065369209809542\n",
      "Training epoch 231, calculated loss: 0.21023967067439361\n",
      "Training epoch 232, calculated loss: 0.20982902672714915\n",
      "Training epoch 233, calculated loss: 0.20939613306028493\n",
      "Training epoch 234, calculated loss: 0.2089876071558346\n",
      "Training epoch 235, calculated loss: 0.20857927972805704\n",
      "Training epoch 236, calculated loss: 0.20816528682003066\n",
      "Training epoch 237, calculated loss: 0.2077553652163357\n",
      "Training epoch 238, calculated loss: 0.20733396932847467\n",
      "Training epoch 239, calculated loss: 0.20691243935101947\n",
      "Training epoch 240, calculated loss: 0.2064980819185977\n",
      "Training epoch 241, calculated loss: 0.20609397478307614\n",
      "Training epoch 242, calculated loss: 0.20566760276359936\n",
      "Training epoch 243, calculated loss: 0.20525538044164046\n",
      "Training epoch 244, calculated loss: 0.20484343749057887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 245, calculated loss: 0.20441927750730154\n",
      "Training epoch 246, calculated loss: 0.2040126296924069\n",
      "Training epoch 247, calculated loss: 0.20362076530831855\n",
      "Training epoch 248, calculated loss: 0.20319828652626068\n",
      "Training epoch 249, calculated loss: 0.20277780790284053\n",
      "Training epoch 250, calculated loss: 0.20236571298216968\n",
      "Training epoch 251, calculated loss: 0.20195238099992036\n",
      "Training epoch 252, calculated loss: 0.20156261838124942\n",
      "Training epoch 253, calculated loss: 0.20115952703475037\n",
      "Training epoch 254, calculated loss: 0.2007483563441138\n",
      "Training epoch 255, calculated loss: 0.2003377703110201\n",
      "Training epoch 256, calculated loss: 0.1999290356881397\n",
      "Training epoch 257, calculated loss: 0.19954320203371445\n",
      "Training epoch 258, calculated loss: 0.199182485115241\n",
      "Training epoch 259, calculated loss: 0.19880105043699905\n",
      "Training epoch 260, calculated loss: 0.19840192422729222\n",
      "Training epoch 261, calculated loss: 0.1980207292858923\n",
      "Training epoch 262, calculated loss: 0.19764848083957062\n",
      "Training epoch 263, calculated loss: 0.19726230645401954\n",
      "Training epoch 264, calculated loss: 0.19688050465637108\n",
      "Training epoch 265, calculated loss: 0.19651275770998625\n",
      "Training epoch 266, calculated loss: 0.19612384179538422\n",
      "Training epoch 267, calculated loss: 0.1957630147609834\n",
      "Training epoch 268, calculated loss: 0.1953722569566314\n",
      "Training epoch 269, calculated loss: 0.19498930212044344\n",
      "Training epoch 270, calculated loss: 0.19461641793214018\n",
      "Training epoch 271, calculated loss: 0.1942441575982174\n",
      "Training epoch 272, calculated loss: 0.19386863167293533\n",
      "Training epoch 273, calculated loss: 0.19348478056753204\n",
      "Training epoch 274, calculated loss: 0.19311064629426253\n",
      "Training epoch 275, calculated loss: 0.192726628477748\n",
      "Training epoch 276, calculated loss: 0.19235661948829577\n",
      "Training epoch 277, calculated loss: 0.19196502553151357\n",
      "Training epoch 278, calculated loss: 0.1915986397577637\n",
      "Training epoch 279, calculated loss: 0.19122341022500783\n",
      "Training epoch 280, calculated loss: 0.19083085175095132\n",
      "Training epoch 281, calculated loss: 0.19046145055031888\n",
      "Training epoch 282, calculated loss: 0.19009102370011108\n",
      "Training epoch 283, calculated loss: 0.18970648638050977\n",
      "Training epoch 284, calculated loss: 0.18934881202258527\n",
      "Training epoch 285, calculated loss: 0.1889738749204568\n",
      "Training epoch 286, calculated loss: 0.1886218398356439\n",
      "Training epoch 287, calculated loss: 0.18825106361899036\n",
      "Training epoch 288, calculated loss: 0.18786522308620968\n",
      "Training epoch 289, calculated loss: 0.1875090812628159\n",
      "Training epoch 290, calculated loss: 0.1871345473867985\n",
      "Training epoch 291, calculated loss: 0.18676501545272986\n",
      "Training epoch 292, calculated loss: 0.18641840187728403\n",
      "Training epoch 293, calculated loss: 0.18604482687305232\n",
      "Training epoch 294, calculated loss: 0.18568545639933703\n",
      "Training epoch 295, calculated loss: 0.18532975861852727\n",
      "Training epoch 296, calculated loss: 0.18498483777153138\n",
      "Training epoch 297, calculated loss: 0.18461185314995485\n",
      "Training epoch 298, calculated loss: 0.18425013332752715\n",
      "Training epoch 299, calculated loss: 0.18391152812839598\n",
      "Training epoch 300, calculated loss: 0.18355248732093069\n",
      "Training epoch 301, calculated loss: 0.18320601006803303\n",
      "Training epoch 302, calculated loss: 0.1828540115677848\n",
      "Training epoch 303, calculated loss: 0.18251686411492968\n",
      "Training epoch 304, calculated loss: 0.1821732197656081\n",
      "Training epoch 305, calculated loss: 0.1818170142445932\n",
      "Training epoch 306, calculated loss: 0.1814719297134536\n",
      "Training epoch 307, calculated loss: 0.18113902574112584\n",
      "Training epoch 308, calculated loss: 0.1807825447128808\n",
      "Training epoch 309, calculated loss: 0.18045659233714778\n",
      "Training epoch 310, calculated loss: 0.1801078197240961\n",
      "Training epoch 311, calculated loss: 0.17979673876733465\n",
      "Training epoch 312, calculated loss: 0.1794675592851685\n",
      "Training epoch 313, calculated loss: 0.1791372893996969\n",
      "Training epoch 314, calculated loss: 0.1788165049438514\n",
      "Training epoch 315, calculated loss: 0.17849558383987\n",
      "Training epoch 316, calculated loss: 0.17816066147475693\n",
      "Training epoch 317, calculated loss: 0.1778349304644559\n",
      "Training epoch 318, calculated loss: 0.17751311246325255\n",
      "Training epoch 319, calculated loss: 0.17719705052409218\n",
      "Training epoch 320, calculated loss: 0.17686148026632312\n",
      "Training epoch 321, calculated loss: 0.17655129211747075\n",
      "Training epoch 322, calculated loss: 0.17623781820387602\n",
      "Training epoch 323, calculated loss: 0.1759291444132664\n",
      "Training epoch 324, calculated loss: 0.1756326973705929\n",
      "Training epoch 325, calculated loss: 0.17534433945718303\n",
      "Training epoch 326, calculated loss: 0.17505326948737784\n",
      "Training epoch 327, calculated loss: 0.17476336279446636\n",
      "Training epoch 328, calculated loss: 0.17447883345769297\n",
      "Training epoch 329, calculated loss: 0.17419661358090496\n",
      "Training epoch 330, calculated loss: 0.1739487350853175\n",
      "Training epoch 331, calculated loss: 0.1736765846606785\n",
      "Training epoch 332, calculated loss: 0.1734207068236491\n",
      "Training epoch 333, calculated loss: 0.1731866336973259\n",
      "Training epoch 334, calculated loss: 0.17294227082277783\n",
      "Training epoch 335, calculated loss: 0.1726946777873684\n",
      "Training epoch 336, calculated loss: 0.17243647813121277\n",
      "Training epoch 337, calculated loss: 0.1721803136433498\n",
      "Training epoch 338, calculated loss: 0.17195028784810754\n",
      "Training epoch 339, calculated loss: 0.17169346297302743\n",
      "Training epoch 340, calculated loss: 0.17145706532276012\n",
      "Training epoch 341, calculated loss: 0.1711999783518241\n",
      "Training epoch 342, calculated loss: 0.17097264265344975\n",
      "Training epoch 343, calculated loss: 0.17072128054425517\n",
      "Training epoch 344, calculated loss: 0.17048209845709925\n",
      "Training epoch 345, calculated loss: 0.17023203315163624\n",
      "Training epoch 346, calculated loss: 0.1699941945008134\n",
      "Training epoch 347, calculated loss: 0.16974128122812893\n",
      "Training epoch 348, calculated loss: 0.16948323256936773\n",
      "Training epoch 349, calculated loss: 0.16924536060296808\n",
      "Training epoch 350, calculated loss: 0.16898732423520862\n",
      "Training epoch 351, calculated loss: 0.1687340466210543\n",
      "Training epoch 352, calculated loss: 0.16847927733185036\n",
      "Training epoch 353, calculated loss: 0.16825200969880444\n",
      "Training epoch 354, calculated loss: 0.1680184489296365\n",
      "Training epoch 355, calculated loss: 0.16775547357820084\n",
      "Training epoch 356, calculated loss: 0.16751159568829121\n",
      "Training epoch 357, calculated loss: 0.1672623117203635\n",
      "Training epoch 358, calculated loss: 0.1670342009015608\n",
      "Training epoch 359, calculated loss: 0.16680226621584215\n",
      "Training epoch 360, calculated loss: 0.16657145511818808\n",
      "Training epoch 361, calculated loss: 0.16630767040088318\n",
      "Training epoch 362, calculated loss: 0.16607419005800161\n",
      "Training epoch 363, calculated loss: 0.16585328429306176\n",
      "Training epoch 364, calculated loss: 0.16560557466739984\n",
      "Training epoch 365, calculated loss: 0.16536030056040155\n",
      "Training epoch 366, calculated loss: 0.16513041112805482\n",
      "Training epoch 367, calculated loss: 0.1648936309149001\n",
      "Training epoch 368, calculated loss: 0.16466072675561894\n",
      "Training epoch 369, calculated loss: 0.1644214020694981\n",
      "Training epoch 370, calculated loss: 0.16418212751306865\n",
      "Training epoch 371, calculated loss: 0.16394143349632975\n",
      "Training epoch 372, calculated loss: 0.16371446101932746\n",
      "Training epoch 373, calculated loss: 0.16349024136128779\n",
      "Training epoch 374, calculated loss: 0.16324980532824224\n",
      "Training epoch 375, calculated loss: 0.16301616856081388\n",
      "Training epoch 376, calculated loss: 0.16277348779936215\n",
      "Training epoch 377, calculated loss: 0.16255532024251432\n",
      "Training epoch 378, calculated loss: 0.16231595238518823\n",
      "Training epoch 379, calculated loss: 0.1620866097438773\n",
      "Training epoch 380, calculated loss: 0.16184450053069116\n",
      "Training epoch 381, calculated loss: 0.16163600750490625\n",
      "Training epoch 382, calculated loss: 0.16140096811266103\n",
      "Training epoch 383, calculated loss: 0.16116446442739135\n",
      "Training epoch 384, calculated loss: 0.16093534478141389\n",
      "Training epoch 385, calculated loss: 0.1607157263221618\n",
      "Training epoch 386, calculated loss: 0.1604797973286627\n",
      "Training epoch 387, calculated loss: 0.1602524754063829\n",
      "Training epoch 388, calculated loss: 0.16002321456542906\n",
      "Training epoch 389, calculated loss: 0.15980338227215293\n",
      "Training epoch 390, calculated loss: 0.15958713099884503\n",
      "Training epoch 391, calculated loss: 0.15938264923304102\n",
      "Training epoch 392, calculated loss: 0.15914168270242113\n",
      "Training epoch 393, calculated loss: 0.1589254563886855\n",
      "Training epoch 394, calculated loss: 0.15871048884938777\n",
      "Training epoch 395, calculated loss: 0.15849417766186147\n",
      "Training epoch 396, calculated loss: 0.158271723475272\n",
      "Training epoch 397, calculated loss: 0.15804571410781076\n",
      "Training epoch 398, calculated loss: 0.15784562631186577\n",
      "Training epoch 399, calculated loss: 0.1576369728677055\n",
      "Training epoch 400, calculated loss: 0.15742515984689748\n",
      "Training epoch 401, calculated loss: 0.1571858211718213\n",
      "Training epoch 402, calculated loss: 0.15698599481324108\n",
      "Training epoch 403, calculated loss: 0.1567750978752191\n",
      "Training epoch 404, calculated loss: 0.15655796423517826\n",
      "Training epoch 405, calculated loss: 0.1563291245160032\n",
      "Training epoch 406, calculated loss: 0.1561085345953448\n",
      "Training epoch 407, calculated loss: 0.155913303447273\n",
      "Training epoch 408, calculated loss: 0.15569104338576284\n",
      "Training epoch 409, calculated loss: 0.1554782958364362\n",
      "Training epoch 410, calculated loss: 0.15524938642490338\n",
      "Training epoch 411, calculated loss: 0.1550566188530461\n",
      "Training epoch 412, calculated loss: 0.15484971009912843\n",
      "Training epoch 413, calculated loss: 0.15462897713674464\n",
      "Training epoch 414, calculated loss: 0.1543972762265905\n",
      "Training epoch 415, calculated loss: 0.1542018293446751\n",
      "Training epoch 416, calculated loss: 0.15398717441734588\n",
      "Training epoch 417, calculated loss: 0.1537806086744322\n",
      "Training epoch 418, calculated loss: 0.15354170684750246\n",
      "Training epoch 419, calculated loss: 0.15335972380353524\n",
      "Training epoch 420, calculated loss: 0.15314548771175726\n",
      "Training epoch 421, calculated loss: 0.15292860566390828\n",
      "Training epoch 422, calculated loss: 0.15271025318384124\n",
      "Training epoch 423, calculated loss: 0.1525127899603239\n",
      "Training epoch 424, calculated loss: 0.15231247878553822\n",
      "Training epoch 425, calculated loss: 0.1521108764328087\n",
      "Training epoch 426, calculated loss: 0.15190066513571876\n",
      "Training epoch 427, calculated loss: 0.15174397750898871\n",
      "Training epoch 428, calculated loss: 0.15154324967660396\n",
      "Training epoch 429, calculated loss: 0.1513364468688277\n",
      "Training epoch 430, calculated loss: 0.1511480397586336\n",
      "Training epoch 431, calculated loss: 0.15095051070204293\n",
      "Training epoch 432, calculated loss: 0.1507677867515939\n",
      "Training epoch 433, calculated loss: 0.1505720638455779\n",
      "Training epoch 434, calculated loss: 0.1503764324283442\n",
      "Training epoch 435, calculated loss: 0.15019125186645826\n",
      "Training epoch 436, calculated loss: 0.14999649628648593\n",
      "Training epoch 437, calculated loss: 0.14981817237114212\n",
      "Training epoch 438, calculated loss: 0.14960372682190795\n",
      "Training epoch 439, calculated loss: 0.14943510557256834\n",
      "Training epoch 440, calculated loss: 0.14923156198711446\n",
      "Training epoch 441, calculated loss: 0.1490323110302897\n",
      "Training epoch 442, calculated loss: 0.14883953673094974\n",
      "Training epoch 443, calculated loss: 0.1486518024178283\n",
      "Training epoch 444, calculated loss: 0.14846139440125317\n",
      "Training epoch 445, calculated loss: 0.14826404317333006\n",
      "Training epoch 446, calculated loss: 0.14807262470931376\n",
      "Training epoch 447, calculated loss: 0.14789467609569903\n",
      "Training epoch 448, calculated loss: 0.14768747847749986\n",
      "Training epoch 449, calculated loss: 0.1474960417560906\n",
      "Training epoch 450, calculated loss: 0.14730509278325882\n",
      "Training epoch 451, calculated loss: 0.1471359462572863\n",
      "Training epoch 452, calculated loss: 0.14693179436827272\n",
      "Training epoch 453, calculated loss: 0.1467399177914218\n",
      "Training epoch 454, calculated loss: 0.14655330684991322\n",
      "Training epoch 455, calculated loss: 0.14637761127055562\n",
      "Training epoch 456, calculated loss: 0.1461791588962648\n",
      "Training epoch 457, calculated loss: 0.14598535187828576\n",
      "Training epoch 458, calculated loss: 0.14580865595688222\n",
      "Training epoch 459, calculated loss: 0.14562088851220337\n",
      "Training epoch 460, calculated loss: 0.14544189874108715\n",
      "Training epoch 461, calculated loss: 0.14524241900311582\n",
      "Training epoch 462, calculated loss: 0.14507407066121558\n",
      "Training epoch 463, calculated loss: 0.14487802313960552\n",
      "Training epoch 464, calculated loss: 0.14469551592094915\n",
      "Training epoch 465, calculated loss: 0.14451702463635938\n",
      "Training epoch 466, calculated loss: 0.14433866494052291\n",
      "Training epoch 467, calculated loss: 0.14415349748542902\n",
      "Training epoch 468, calculated loss: 0.1439582078598358\n",
      "Training epoch 469, calculated loss: 0.14378872473525617\n",
      "Training epoch 470, calculated loss: 0.14360374420805855\n",
      "Training epoch 471, calculated loss: 0.14341147973412957\n",
      "Training epoch 472, calculated loss: 0.14322867211484686\n",
      "Training epoch 473, calculated loss: 0.14305041911098115\n",
      "Training epoch 474, calculated loss: 0.142869843422236\n",
      "Training epoch 475, calculated loss: 0.14268017204880018\n",
      "Training epoch 476, calculated loss: 0.1425192685560983\n",
      "Training epoch 477, calculated loss: 0.14233459141981308\n",
      "Training epoch 478, calculated loss: 0.1421460365936759\n",
      "Training epoch 479, calculated loss: 0.1419797122652719\n",
      "Training epoch 480, calculated loss: 0.14182147289761057\n",
      "Training epoch 481, calculated loss: 0.14163200080777402\n",
      "Training epoch 482, calculated loss: 0.14143658552482288\n",
      "Training epoch 483, calculated loss: 0.14128617243524833\n",
      "Training epoch 484, calculated loss: 0.14111372747242554\n",
      "Training epoch 485, calculated loss: 0.14092477685885882\n",
      "Training epoch 486, calculated loss: 0.1407490590506156\n",
      "Training epoch 487, calculated loss: 0.14058417728075964\n",
      "Training epoch 488, calculated loss: 0.14042727248020903\n",
      "Training epoch 489, calculated loss: 0.1402195545926085\n",
      "Training epoch 490, calculated loss: 0.14006221153078655\n",
      "Training epoch 491, calculated loss: 0.13990036718235757\n",
      "Training epoch 492, calculated loss: 0.13972298261865274\n",
      "Training epoch 493, calculated loss: 0.13955754921744454\n",
      "Training epoch 494, calculated loss: 0.1393711758206643\n",
      "Training epoch 495, calculated loss: 0.139227297141135\n",
      "Training epoch 496, calculated loss: 0.1390445106681748\n",
      "Training epoch 497, calculated loss: 0.1388813906825923\n",
      "Training epoch 498, calculated loss: 0.13870089357746185\n",
      "Training epoch 499, calculated loss: 0.13854056453284114\n",
      "Training epoch 500, calculated loss: 0.13838941320395895\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "nn.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model loss\n",
    "\n",
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 94%\n",
      "Test accuracy is 68%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn.accuracy(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {}%\".format(nn.accuracy(ytest, test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Banknote Authentication Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe\n",
    "\n",
    "banknote_df = pd.read_csv('Data/data_banknote_authentication.txt', sep = ',', names = ['variance', 'skewness', 'kurtosis', 'entropy', 'classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "banknote_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance          0\n",
       "skewness          0\n",
       "kurtosis          0\n",
       "entropy           0\n",
       "classification    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "banknote_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance          float64\n",
       "skewness          float64\n",
       "kurtosis          float64\n",
       "entropy           float64\n",
       "classification      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "banknote_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X2 = banknote_df.drop(columns = ['classification'])\n",
    "\n",
    "y_label2 = banknote_df['classification'].values.reshape(X2.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain2, Xtest2, ytrain2, ytest2 = train_test_split(X2, y_label2, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1097, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (1097, 4)\n",
      "Shape of test set is (275, 4)\n",
      "Shape of train label is (1097, 1)\n",
      "Shape of test labels is (275, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale Data\n",
    "\n",
    "sc.fit(Xtrain2)\n",
    "Xtrain2 = sc.transform(Xtrain2)\n",
    "Xtest2 = sc.transform(Xtest2)\n",
    "\n",
    "print(f\"Shape of train set is {Xtrain2.shape}\")\n",
    "print(f\"Shape of test set is {Xtest2.shape}\")\n",
    "print(f\"Shape of train label is {ytrain2.shape}\")\n",
    "print(f\"Shape of test labels is {ytest2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new neural network model with layers corresponding to input dimensions of 4\n",
    "\n",
    "nn2 = NeuralNet(layers=[4, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1, calculated loss: 1.5543208674243605\n",
      "Training epoch 2, calculated loss: 1.0206359831199538\n",
      "Training epoch 3, calculated loss: 1.0457455064409347\n",
      "Training epoch 4, calculated loss: 0.68995373571536\n",
      "Training epoch 5, calculated loss: 0.6397701113244398\n",
      "Training epoch 6, calculated loss: 0.541329566993813\n",
      "Training epoch 7, calculated loss: 0.4659455632611285\n",
      "Training epoch 8, calculated loss: 0.3925161037720746\n",
      "Training epoch 9, calculated loss: 0.3346998923982945\n",
      "Training epoch 10, calculated loss: 0.29078823986940117\n",
      "Training epoch 11, calculated loss: 0.2562930989024405\n",
      "Training epoch 12, calculated loss: 0.2282851319826521\n",
      "Training epoch 13, calculated loss: 0.20488885261056808\n",
      "Training epoch 14, calculated loss: 0.1851624968060071\n",
      "Training epoch 15, calculated loss: 0.16851992047006334\n",
      "Training epoch 16, calculated loss: 0.15451366892854068\n",
      "Training epoch 17, calculated loss: 0.14271935539455005\n",
      "Training epoch 18, calculated loss: 0.1327178585577677\n",
      "Training epoch 19, calculated loss: 0.1242114119817114\n",
      "Training epoch 20, calculated loss: 0.11693653948818471\n",
      "Training epoch 21, calculated loss: 0.11065652425393313\n",
      "Training epoch 22, calculated loss: 0.10521455155627073\n",
      "Training epoch 23, calculated loss: 0.10046281052859143\n",
      "Training epoch 24, calculated loss: 0.09627467937115043\n",
      "Training epoch 25, calculated loss: 0.0925565607739147\n",
      "Training epoch 26, calculated loss: 0.08924572292820257\n",
      "Training epoch 27, calculated loss: 0.086269853956357\n",
      "Training epoch 28, calculated loss: 0.08357752772065255\n",
      "Training epoch 29, calculated loss: 0.08113397335597433\n",
      "Training epoch 30, calculated loss: 0.07891180142258644\n",
      "Training epoch 31, calculated loss: 0.07688088421106286\n",
      "Training epoch 32, calculated loss: 0.07500912241677232\n",
      "Training epoch 33, calculated loss: 0.07327944372750338\n",
      "Training epoch 34, calculated loss: 0.0716748154259605\n",
      "Training epoch 35, calculated loss: 0.07018269332773172\n",
      "Training epoch 36, calculated loss: 0.06879070248092817\n",
      "Training epoch 37, calculated loss: 0.0674882418533794\n",
      "Training epoch 38, calculated loss: 0.06626627301947387\n",
      "Training epoch 39, calculated loss: 0.06511619798812292\n",
      "Training epoch 40, calculated loss: 0.06403074115054787\n",
      "Training epoch 41, calculated loss: 0.0630067530724622\n",
      "Training epoch 42, calculated loss: 0.062038377206095965\n",
      "Training epoch 43, calculated loss: 0.06112257026858867\n",
      "Training epoch 44, calculated loss: 0.06025383015988778\n",
      "Training epoch 45, calculated loss: 0.059426603499905736\n",
      "Training epoch 46, calculated loss: 0.058641335949006036\n",
      "Training epoch 47, calculated loss: 0.05788980329925547\n",
      "Training epoch 48, calculated loss: 0.05717066311461107\n",
      "Training epoch 49, calculated loss: 0.05648121899775661\n",
      "Training epoch 50, calculated loss: 0.05581797061107715\n",
      "Training epoch 51, calculated loss: 0.055179673294310526\n",
      "Training epoch 52, calculated loss: 0.0545635054395891\n",
      "Training epoch 53, calculated loss: 0.053971044187427666\n",
      "Training epoch 54, calculated loss: 0.053399121615576375\n",
      "Training epoch 55, calculated loss: 0.05284763421797799\n",
      "Training epoch 56, calculated loss: 0.05231690899776951\n",
      "Training epoch 57, calculated loss: 0.05180354746070126\n",
      "Training epoch 58, calculated loss: 0.051306344978187146\n",
      "Training epoch 59, calculated loss: 0.05082316248311763\n",
      "Training epoch 60, calculated loss: 0.05035270501569593\n",
      "Training epoch 61, calculated loss: 0.04989467120753938\n",
      "Training epoch 62, calculated loss: 0.04944827600727314\n",
      "Training epoch 63, calculated loss: 0.04901420079460849\n",
      "Training epoch 64, calculated loss: 0.0485923925043825\n",
      "Training epoch 65, calculated loss: 0.048180721806699564\n",
      "Training epoch 66, calculated loss: 0.04777998415196984\n",
      "Training epoch 67, calculated loss: 0.04738861501936501\n",
      "Training epoch 68, calculated loss: 0.04700676105602569\n",
      "Training epoch 69, calculated loss: 0.0466338589631813\n",
      "Training epoch 70, calculated loss: 0.04626756950777004\n",
      "Training epoch 71, calculated loss: 0.04590865107738617\n",
      "Training epoch 72, calculated loss: 0.0455568603275545\n",
      "Training epoch 73, calculated loss: 0.045211871372383916\n",
      "Training epoch 74, calculated loss: 0.044873273421436786\n",
      "Training epoch 75, calculated loss: 0.044540955249159755\n",
      "Training epoch 76, calculated loss: 0.04421479609650159\n",
      "Training epoch 77, calculated loss: 0.04389400674958064\n",
      "Training epoch 78, calculated loss: 0.043578543277597526\n",
      "Training epoch 79, calculated loss: 0.04326822900578364\n",
      "Training epoch 80, calculated loss: 0.04296265039535079\n",
      "Training epoch 81, calculated loss: 0.04266276950719431\n",
      "Training epoch 82, calculated loss: 0.04236768834539286\n",
      "Training epoch 83, calculated loss: 0.04207690900328223\n",
      "Training epoch 84, calculated loss: 0.04179032054927925\n",
      "Training epoch 85, calculated loss: 0.041508026293390905\n",
      "Training epoch 86, calculated loss: 0.041229431199347605\n",
      "Training epoch 87, calculated loss: 0.04095482971965161\n",
      "Training epoch 88, calculated loss: 0.04068438729882652\n",
      "Training epoch 89, calculated loss: 0.04041748445608445\n",
      "Training epoch 90, calculated loss: 0.04015382058302206\n",
      "Training epoch 91, calculated loss: 0.03989351476831779\n",
      "Training epoch 92, calculated loss: 0.03963662029594152\n",
      "Training epoch 93, calculated loss: 0.03938327134298444\n",
      "Training epoch 94, calculated loss: 0.039133346475117466\n",
      "Training epoch 95, calculated loss: 0.03888655045579658\n",
      "Training epoch 96, calculated loss: 0.03864279982359351\n",
      "Training epoch 97, calculated loss: 0.03840189121100795\n",
      "Training epoch 98, calculated loss: 0.038163331959375435\n",
      "Training epoch 99, calculated loss: 0.03792738386710379\n",
      "Training epoch 100, calculated loss: 0.03769377383713873\n",
      "Training epoch 101, calculated loss: 0.037462689418297355\n",
      "Training epoch 102, calculated loss: 0.03723403693502435\n",
      "Training epoch 103, calculated loss: 0.03700769225878051\n",
      "Training epoch 104, calculated loss: 0.03678356514891007\n",
      "Training epoch 105, calculated loss: 0.03656173605209449\n",
      "Training epoch 106, calculated loss: 0.03634214315897342\n",
      "Training epoch 107, calculated loss: 0.036124582625798526\n",
      "Training epoch 108, calculated loss: 0.035908923336170705\n",
      "Training epoch 109, calculated loss: 0.0356951344847171\n",
      "Training epoch 110, calculated loss: 0.03548337916352172\n",
      "Training epoch 111, calculated loss: 0.035273534369115435\n",
      "Training epoch 112, calculated loss: 0.0350654350920906\n",
      "Training epoch 113, calculated loss: 0.034859169062589276\n",
      "Training epoch 114, calculated loss: 0.03465477518934033\n",
      "Training epoch 115, calculated loss: 0.034452217303191715\n",
      "Training epoch 116, calculated loss: 0.034251341906204574\n",
      "Training epoch 117, calculated loss: 0.03405209144837716\n",
      "Training epoch 118, calculated loss: 0.03385445319751331\n",
      "Training epoch 119, calculated loss: 0.033658426991012146\n",
      "Training epoch 120, calculated loss: 0.03346397588921645\n",
      "Training epoch 121, calculated loss: 0.03327123804660006\n",
      "Training epoch 122, calculated loss: 0.03308119855493258\n",
      "Training epoch 123, calculated loss: 0.0328944176523672\n",
      "Training epoch 124, calculated loss: 0.032710167420140206\n",
      "Training epoch 125, calculated loss: 0.03253565879959821\n",
      "Training epoch 126, calculated loss: 0.0323654759789022\n",
      "Training epoch 127, calculated loss: 0.032200233753250665\n",
      "Training epoch 128, calculated loss: 0.0320362383428567\n",
      "Training epoch 129, calculated loss: 0.031873527794667206\n",
      "Training epoch 130, calculated loss: 0.03171333682521618\n",
      "Training epoch 131, calculated loss: 0.031556156947719745\n",
      "Training epoch 132, calculated loss: 0.03140015770032732\n",
      "Training epoch 133, calculated loss: 0.031245408408042137\n",
      "Training epoch 134, calculated loss: 0.03109191352121647\n",
      "Training epoch 135, calculated loss: 0.030940204094052154\n",
      "Training epoch 136, calculated loss: 0.030790514266984977\n",
      "Training epoch 137, calculated loss: 0.030642029313758302\n",
      "Training epoch 138, calculated loss: 0.030494727417446726\n",
      "Training epoch 139, calculated loss: 0.03034856127636516\n",
      "Training epoch 140, calculated loss: 0.03020347071039323\n",
      "Training epoch 141, calculated loss: 0.030059420929505778\n",
      "Training epoch 142, calculated loss: 0.029916475764406165\n",
      "Training epoch 143, calculated loss: 0.029775021422206838\n",
      "Training epoch 144, calculated loss: 0.029635051917673818\n",
      "Training epoch 145, calculated loss: 0.02949613339139677\n",
      "Training epoch 146, calculated loss: 0.02935993840226471\n",
      "Training epoch 147, calculated loss: 0.02922833969513686\n",
      "Training epoch 148, calculated loss: 0.029097941234055902\n",
      "Training epoch 149, calculated loss: 0.028969798883189152\n",
      "Training epoch 150, calculated loss: 0.028848024305662412\n",
      "Training epoch 151, calculated loss: 0.028727262587807886\n",
      "Training epoch 152, calculated loss: 0.028607510565560833\n",
      "Training epoch 153, calculated loss: 0.028489185575254688\n",
      "Training epoch 154, calculated loss: 0.028377281942022336\n",
      "Training epoch 155, calculated loss: 0.028269220143985804\n",
      "Training epoch 156, calculated loss: 0.028162084411865917\n",
      "Training epoch 157, calculated loss: 0.028055871156658148\n",
      "Training epoch 158, calculated loss: 0.027950567489597563\n",
      "Training epoch 159, calculated loss: 0.02784614561575677\n",
      "Training epoch 160, calculated loss: 0.027742585946377997\n",
      "Training epoch 161, calculated loss: 0.027639864357811548\n",
      "Training epoch 162, calculated loss: 0.027537967615568898\n",
      "Training epoch 163, calculated loss: 0.027436898940108888\n",
      "Training epoch 164, calculated loss: 0.02733553830780845\n",
      "Training epoch 165, calculated loss: 0.02722675155685308\n",
      "Training epoch 166, calculated loss: 0.027118949229569714\n",
      "Training epoch 167, calculated loss: 0.027012104301842855\n",
      "Training epoch 168, calculated loss: 0.026906031501376302\n",
      "Training epoch 169, calculated loss: 0.026800709318339073\n",
      "Training epoch 170, calculated loss: 0.026696134748914416\n",
      "Training epoch 171, calculated loss: 0.026592336827085995\n",
      "Training epoch 172, calculated loss: 0.026490421784158523\n",
      "Training epoch 173, calculated loss: 0.026390662564046114\n",
      "Training epoch 174, calculated loss: 0.026291641973936186\n",
      "Training epoch 175, calculated loss: 0.026193262835695307\n",
      "Training epoch 176, calculated loss: 0.026095783196075414\n",
      "Training epoch 177, calculated loss: 0.02599903307711625\n",
      "Training epoch 178, calculated loss: 0.02590335043068263\n",
      "Training epoch 179, calculated loss: 0.025809522796089182\n",
      "Training epoch 180, calculated loss: 0.025716387716753292\n",
      "Training epoch 181, calculated loss: 0.02562391062653899\n",
      "Training epoch 182, calculated loss: 0.025535863574221718\n",
      "Training epoch 183, calculated loss: 0.0254517669312273\n",
      "Training epoch 184, calculated loss: 0.025369094884654924\n",
      "Training epoch 185, calculated loss: 0.025287875945016483\n",
      "Training epoch 186, calculated loss: 0.02520919821321161\n",
      "Training epoch 187, calculated loss: 0.025131155061378512\n",
      "Training epoch 188, calculated loss: 0.025053776873631514\n",
      "Training epoch 189, calculated loss: 0.02497699879850671\n",
      "Training epoch 190, calculated loss: 0.024900811382452148\n",
      "Training epoch 191, calculated loss: 0.024825196493763895\n",
      "Training epoch 192, calculated loss: 0.024750203250312897\n",
      "Training epoch 193, calculated loss: 0.024675755312424136\n",
      "Training epoch 194, calculated loss: 0.0246018101911868\n",
      "Training epoch 195, calculated loss: 0.024528412424500098\n",
      "Training epoch 196, calculated loss: 0.024455563550236755\n",
      "Training epoch 197, calculated loss: 0.024383256049459078\n",
      "Training epoch 198, calculated loss: 0.024311482464479794\n",
      "Training epoch 199, calculated loss: 0.02424029131600126\n",
      "Training epoch 200, calculated loss: 0.024169598157041484\n",
      "Training epoch 201, calculated loss: 0.024099418847350257\n",
      "Training epoch 202, calculated loss: 0.024029715734076598\n",
      "Training epoch 203, calculated loss: 0.023960476706554865\n",
      "Training epoch 204, calculated loss: 0.02389175714727621\n",
      "Training epoch 205, calculated loss: 0.023823533863734196\n",
      "Training epoch 206, calculated loss: 0.023755818941390325\n",
      "Training epoch 207, calculated loss: 0.02368865603602478\n",
      "Training epoch 208, calculated loss: 0.023621971869010196\n",
      "Training epoch 209, calculated loss: 0.02355576090957149\n",
      "Training epoch 210, calculated loss: 0.023490019750692922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 211, calculated loss: 0.02342474204123094\n",
      "Training epoch 212, calculated loss: 0.023359920265250397\n",
      "Training epoch 213, calculated loss: 0.02329555815918436\n",
      "Training epoch 214, calculated loss: 0.023231615511551312\n",
      "Training epoch 215, calculated loss: 0.023168167189470932\n",
      "Training epoch 216, calculated loss: 0.023106503740087346\n",
      "Training epoch 217, calculated loss: 0.023045262943421428\n",
      "Training epoch 218, calculated loss: 0.022984457449207463\n",
      "Training epoch 219, calculated loss: 0.022924086456384756\n",
      "Training epoch 220, calculated loss: 0.02286443977695804\n",
      "Training epoch 221, calculated loss: 0.02280572172555671\n",
      "Training epoch 222, calculated loss: 0.0227474077251105\n",
      "Training epoch 223, calculated loss: 0.022689477924454853\n",
      "Training epoch 224, calculated loss: 0.022631933175149947\n",
      "Training epoch 225, calculated loss: 0.02257474729379584\n",
      "Training epoch 226, calculated loss: 0.02251793322395557\n",
      "Training epoch 227, calculated loss: 0.02246149483281463\n",
      "Training epoch 228, calculated loss: 0.022405433208505115\n",
      "Training epoch 229, calculated loss: 0.022349741194258147\n",
      "Training epoch 230, calculated loss: 0.022294395850001023\n",
      "Training epoch 231, calculated loss: 0.02223941082052468\n",
      "Training epoch 232, calculated loss: 0.022185074002067543\n",
      "Training epoch 233, calculated loss: 0.022131642425037668\n",
      "Training epoch 234, calculated loss: 0.022078571450039428\n",
      "Training epoch 235, calculated loss: 0.02202585954728002\n",
      "Training epoch 236, calculated loss: 0.021973566337548836\n",
      "Training epoch 237, calculated loss: 0.021921986342531407\n",
      "Training epoch 238, calculated loss: 0.021870729922543308\n",
      "Training epoch 239, calculated loss: 0.021819795248452657\n",
      "Training epoch 240, calculated loss: 0.02176917289007299\n",
      "Training epoch 241, calculated loss: 0.021718871369279004\n",
      "Training epoch 242, calculated loss: 0.021668887503075624\n",
      "Training epoch 243, calculated loss: 0.021619218162804186\n",
      "Training epoch 244, calculated loss: 0.021569860213154584\n",
      "Training epoch 245, calculated loss: 0.021520809638471017\n",
      "Training epoch 246, calculated loss: 0.021472057204516306\n",
      "Training epoch 247, calculated loss: 0.021423631068818255\n",
      "Training epoch 248, calculated loss: 0.02137550013346932\n",
      "Training epoch 249, calculated loss: 0.02132765548280479\n",
      "Training epoch 250, calculated loss: 0.02128010485405213\n",
      "Training epoch 251, calculated loss: 0.02123284548711692\n",
      "Training epoch 252, calculated loss: 0.021186781607609013\n",
      "Training epoch 253, calculated loss: 0.021141733105287296\n",
      "Training epoch 254, calculated loss: 0.021097234128283016\n",
      "Training epoch 255, calculated loss: 0.02105349638515682\n",
      "Training epoch 256, calculated loss: 0.021010083801861694\n",
      "Training epoch 257, calculated loss: 0.02096707065064579\n",
      "Training epoch 258, calculated loss: 0.020924282671709576\n",
      "Training epoch 259, calculated loss: 0.020881732591795914\n",
      "Training epoch 260, calculated loss: 0.02083941797737827\n",
      "Training epoch 261, calculated loss: 0.020797336249471478\n",
      "Training epoch 262, calculated loss: 0.02075548494001586\n",
      "Training epoch 263, calculated loss: 0.020713861690447574\n",
      "Training epoch 264, calculated loss: 0.02067246471571141\n",
      "Training epoch 265, calculated loss: 0.020631294105844807\n",
      "Training epoch 266, calculated loss: 0.020590343323235658\n",
      "Training epoch 267, calculated loss: 0.02054959718955501\n",
      "Training epoch 268, calculated loss: 0.02050906791176532\n",
      "Training epoch 269, calculated loss: 0.02046875447928981\n",
      "Training epoch 270, calculated loss: 0.02042865518683383\n",
      "Training epoch 271, calculated loss: 0.02038876527021554\n",
      "Training epoch 272, calculated loss: 0.020349080353884982\n",
      "Training epoch 273, calculated loss: 0.020309602096758778\n",
      "Training epoch 274, calculated loss: 0.020270327237427506\n",
      "Training epoch 275, calculated loss: 0.020231249089619385\n",
      "Training epoch 276, calculated loss: 0.020192395220794416\n",
      "Training epoch 277, calculated loss: 0.02015378568914743\n",
      "Training epoch 278, calculated loss: 0.020115372937426216\n",
      "Training epoch 279, calculated loss: 0.02007715507788205\n",
      "Training epoch 280, calculated loss: 0.020039130259555525\n",
      "Training epoch 281, calculated loss: 0.020001296627910566\n",
      "Training epoch 282, calculated loss: 0.019963727902689618\n",
      "Training epoch 283, calculated loss: 0.019926469696129937\n",
      "Training epoch 284, calculated loss: 0.019889399057232634\n",
      "Training epoch 285, calculated loss: 0.01985250867954502\n",
      "Training epoch 286, calculated loss: 0.01981578902672583\n",
      "Training epoch 287, calculated loss: 0.019779244544301487\n",
      "Training epoch 288, calculated loss: 0.01974287469919798\n",
      "Training epoch 289, calculated loss: 0.019706668368943078\n",
      "Training epoch 290, calculated loss: 0.01967061229883547\n",
      "Training epoch 291, calculated loss: 0.01963472790177622\n",
      "Training epoch 292, calculated loss: 0.019599015432734384\n",
      "Training epoch 293, calculated loss: 0.01956352693739724\n",
      "Training epoch 294, calculated loss: 0.01952820511825909\n",
      "Training epoch 295, calculated loss: 0.019493048446728743\n",
      "Training epoch 296, calculated loss: 0.01945805525046133\n",
      "Training epoch 297, calculated loss: 0.01942322399259875\n",
      "Training epoch 298, calculated loss: 0.019388552764505806\n",
      "Training epoch 299, calculated loss: 0.01935403388231458\n",
      "Training epoch 300, calculated loss: 0.01931965884117488\n",
      "Training epoch 301, calculated loss: 0.019285439891646914\n",
      "Training epoch 302, calculated loss: 0.019251376014061048\n",
      "Training epoch 303, calculated loss: 0.019217466591435518\n",
      "Training epoch 304, calculated loss: 0.019183708937534107\n",
      "Training epoch 305, calculated loss: 0.019150101708966392\n",
      "Training epoch 306, calculated loss: 0.019116643422541553\n",
      "Training epoch 307, calculated loss: 0.019083332714025326\n",
      "Training epoch 308, calculated loss: 0.01905016833109527\n",
      "Training epoch 309, calculated loss: 0.019017148767328167\n",
      "Training epoch 310, calculated loss: 0.018984331600369872\n",
      "Training epoch 311, calculated loss: 0.01895173337927134\n",
      "Training epoch 312, calculated loss: 0.018919273520084094\n",
      "Training epoch 313, calculated loss: 0.01888695576066753\n",
      "Training epoch 314, calculated loss: 0.01885478211541755\n",
      "Training epoch 315, calculated loss: 0.018822737788818468\n",
      "Training epoch 316, calculated loss: 0.018790876850025277\n",
      "Training epoch 317, calculated loss: 0.018759243829079793\n",
      "Training epoch 318, calculated loss: 0.01872774015249987\n",
      "Training epoch 319, calculated loss: 0.018696362957911668\n",
      "Training epoch 320, calculated loss: 0.01866509330963509\n",
      "Training epoch 321, calculated loss: 0.018633926944898675\n",
      "Training epoch 322, calculated loss: 0.01860288387934998\n",
      "Training epoch 323, calculated loss: 0.01857196461163533\n",
      "Training epoch 324, calculated loss: 0.018541167970029127\n",
      "Training epoch 325, calculated loss: 0.018510492766506743\n",
      "Training epoch 326, calculated loss: 0.018479937951811003\n",
      "Training epoch 327, calculated loss: 0.018449502414069962\n",
      "Training epoch 328, calculated loss: 0.018419185053160942\n",
      "Training epoch 329, calculated loss: 0.018388984780467292\n",
      "Training epoch 330, calculated loss: 0.018358900518674528\n",
      "Training epoch 331, calculated loss: 0.018328931201589113\n",
      "Training epoch 332, calculated loss: 0.018299075773971402\n",
      "Training epoch 333, calculated loss: 0.018269333191377725\n",
      "Training epoch 334, calculated loss: 0.018239702420008955\n",
      "Training epoch 335, calculated loss: 0.018210183853748407\n",
      "Training epoch 336, calculated loss: 0.018180778734885623\n",
      "Training epoch 337, calculated loss: 0.0181514859466068\n",
      "Training epoch 338, calculated loss: 0.018122346460517375\n",
      "Training epoch 339, calculated loss: 0.018093313408349514\n",
      "Training epoch 340, calculated loss: 0.018064388249859332\n",
      "Training epoch 341, calculated loss: 0.018035575986085354\n",
      "Training epoch 342, calculated loss: 0.01800687472140334\n",
      "Training epoch 343, calculated loss: 0.01797828760862383\n",
      "Training epoch 344, calculated loss: 0.017949801530181893\n",
      "Training epoch 345, calculated loss: 0.01792141572979192\n",
      "Training epoch 346, calculated loss: 0.017893129274587822\n",
      "Training epoch 347, calculated loss: 0.017864941289680997\n",
      "Training epoch 348, calculated loss: 0.01783685088304736\n",
      "Training epoch 349, calculated loss: 0.017808857172134884\n",
      "Training epoch 350, calculated loss: 0.017780958674743066\n",
      "Training epoch 351, calculated loss: 0.017753154363452588\n",
      "Training epoch 352, calculated loss: 0.01772544420130223\n",
      "Training epoch 353, calculated loss: 0.017697827336810353\n",
      "Training epoch 354, calculated loss: 0.017670505139356477\n",
      "Training epoch 355, calculated loss: 0.01764312157620326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 356, calculated loss: 0.017615794666522796\n",
      "Training epoch 357, calculated loss: 0.01758877334368935\n",
      "Training epoch 358, calculated loss: 0.01756160260899918\n",
      "Training epoch 359, calculated loss: 0.017534618154305317\n",
      "Training epoch 360, calculated loss: 0.017507773942776382\n",
      "Training epoch 361, calculated loss: 0.017480852005937925\n",
      "Training epoch 362, calculated loss: 0.017454183100791016\n",
      "Training epoch 363, calculated loss: 0.017427496928137515\n",
      "Training epoch 364, calculated loss: 0.01740082613664551\n",
      "Training epoch 365, calculated loss: 0.017374472107268273\n",
      "Training epoch 366, calculated loss: 0.01734795801760313\n",
      "Training epoch 367, calculated loss: 0.017321616845881228\n",
      "Training epoch 368, calculated loss: 0.01729542228899278\n",
      "Training epoch 369, calculated loss: 0.017269151403135292\n",
      "Training epoch 370, calculated loss: 0.017243126761711802\n",
      "Training epoch 371, calculated loss: 0.01721709001590805\n",
      "Training epoch 372, calculated loss: 0.017191059244768204\n",
      "Training epoch 373, calculated loss: 0.0171653355779045\n",
      "Training epoch 374, calculated loss: 0.017139453513520645\n",
      "Training epoch 375, calculated loss: 0.0171137314795393\n",
      "Training epoch 376, calculated loss: 0.017088155704738508\n",
      "Training epoch 377, calculated loss: 0.017062552776003256\n",
      "Training epoch 378, calculated loss: 0.01703717154459236\n",
      "Training epoch 379, calculated loss: 0.017011786313254278\n",
      "Training epoch 380, calculated loss: 0.016986398382719318\n",
      "Training epoch 381, calculated loss: 0.016961299064915135\n",
      "Training epoch 382, calculated loss: 0.016936067231226582\n",
      "Training epoch 383, calculated loss: 0.01691094674869566\n",
      "Training epoch 384, calculated loss: 0.016886008516901266\n",
      "Training epoch 385, calculated loss: 0.016860971292149196\n",
      "Training epoch 386, calculated loss: 0.01683611417695145\n",
      "Training epoch 387, calculated loss: 0.01681132174293982\n",
      "Training epoch 388, calculated loss: 0.01678649017626591\n",
      "Training epoch 389, calculated loss: 0.016761889207288282\n",
      "Training epoch 390, calculated loss: 0.016737233985821144\n",
      "Training epoch 391, calculated loss: 0.016712593479934926\n",
      "Training epoch 392, calculated loss: 0.016688231134902066\n",
      "Training epoch 393, calculated loss: 0.016663729211307553\n",
      "Training epoch 394, calculated loss: 0.01663935266942703\n",
      "Training epoch 395, calculated loss: 0.016615125763297142\n",
      "Training epoch 396, calculated loss: 0.016590814529033538\n",
      "Training epoch 397, calculated loss: 0.016566666218162246\n",
      "Training epoch 398, calculated loss: 0.016542568404101638\n",
      "Training epoch 399, calculated loss: 0.016518436432256322\n",
      "Training epoch 400, calculated loss: 0.016494509895733853\n",
      "Training epoch 401, calculated loss: 0.016470529463194432\n",
      "Training epoch 402, calculated loss: 0.016446559918497566\n",
      "Training epoch 403, calculated loss: 0.016422838466575714\n",
      "Training epoch 404, calculated loss: 0.016398990347542435\n",
      "Training epoch 405, calculated loss: 0.016375232609171946\n",
      "Training epoch 406, calculated loss: 0.01635164757184817\n",
      "Training epoch 407, calculated loss: 0.01632796486793203\n",
      "Training epoch 408, calculated loss: 0.016304410694098145\n",
      "Training epoch 409, calculated loss: 0.016280942600118104\n",
      "Training epoch 410, calculated loss: 0.016257478639199432\n",
      "Training epoch 411, calculated loss: 0.01623417543191887\n",
      "Training epoch 412, calculated loss: 0.016210849784015102\n",
      "Training epoch 413, calculated loss: 0.016187593674417976\n",
      "Training epoch 414, calculated loss: 0.01616442671750274\n",
      "Training epoch 415, calculated loss: 0.01614138714754772\n",
      "Training epoch 416, calculated loss: 0.01611828421367016\n",
      "Training epoch 417, calculated loss: 0.01609538633272783\n",
      "Training epoch 418, calculated loss: 0.01607245424551852\n",
      "Training epoch 419, calculated loss: 0.016049515297991753\n",
      "Training epoch 420, calculated loss: 0.016026772812110287\n",
      "Training epoch 421, calculated loss: 0.016003974410484165\n",
      "Training epoch 422, calculated loss: 0.01598122937285836\n",
      "Training epoch 423, calculated loss: 0.015958581203497874\n",
      "Training epoch 424, calculated loss: 0.015935996147799513\n",
      "Training epoch 425, calculated loss: 0.015913348680122976\n",
      "Training epoch 426, calculated loss: 0.015891002990864034\n",
      "Training epoch 427, calculated loss: 0.01586883105921396\n",
      "Training epoch 428, calculated loss: 0.015846671267835684\n",
      "Training epoch 429, calculated loss: 0.015824614083511875\n",
      "Training epoch 430, calculated loss: 0.015802601238421792\n",
      "Training epoch 431, calculated loss: 0.015780578292568057\n",
      "Training epoch 432, calculated loss: 0.015758642009637423\n",
      "Training epoch 433, calculated loss: 0.0157368177463153\n",
      "Training epoch 434, calculated loss: 0.01571487717244905\n",
      "Training epoch 435, calculated loss: 0.015693144401486798\n",
      "Training epoch 436, calculated loss: 0.0156714024601336\n",
      "Training epoch 437, calculated loss: 0.01564965718696547\n",
      "Training epoch 438, calculated loss: 0.01562799610545084\n",
      "Training epoch 439, calculated loss: 0.015606401550491589\n",
      "Training epoch 440, calculated loss: 0.01558484269601838\n",
      "Training epoch 441, calculated loss: 0.01556324910988694\n",
      "Training epoch 442, calculated loss: 0.015541846564335906\n",
      "Training epoch 443, calculated loss: 0.015520395000633826\n",
      "Training epoch 444, calculated loss: 0.01549896292420112\n",
      "Training epoch 445, calculated loss: 0.015477738434365033\n",
      "Training epoch 446, calculated loss: 0.015456451643058622\n",
      "Training epoch 447, calculated loss: 0.015435144268066293\n",
      "Training epoch 448, calculated loss: 0.015414030828237238\n",
      "Training epoch 449, calculated loss: 0.015392880378944332\n",
      "Training epoch 450, calculated loss: 0.015371699997255843\n",
      "Training epoch 451, calculated loss: 0.015350777775014758\n",
      "Training epoch 452, calculated loss: 0.01532985346038134\n",
      "Training epoch 453, calculated loss: 0.015308877608725546\n",
      "Training epoch 454, calculated loss: 0.015288066735668472\n",
      "Training epoch 455, calculated loss: 0.015267312552867986\n",
      "Training epoch 456, calculated loss: 0.015246479688444984\n",
      "Training epoch 457, calculated loss: 0.015225740138539464\n",
      "Training epoch 458, calculated loss: 0.015205136577414973\n",
      "Training epoch 459, calculated loss: 0.015184409081631182\n",
      "Training epoch 460, calculated loss: 0.01516372133056533\n",
      "Training epoch 461, calculated loss: 0.015143236011201027\n",
      "Training epoch 462, calculated loss: 0.015122625647630292\n",
      "Training epoch 463, calculated loss: 0.01510203096718133\n",
      "Training epoch 464, calculated loss: 0.01508156553376889\n",
      "Training epoch 465, calculated loss: 0.01506112028061662\n",
      "Training epoch 466, calculated loss: 0.015040623655817115\n",
      "Training epoch 467, calculated loss: 0.01502023340882088\n",
      "Training epoch 468, calculated loss: 0.014999864126617166\n",
      "Training epoch 469, calculated loss: 0.014979518283802541\n",
      "Training epoch 470, calculated loss: 0.014959208809232464\n",
      "Training epoch 471, calculated loss: 0.014938962000623479\n",
      "Training epoch 472, calculated loss: 0.014918670750383299\n",
      "Training epoch 473, calculated loss: 0.014898431477886132\n",
      "Training epoch 474, calculated loss: 0.01487834565419436\n",
      "Training epoch 475, calculated loss: 0.014858189172753956\n",
      "Training epoch 476, calculated loss: 0.01483858637358962\n",
      "Training epoch 477, calculated loss: 0.014819121424711272\n",
      "Training epoch 478, calculated loss: 0.014799598858749997\n",
      "Training epoch 479, calculated loss: 0.014780102467373048\n",
      "Training epoch 480, calculated loss: 0.014760734610105698\n",
      "Training epoch 481, calculated loss: 0.014741333240890721\n",
      "Training epoch 482, calculated loss: 0.014721925628113237\n",
      "Training epoch 483, calculated loss: 0.014702595955525666\n",
      "Training epoch 484, calculated loss: 0.01468331363495849\n",
      "Training epoch 485, calculated loss: 0.014663987657744671\n",
      "Training epoch 486, calculated loss: 0.01464470305266625\n",
      "Training epoch 487, calculated loss: 0.01462553212848643\n",
      "Training epoch 488, calculated loss: 0.014606324989534389\n",
      "Training epoch 489, calculated loss: 0.014587134029664316\n",
      "Training epoch 490, calculated loss: 0.014568042591314017\n",
      "Training epoch 491, calculated loss: 0.014548924830277485\n",
      "Training epoch 492, calculated loss: 0.014529812388646646\n",
      "Training epoch 493, calculated loss: 0.014510774305594302\n",
      "Training epoch 494, calculated loss: 0.014491769521271815\n",
      "Training epoch 495, calculated loss: 0.014472873689609119\n",
      "Training epoch 496, calculated loss: 0.014454969430756892\n",
      "Training epoch 497, calculated loss: 0.014437170641552787\n",
      "Training epoch 498, calculated loss: 0.014419318261127906\n",
      "Training epoch 499, calculated loss: 0.014401495725134454\n",
      "Training epoch 500, calculated loss: 0.014383760990065046\n"
     ]
    }
   ],
   "source": [
    "# Fit new model \n",
    "\n",
    "nn2.fit(Xtrain2, ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZwlZX3v8c/3nNPL9KzQ0+AsMMNm4mgQsYOCWYhR74AGYjTKBNyCkOTG6DXeRL1JwItm03vVGIk6IpJFIbiPOAYNuFxFCY3isGXigAO0MzjN7HtPd//uH1Wnu/r06Z6enq4+3V3f9+t1XqfqqedUPU8znO+ppzZFBGZmVlylRjfAzMway0FgZlZwDgIzs4JzEJiZFZyDwMys4BwEZmYF5yAwm2SSXiDpx5L2SfrNRrenlqSvSbp8suvazCVfR2ATJWkz8MaI+PcGbHsJ8B7gYmAe8FPgX4H3RsT+qW5PTdvuANZFxN9Nwrq+CvxyOtsCBNCbzv9LRPz+8W7DzHsENuNIOhH4HjAHOD8i5gMvBhYBZ0xgfZXJbSErgAcn8sHatkTERRExLyLmAZ8iCbp56WtECOTQFysAB4HlQtJVkjZJ2iFpnaSlabkkfUDSNkm7JW2Q9Kx02cWSHpK0V9JPJf3PUVb/x8Be4IqI2AwQEU9ExFsiYoOklZIi+6Uo6ZuS3phOv17Sd9N27ADeLWlXtR1pnQ5JByWdlM6/TNJ9ab27JJ09Sr8fAU4HvpwODbVIWpr+DXakf5OrMvXfJemzkv5F0h7g9cf4d36RpM2S/pekJ4GPS2qXtF5Sj6Sdkr4saVnmM9+R9Pp0+o2SvpX+LXZJelTSSyZY94y0/t50SOkjkm46lv5YYzgIbNJJeiHw18CrgCXAY8At6eKXAL8CPJ3kF/yrge3psk8Av5f+wn8WcOcom3gR8PmIGDiOZj4PeBQ4CbgO+DywJrP8VcC3ImKbpHOBG4HfA9qBjwHrJLXUrjQizgAeB34j/dV+GLgZ6AaWAq8E/krSr2c+dinwWZK/x6cm0JflJMNjpwL/neT/64+n8yuAI8BYw1QXAPenffsAyX+HidS9Gfhuuuw9wBXH3hVrBAeB5eFy4MaI+EH6RfhO4HxJK0m+lOYDP09yjOrhiNiafu4IsErSgojYGRE/GGX97cDWUZaN15aI+PuI6IuIg8CnGR4Ev5OWAVwFfCwi7o6I/oj4R+Aw8PyjbUTSKcAvAW+PiEMRcR9wA/CaTLXvRcQXI2Igbcux6gPeFRG9EXEwInoi4gvp9B7gr4BfHePzj0TEjRHRD/wjsFzS4mOpK+l04NmZdnwb+MoE+mIN4CCwPCwl2QsAICL2kfzqXxYRdwIfBq4HfiZpraQFadVXkBz8fSwdgjh/lPVvJ9nTOB5P1MzfCcyR9DxJK4BzgC+ky1YAb0uHQ3ZJ2gWckvbzaJYCOyJib6bsMWBZZr62LcfqZxFRPYCMpLmSbpD0eDrcdCcw2hc7wJOZ6QPp+7xjrLsU2F4TZMfbL5siDgLLwxaSL08g+WIi+RX/U4CI+FBEPBd4JskQ0Z+k5fdExKUkwzVfBG4dZf3/Drxc0mj/fqtnDbVlyp5WU2fY6XLpMNOtJHsFvwPclvnyfgL4y4hYlHm1RcTNo2w/awtwoqT5mbJTSf8W9doyAbWf/1PgNOC8iFgAvPA41z8eW4F2Sa2ZslOmYLs2CRwEdryaJLVmXhWSIZU3SDonHUf/K+DuiNgs6RfTX91NJF/Yh4B+Sc2SLpe0MCKOAHuA/lG2+X5gAfCP6a93JC2T9H5JZ0dED8kX7RWSypJ+l/GdTfRpkmMWlzM0LATJePvvp+1W+ov7pTVf7nVFxBPAXcBfp3+fs4ErmdixgPGaT/JrfaekduCaHLcFQEQ8QnLs4Nr0v+UvAS/Ne7s2ORwEdrzWAwczr3dFxB3AXwCfI/mleAZwWVp/AckX606SIZLtwP9Jl70G2JwOZ/w+oxxsjIgdJActjwB3S9oL3AHsBjal1a4i2dPYTrLncdfROhIRd5OE01Lgq5nyrnR9H07bvYljO7tnDbCSZO/gC8C1EfH1Y/j8sXo/sJCk73eR6UvO1pCcCLAduJbkuo7DU7RtOw6+oMzMciHpc8B9EfHuRrfFxuY9AjObFJLOk3SapJKki4GXAV9qdLvs6HwVoplNlqUkw4Enklw3cVVEbGhsk2w8PDRkZlZwHhoyMyu4GTc0tHjx4li5cmWjm2FmNqPce++9T0VER71luQWBpBtJDhZti4hnjVLnQuCDQBPwVESMdRk8ACtXrqSrq2sym2pmNutJemy0ZXkODd0ErB5toaRFwD8Al0TEM4HfzrEtZmY2ityCIL3p1I4xqvwOyR0kH0/rb8urLWZmNrpGHix+OnCCkvvE3yvptaNVlHS1pC5JXT09PVPYRDOz2a+RQVABnktyP5L/BvyFpKfXqxgRayOiMyI6OzrqHuswM7MJauRZQ90kB4j3A/slfZvkfub/1cA2mZkVTiP3CL4E/LKkiqQ2kidGPdzA9piZFVKep4/eDFwILJbUTXI3wiaAiPhoRDws6d+ADcAAcENEPJBXe8zMrL7cgiAi1oyjzvuA9+XVhqyNT+7lKxu28NoLVrJ43ohHzZqZFVZhbjGxads+PnTnJnbs7z16ZTOzAilMEJSUvA/4JntmZsMUJgikJAn6BxwEZmZZhQmC6h6BdwjMzIYrTBCU0yTw0JCZ2XCFCYKSqkHQ4IaYmU0zhQmCNAd8jMDMrEZhgqA6NORHc5qZDVeYIPDQkJlZfYUJAvk6AjOzugoTBIN7BN4lMDMbpjBBMHT6aIMbYmY2zRQmCHyLCTOz+goTBIO3mHAQmJkNU5ggKMunj5qZ1VOYIBg6WNzghpiZTTOFCQKfPmpmVl9uQSDpRknbJI35+ElJvyipX9Ir82oLZC8ocxCYmWXluUdwE7B6rAqSysDfArfn2A7Ap4+amY0mtyCIiG8DO45S7Y+AzwHb8mpHlU8fNTOrr2HHCCQtA14OfHQcda+W1CWpq6enZ6LbA3z3UTOzWo08WPxB4O0R0X+0ihGxNiI6I6Kzo6NjQhsbuvvohD5uZjZrVRq47U7glvSX+mLgYkl9EfHFPDbmoSEzs/oaFgQRcVp1WtJNwG15hQD4NtRmZqPJLQgk3QxcCCyW1A1cCzQBRMRRjwtMfnuSd9991MxsuNyCICLWHEPd1+fVjio/vN7MrL7CXFnsoSEzs/oKEwSDD6/3HoGZ2TCFCQLffdTMrL7CBIEfVWlmVl/xgsA5YGY2TGGCQGlPfdaQmdlwhQmCsm9DbWZWV2GCwENDZmb1FSYIBk8fdRKYmQ1TmCAo+fRRM7O6ChMEfkKZmVl9hQkC34bazKy+wgSBfEGZmVldhQkCSIaHnANmZsMVKghK8tCQmVmtQgWB5D0CM7NahQoC7xGYmY2UWxBIulHSNkkPjLL8ckkb0tddkp6dV1uqypIPFpuZ1chzj+AmYPUYy38C/GpEnA28G1ibY1uA5KIy54CZ2XB5PrP425JWjrH8rszs94HlebWlSh4aMjMbYbocI7gS+OpoCyVdLalLUldPT8+EN5KcPuogMDPLangQSPo1kiB4+2h1ImJtRHRGRGdHR8eEt5UMDTkIzMyychsaGg9JZwM3ABdFxPYp2J6PEZiZ1WjYHoGkU4HPA6+JiP+aim2W5FtMmJnVym2PQNLNwIXAYkndwLVAE0BEfBS4BmgH/iG9D1BfRHTm1R7wMQIzs3ryPGtozVGWvxF4Y17br8enj5qZjdTwg8VTSR4aMjMboVBB4KEhM7ORChUEHhoyMxupUEHgK4vNzEYqVBD4gjIzs5EKFQTJ3Ucb3Qozs+mlUEHgoSEzs5EKFQQeGjIzG6lQQeCH15uZjVSoIPCjKs3MRipUEPjuo2ZmIxUqCHz3UTOzkQoVBOWS2Lr7oMPAzCyjUEHwk6f280jPfj551+ZGN8XMbNooVBA8ta8XgI1P7mlwS8zMpo9CBUHVmSfNa3QTzMymjdyCQNKNkrZJemCU5ZL0IUmbJG2QdG5ebanVXC5k/pmZ1ZXnN+JNwOoxll8EnJW+rgY+kmNbAPj0Vc8DoN/His3MBuUWBBHxbWDHGFUuBf4pEt8HFklakld7AM5evgiAft95zsxsUCPHSJYBT2Tmu9OyESRdLalLUldPT8+EN1gpCYB+54CZ2aBGBoHqlNUdtImItRHRGRGdHR0dE95gSdUgcBKYmVU1Mgi6gVMy88uBLXlu0HsEZmYjNTII1gGvTc8eej6wOyK25rnBUjUIfOM5M7NBlbxWLOlm4EJgsaRu4FqgCSAiPgqsBy4GNgEHgDfk1ZasckkeGjIzy8gtCCJizVGWB/CHeW1/NEkQTPVWzcymr8JdWVWW9wjMzLIKFwQV7xGYmQ1TuCAo+RiBmdkwhQuCSkk+a8jMLKNwQVDy0JCZ2TCFCwIfLDYzG654QeA9AjOzYQoaBE4CM7OqwgVBcrC40a0wM5s+ChcEPn3UzGy4wgVBckGZdwnMzKoKFwQl+WCxmVlW4YKgUvbQkJlZVuGCoCQfLDYzyxpXEEg6Q1JLOn2hpDdLWpRv0/Lh00fNzIYb7x7B54B+SWcCnwBOAz6dW6tyVPbBYjOzYcYbBAMR0Qe8HPhgRLwVWJJfs/JTlvAOgZnZkPEGwRFJa4DXAbelZU1H+5Ck1ZI2Stok6R11lp8q6RuSfihpg6SLx9/0iamURZ+TwMxs0HiD4A3A+cBfRsRPJJ0G/MtYH5BUBq4HLgJWAWskraqp9ufArRHxHOAy4B+OpfET4YPFZmbDjeuZxRHxEPBmAEknAPMj4m+O8rHzgE0R8Wj6uVuAS4GHsqsGFqTTC4Et42/6xFR8sNjMbJjxnjX0TUkLJJ0I/Aj4pKT3H+Vjy4AnMvPdaVnWu4ArJHUD64E/GmX7V0vqktTV09MzniaPys8jMDMbbrxDQwsjYg/wW8AnI+K5wIuO8hnVKasdlFkD3BQRy4GLgX+WNKJNEbE2IjojorOjo2OcTa7PzyMwMxtuvEFQkbQEeBVDB4uPphs4JTO/nJFDP1cCtwJExPeAVmDxONc/IeWyTx81M8sabxBcB9wOPBIR90g6HfjxUT5zD3CWpNMkNZMcDF5XU+dx4NcBJD2DJAiOb+znKMoSzgEzsyHjPVj8GeAzmflHgVcc5TN9kt5EEiBl4MaIeFDSdUBXRKwD3gZ8XNJbSYaNXh+R75PlKyWfPmpmljWuIJC0HPh74AUkX9jfAd4SEd1jfS4i1pMcBM6WXZOZfihd55QplXxBmZlZ1niHhj5JMqyzlOTMny+nZTOO9wjMzIYbbxB0RMQnI6Ivfd0EHN/pOw3i00fNzIYbbxA8JekKSeX0dQWwPc+G5aVSEgP5HoYwM5tRxhsEv0ty6uiTwFbglSS3nZhxShJ93iUwMxs0riCIiMcj4pKI6IiIkyLiN0kuLptxyiWfPmpmlnU8Tyj740lrxRTywWIzs+GOJwjq3UJi2vPpo2Zmwx1PEMzIARbvEZiZDTfmBWWS9lL/C1/AnFxalLPWpjIDAYeO9NPaVG50c8zMGm7MIIiI+VPVkKnSMa8FgKf2HWb5CW0Nbo2ZWeMdz9DQjNQxPwmCnr2HG9wSM7PpobBBsM1BYGYGFDgIvEdgZpYoXBC0z21GchCYmVUVLggq5RIntjXTs89BYGYGBQwCgPmtFfYd6mt0M8zMpoVCBsHclgoHeh0EZmaQcxBIWi1po6RNkt4xSp1XSXpI0oOSPp1ne6rmNlfYd9hBYGYG43xU5URIKgPXAy8GuoF7JK1LH09ZrXMW8E7gBRGxU9JJebUnq62lzI79vVOxKTOzaS/PPYLzgE0R8WhE9AK3AJfW1LkKuD4idgJExLYc2zNobkuF/d4jMDMD8g2CZcATmfnutCzr6cDTJX1X0vclra63IklXS+qS1NXT03PcDZvbXGb/4f7jXo+Z2WyQZxDUu0117Q3sKsBZwIXAGuAGSYtGfChibUR0RkRnR8fxPyq5rbnCfh8sNjMD8g2CbuCUzPxyYEudOl+KiCMR8RNgI0kw5GpeS4UDvf2En11sZpZrENwDnCXpNEnNwGXAupo6XwR+DUDSYpKhokdzbBOQHCzuHwgO9/m5BGZmuQVBRPQBbwJuBx4Gbo2IByVdJ+mStNrtwHZJDwHfAP4kIrbn1aaqeS3JyVI+YGxmluPpowARsR5YX1N2TWY6SJ59PKXPP25rTrp9oLef9qncsJnZNFTMK4ubkyeT+aIyM7OCBsG81mSPwEFgZlbQIFjQ2gTA3kNHGtwSM7PGK2YQzEmCYPdBB4GZWTGDIB0a2nPQQ0NmZsUMAu8RmJkNKmQQNJVLtDWX2eMgMDMrZhAALJzTxB4fLDYzK24QLGht8tCQmRlFDoI5FR8sNjOjwEGwcI73CMzMoMBBsKDVxwjMzKDIQTCnyWcNmZlR8CDYe7iPgQE/nMbMiq24QdBaIQL2+sZzZlZwhQ2ChenVxR4eMrOiyzUIJK2WtFHSJknvGKPeKyWFpM4825Pl20yYmSVyCwJJZeB64CJgFbBG0qo69eYDbwbuzqst9VRvRe0zh8ys6PLcIzgP2BQRj0ZEL3ALcGmdeu8G3gscyrEtI3hoyMwskWcQLAOeyMx3p2WDJD0HOCUibhtrRZKultQlqaunp2dSGrdgjm9FbWYG+QaB6pQNnqspqQR8AHjb0VYUEWsjojMiOjs6OialcdU9gl0HeydlfWZmM1WeQdANnJKZXw5syczPB54FfFPSZuD5wLqpOmA8r6VCU1ls3+8gMLNiyzMI7gHOknSapGbgMmBddWFE7I6IxRGxMiJWAt8HLomIrhzbNEgSJ85tZsc+B4GZFVtuQRARfcCbgNuBh4FbI+JBSddJuiSv7R6LE+e2sPOAg8DMiq2S58ojYj2wvqbsmlHqXphnW+ppn9vsoSEzK7zCXlkMJENDDgIzKzgHgY8RmFnBFToI2uc2s/dwH4f7+hvdFDOzhil0EJy8oBWAbXsON7glZmaNU+ggeNrCJAie3DOld7cwM5tWHATA1t0OAjMrLgcB8DMHgZkVWKGDYH5LhbnNZe8RmFmhFToIJPG0ha1s2XWw0U0xM2uYQgcBwKkntvHYjgONboaZWcMUPghWtM/l8e37iYijVzYzm4UcBO1t7O/t9z2HzKywHATtbQA8tn1/g1tiZtYYhQ+Cs06aD8DGJ/c1uCVmZo1R+CBYfsIc5rdUeHjrnkY3xcysIQofBJL4+SXzHQRmVliFDwKAZyxZwH8+uZeBAZ85ZGbFk2sQSFotaaOkTZLeUWf5H0t6SNIGSXdIWpFne0azaskC9h3u44mdvp7AzIontyCQVAauBy4CVgFrJK2qqfZDoDMizgY+C7w3r/aM5RlLFgB4eMjMCinPPYLzgE0R8WhE9AK3AJdmK0TENyKi+jP8+8DyHNszqp972nwqJbGhe3cjNm9m1lB5BsEy4InMfHdaNporga/WWyDpakldkrp6enomsYmJ1qYyzz5lEXc9sn3S121mNt3lGQSqU1b3aKykK4BO4H31lkfE2ojojIjOjo6OSWzikAvOaGdD9y52HzySy/rNzKarPIOgGzglM78c2FJbSdKLgD8DLomIhj0z8oIzFjMQcPej3isws2LJMwjuAc6SdJqkZuAyYF22gqTnAB8jCYFtObblqM5dsYjWppKHh8yscHILgojoA94E3A48DNwaEQ9Kuk7SJWm19wHzgM9Iuk/SulFWl7uWSpkLzljM7Q8+Sb+vJzCzAqnkufKIWA+srym7JjP9ojy3f6xe/pxl3Pmf2/j+o9t5wZmLG90cM7Mp4SuLM1686mTmt1T43A+6G90UM7Mp4yDIaG0q87JnL2X9/VvZttfPMTazYnAQ1Pi9XzmdI/3BR775SKObYmY2JRwENVYunssrzl3Gp+5+nM1P+WE1Zjb7OQjqeOuLn05rpcRbb72Pvv6BRjfHzCxXDoI6liycw1++/Bf44eO7uHbdg36wvZnNarmePjqT/cazl/Lglj189FuP0FQu8ecvfQaVsnPTzGYfB8EY3r765zjSP8AnvvMTNj65l795xS+won1uo5tlZjap/BN3DJL4i5et4r2vOJv7f7qbl3zg2/zfr21k+76G3RLJzGzSaaaNf3d2dkZXV9eUb/fJ3Yd491ce4isbttLaVOKlv7CUS89ZygVntHvIyMymPUn3RkRn3WUOgmOzadtebvh/P+Er929l76E+Fs5p4vzT27ngzHaed1o7Z3TMdTCY2bTjIMjBoSP9fHPjNu54eBt3PbKdn+46CEBLpcTPP20+q5Yu5MyT5rGyvY0V7W0sP6GN1qZyg1ttZkU1VhD4YPEEtTaVWf2sJax+1hIigsd3HODex3by0JY9PLR1D+vv3zrsITcSPG1BK0sWtnLyguTVMb+Fkxe0ctL8Fk5a0MKJbc0sbGuipeLAMLOp4yCYBJJY0T6XFe1z+a1zk7KIYMf+Xh7bcYDHtx/gse0HeGzHfn625xA/3raP7256ij2H+uqur625zAltzSyc08QJc5tY1NbMCW1NLJrTzPzWCvNaK8xvbWJ+SzI9ryV5zW+tMLelQpOHpszsGDgIciKJ9nkttM9r4dxTT6hb52BvPz17D/OzvYfYtucwOw/0svvgEXbu72XngSPsOtDLzgO9bN29h13p/HgeldDaVGJeS1MaDGXamirMaS7T1lweem8qM6e5Qls639pUHpye01QZrDsnLW+ulJJXuYRU7ymkZjZTOQgaaE5zmVPb2zi1vW1c9SOCA7397Dvcx95Dfew73Me+Q33sPXSEven0vsN9NcuPcKC3n50Hevnprn4O9vZz8Eg/B3r7OHRkYrfPaK6UaCmXBsOhpVIaFhRJ2VB4ZOs2l0tUyiWayqJcEk3lEpXS0HTyLsqlpE6llC0bql8ZtmyoTqW6vJQsr5SH6paEQ8ysDgfBDCKJuS3J8M/JC45/fQMDwaG+fg70JgFxIBMS2fmDvf309g1wuC997x+gt2/odbg6nSnfdaA3Ke8fWa9/IDgyMEAjzlOQoCxRKil5F8l0db6UlA3WKYlSWq86PVhWEuVM+eCyTLmUrLdanl13SVBSUkdicL6UhlV1u8rUq51XTT1JdT+X1Ev6KkbWq84LKJWOvv5qvWHzaT0y/aj2q7ZeNY+H2gii+j60rWpblanLiOUMrpvMekqZ5UoW1C2vtmWoHcPXXYQfD7kGgaTVwN8BZeCGiPibmuUtwD8BzwW2A6+OiM15tsmGlEqirblCW3Njfg8MpIHQPxAc6Q/6B4K+/gGODAT9/dllQ3X6qtMDQf/AQFoW9A0M0JeuI7vOvv4B+gaSOgORvPoHgv4IIkimB4bKByJpV3/E0Hu1bPBz1emh8oFI2nC4b6h8aJ3JewT0V8vSbQXpewx/H4iA9H2sejY1agOilBZkA6WUCScy9Us1oQWZYKUmfOqUZ7e15rxTeeMvnz7p/cvtG0BSGbgeeDHQDdwjaV1EPJSpdiWwMyLOlHQZ8LfAq/Nqk00vpZJoKfkMqeMRaaBlA2PYfOa9tjyomU+Xj1pvYPj669YbGNmOseone4XVsqHPRaZvg2VpeTUkh9dNt5H8UYaXZdZHTd3h6x++vmHltWXD2pH9+w3VGfzvQ2070j4PjOxD/W0Obatjfksu/47y/Cl4HrApIh4FkHQLcCmQDYJLgXel058FPixJMdMubjBrkMFhJWb/8IXlJ8/zDJcBT2Tmu9OyunUiog/YDbTXrkjS1ZK6JHX19PTk1Fwzs2LKMwjq/USp/aU/njpExNqI6IyIzo6OjklpnJmZJfIMgm7glMz8cmDLaHUkVYCFwI4c22RmZjXyDIJ7gLMknSapGbgMWFdTZx3wunT6lcCdPj5gZja1cjtYHBF9kt4E3E5y+uiNEfGgpOuArohYB3wC+GdJm0j2BC7Lqz1mZlZfrieQR8R6YH1N2TWZ6UPAb+fZBjMzG5vvTmZmVnAOAjOzgptxD6aR1AM8NsGPLwaemsTmzATuczG4z8VwPH1eERF1z7+fcUFwPCR1jfaEntnKfS4G97kY8uqzh4bMzArOQWBmVnBFC4K1jW5AA7jPxeA+F0MufS7UMQIzMxupaHsEZmZWw0FgZlZwhQkCSaslbZS0SdI7Gt2eySLpRknbJD2QKTtR0tcl/Th9PyEtl6QPpX+DDZLObVzLJ07SKZK+IelhSQ9KektaPmv7LalV0n9I+lHa5/+dlp8m6e60z/+a3uARSS3p/KZ0+cpGtn+iJJUl/VDSben8rO4vgKTNku6XdJ+krrQs13/bhQiCzGMzLwJWAWskrWpsqybNTcDqmrJ3AHdExFnAHek8JP0/K31dDXxkito42fqAt0XEM4DnA3+Y/veczf0+DLwwIp4NnAOslvR8kse7fiDt806Sx79C5jGwwAfSejPRW4CHM/Ozvb9VvxYR52SuGcj333akD+OezS/gfOD2zPw7gXc2ul2T2L+VwAOZ+Y3AknR6CbAxnf4YsKZevZn8Ar5E8mzsQvQbaAN+ADyP5CrTSlo++O+c5K6/56fTlbSeGt32Y+zn8vRL74XAbSQPspq1/c30ezOwuKYs13/bhdgjYHyPzZxNTo6IrQDp+0lp+az7O6RDAM8B7maW9zsdJrkP2AZ8HXgE2BXJY15heL/G9RjYae6DwJ8CA+l8O7O7v1UBfE3SvZKuTsty/bed622op5FxPRKzAGbV30HSPOBzwP+IiD3SqA9wnxX9joh+4BxJi4AvAM+oVy19n9F9lvQyYFtE3Cvpwmpxnaqzor81XhARWySdBHxd0n+OUXdS+l2UPYLxPDZzNvmZpCUA6fu2tHzW/B0kNZGEwKci4vNp8azvN0BE7AK+SXJ8ZFH6mFcY3q+Z/hjYFwCXSNoM3EIyPPRBZm9/B0XElvR9G0ngn0fO/7aLEgTjeWzmbJJ9BOjrSMbQq+WvTc80eD6wu7q7OZMo+en/CeDhiHh/ZtGs7bekjnRPAElzgBeRHET9BsljXmFkn2fsY2Aj4p0RsTwiVpL8/3pnRFzOLO1vlaS5kuZXp4GXAA+Q97/tRh8YmcIDMBcD/0UyrlOSXmsAAAIaSURBVPpnjW7PJPbrZmArcITk18GVJGOjdwA/Tt9PTOuK5OypR4D7gc5Gt3+Cff4lkt3fDcB96evi2dxv4Gzgh2mfHwCuSctPB/4D2AR8BmhJy1vT+U3p8tMb3Yfj6PuFwG1F6G/avx+lrwer31V5/9v2LSbMzAquKENDZmY2CgeBmVnBOQjMzArOQWBmVnAOAjOzgnMQmNWQ1J/e+bH6mrS71UpaqcydYs2mg6LcYsLsWByMiHMa3QizqeI9ArNxSu8T/7fpcwH+Q9KZafkKSXek94O/Q9KpafnJkr6QPkPgR5IuSFdVlvTx9LkCX0uvFDZrGAeB2UhzaoaGXp1ZticizgM+THLvG9Lpf4qIs4FPAR9Kyz8EfCuSZwicS3KlKCT3jr8+Ip4J7AJekXN/zMbkK4vNakjaFxHz6pRvJnk4zKPpTe+ejIh2SU+R3AP+SFq+NSIWS+oBlkfE4cw6VgJfj+QBI0h6O9AUEe/Jv2dm9XmPwOzYxCjTo9Wp53Bmuh8fq7MGcxCYHZtXZ96/l07fRXKHTIDLge+k03cAfwCDD5VZMFWNNDsW/iViNtKc9ElgVf8WEdVTSFsk3U3yI2pNWvZm4EZJfwL0AG9Iy98CrJV0Jckv/z8guVOs2bTiYwRm45QeI+iMiKca3RazyeShITOzgvMegZlZwXmPwMys4BwEZmYF5yAwMys4B4GZWcE5CMzMCu7/AzvBmsoU08eNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss \n",
    "\n",
    "nn2.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred2 = nn2.predict(Xtrain2)\n",
    "test_pred2 = nn2.predict(Xtest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 99%\n",
      "Test accuracy is 97%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn2.accuracy(ytrain2, train_pred2)))\n",
    "print(\"Test accuracy is {}%\".format(nn2.accuracy(ytest2, test_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sonar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe\n",
    "\n",
    "sonar_df = pd.read_csv('Data/sonar.all-data', header=None, names = ['variable {}'.format(i) for i in range(0,60)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 60)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "sonar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X3 = np.array(sonar_df.drop(columns = ['variable 59']))\n",
    "\n",
    "sonar_df['variable 59'] = sonar_df['variable 59'].replace({'R': 0, 'M': 1})\n",
    "\n",
    "y_label3 = sonar_df['variable 59'].values.reshape(X3.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain3, Xtest3, ytrain3, ytest3 = train_test_split(X3, y_label3, test_size = 0.3, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new neural network model with layers corresponding to input dimensions of 59 variables\n",
    "\n",
    "nn3 = NeuralNet(layers=[59, 80, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1, calculated loss: 4.8807980404254385\n",
      "Training epoch 2, calculated loss: 7.592149524696717\n",
      "Training epoch 3, calculated loss: 8.82572534847706\n",
      "Training epoch 4, calculated loss: 4.111597939351076\n",
      "Training epoch 5, calculated loss: 4.157141409681627\n",
      "Training epoch 6, calculated loss: 7.40821155954874\n",
      "Training epoch 7, calculated loss: 9.142265693666857\n",
      "Training epoch 8, calculated loss: 3.2723552595313374\n",
      "Training epoch 9, calculated loss: 2.5262529544594514\n",
      "Training epoch 10, calculated loss: 1.921193919595365\n",
      "Training epoch 11, calculated loss: 1.9269248570638207\n",
      "Training epoch 12, calculated loss: 8.873194401907265\n",
      "Training epoch 13, calculated loss: 9.90011663236301\n",
      "Training epoch 14, calculated loss: 2.346977567740791\n",
      "Training epoch 15, calculated loss: 9.232486550573352\n",
      "Training epoch 16, calculated loss: 7.211978939466792\n",
      "Training epoch 17, calculated loss: 6.5035127804178865\n",
      "Training epoch 18, calculated loss: 9.134461110037808\n",
      "Training epoch 19, calculated loss: 1.7687943246481586\n",
      "Training epoch 20, calculated loss: 1.3593981841254514\n",
      "Training epoch 21, calculated loss: 1.3112695199402042\n",
      "Training epoch 22, calculated loss: 4.084267559276362\n",
      "Training epoch 23, calculated loss: 9.781098916244048\n",
      "Training epoch 24, calculated loss: 2.7237426576804284\n",
      "Training epoch 25, calculated loss: 8.135012905628807\n",
      "Training epoch 26, calculated loss: 5.9623538632704145\n",
      "Training epoch 27, calculated loss: 4.655854869832118\n",
      "Training epoch 28, calculated loss: 7.151895274950255\n",
      "Training epoch 29, calculated loss: 2.042783572303769\n",
      "Training epoch 30, calculated loss: 4.284680222094108\n",
      "Training epoch 31, calculated loss: 4.688344264007755\n",
      "Training epoch 32, calculated loss: 5.716725412582923\n",
      "Training epoch 33, calculated loss: 2.4806544677617404\n",
      "Training epoch 34, calculated loss: 4.337337530472779\n",
      "Training epoch 35, calculated loss: 3.0890293420021413\n",
      "Training epoch 36, calculated loss: 4.511693114378722\n",
      "Training epoch 37, calculated loss: 2.398712844488769\n",
      "Training epoch 38, calculated loss: 3.491045412771448\n",
      "Training epoch 39, calculated loss: 2.4819028776557874\n",
      "Training epoch 40, calculated loss: 3.342854006194561\n",
      "Training epoch 41, calculated loss: 2.1476585318558667\n",
      "Training epoch 42, calculated loss: 2.8003757134264418\n",
      "Training epoch 43, calculated loss: 1.934466461470069\n",
      "Training epoch 44, calculated loss: 2.5187261613951244\n",
      "Training epoch 45, calculated loss: 1.7428322935266567\n",
      "Training epoch 46, calculated loss: 2.2616062907901355\n",
      "Training epoch 47, calculated loss: 1.5625603438439186\n",
      "Training epoch 48, calculated loss: 2.0034190719992773\n",
      "Training epoch 49, calculated loss: 1.4030635928929311\n",
      "Training epoch 50, calculated loss: 1.7521630806880275\n",
      "Training epoch 51, calculated loss: 1.273684731868376\n",
      "Training epoch 52, calculated loss: 1.5512372746044194\n",
      "Training epoch 53, calculated loss: 1.173612467472493\n",
      "Training epoch 54, calculated loss: 1.3966503804214656\n",
      "Training epoch 55, calculated loss: 1.087647933329968\n",
      "Training epoch 56, calculated loss: 1.2700587024897338\n",
      "Training epoch 57, calculated loss: 1.0147565310516495\n",
      "Training epoch 58, calculated loss: 1.1766648187178848\n",
      "Training epoch 59, calculated loss: 0.9535916951684251\n",
      "Training epoch 60, calculated loss: 1.1002317590198638\n",
      "Training epoch 61, calculated loss: 0.9021369750848637\n",
      "Training epoch 62, calculated loss: 1.0383253009315059\n",
      "Training epoch 63, calculated loss: 0.8600608776889932\n",
      "Training epoch 64, calculated loss: 0.9926795740286777\n",
      "Training epoch 65, calculated loss: 0.8255188462058343\n",
      "Training epoch 66, calculated loss: 0.9519240459963921\n",
      "Training epoch 67, calculated loss: 0.7924119585867488\n",
      "Training epoch 68, calculated loss: 0.9174356476141564\n",
      "Training epoch 69, calculated loss: 0.7666776808905008\n",
      "Training epoch 70, calculated loss: 0.8862224380518048\n",
      "Training epoch 71, calculated loss: 0.7383950767719601\n",
      "Training epoch 72, calculated loss: 0.8436473712294328\n",
      "Training epoch 73, calculated loss: 0.707305998016325\n",
      "Training epoch 74, calculated loss: 0.8013147913125753\n",
      "Training epoch 75, calculated loss: 0.6773621327493103\n",
      "Training epoch 76, calculated loss: 0.7597567984339727\n",
      "Training epoch 77, calculated loss: 0.65251849192778\n",
      "Training epoch 78, calculated loss: 0.726016318857926\n",
      "Training epoch 79, calculated loss: 0.6314015798563994\n",
      "Training epoch 80, calculated loss: 0.7018420587756266\n",
      "Training epoch 81, calculated loss: 0.6163043270475775\n",
      "Training epoch 82, calculated loss: 0.6823940965343799\n",
      "Training epoch 83, calculated loss: 0.6007154852752004\n",
      "Training epoch 84, calculated loss: 0.6603412731303475\n",
      "Training epoch 85, calculated loss: 0.5875366178169877\n",
      "Training epoch 86, calculated loss: 0.6436504351688591\n",
      "Training epoch 87, calculated loss: 0.5725926060877914\n",
      "Training epoch 88, calculated loss: 0.6242674909446966\n",
      "Training epoch 89, calculated loss: 0.5595495527527045\n",
      "Training epoch 90, calculated loss: 0.608724685615758\n",
      "Training epoch 91, calculated loss: 0.5487664824757806\n",
      "Training epoch 92, calculated loss: 0.594368876439711\n",
      "Training epoch 93, calculated loss: 0.5398884551111582\n",
      "Training epoch 94, calculated loss: 0.5863756533650157\n",
      "Training epoch 95, calculated loss: 0.5307985174447539\n",
      "Training epoch 96, calculated loss: 0.5738251534727422\n",
      "Training epoch 97, calculated loss: 0.5228787634692659\n",
      "Training epoch 98, calculated loss: 0.5626799409910206\n",
      "Training epoch 99, calculated loss: 0.5125204148956508\n",
      "Training epoch 100, calculated loss: 0.5508340972227278\n",
      "Training epoch 101, calculated loss: 0.5046364334004982\n",
      "Training epoch 102, calculated loss: 0.5407921672567811\n",
      "Training epoch 103, calculated loss: 0.497785611617559\n",
      "Training epoch 104, calculated loss: 0.5308666584534975\n",
      "Training epoch 105, calculated loss: 0.48816933876456364\n",
      "Training epoch 106, calculated loss: 0.5180171027310558\n",
      "Training epoch 107, calculated loss: 0.4761688514396342\n",
      "Training epoch 108, calculated loss: 0.5030489762155981\n",
      "Training epoch 109, calculated loss: 0.46705813594769974\n",
      "Training epoch 110, calculated loss: 0.4921896667866742\n",
      "Training epoch 111, calculated loss: 0.45637375512382206\n",
      "Training epoch 112, calculated loss: 0.47926147718259643\n",
      "Training epoch 113, calculated loss: 0.4468052747451304\n",
      "Training epoch 114, calculated loss: 0.4675215328786882\n",
      "Training epoch 115, calculated loss: 0.43763819706625495\n",
      "Training epoch 116, calculated loss: 0.45666817974286555\n",
      "Training epoch 117, calculated loss: 0.4309716438494146\n",
      "Training epoch 118, calculated loss: 0.4495475578539997\n",
      "Training epoch 119, calculated loss: 0.42302393616635786\n",
      "Training epoch 120, calculated loss: 0.4394898621464276\n",
      "Training epoch 121, calculated loss: 0.4166350924235551\n",
      "Training epoch 122, calculated loss: 0.43243737684220696\n",
      "Training epoch 123, calculated loss: 0.4108388063672881\n",
      "Training epoch 124, calculated loss: 0.4259479175677928\n",
      "Training epoch 125, calculated loss: 0.4065944631773517\n",
      "Training epoch 126, calculated loss: 0.4212961909749562\n",
      "Training epoch 127, calculated loss: 0.40126412102105347\n",
      "Training epoch 128, calculated loss: 0.4154050716207778\n",
      "Training epoch 129, calculated loss: 0.39692235545985644\n",
      "Training epoch 130, calculated loss: 0.41072229578124936\n",
      "Training epoch 131, calculated loss: 0.39305837516842285\n",
      "Training epoch 132, calculated loss: 0.4062559883422092\n",
      "Training epoch 133, calculated loss: 0.388516770675242\n",
      "Training epoch 134, calculated loss: 0.40121803716634824\n",
      "Training epoch 135, calculated loss: 0.38432311059159396\n",
      "Training epoch 136, calculated loss: 0.3966897906941587\n",
      "Training epoch 137, calculated loss: 0.3805476426414489\n",
      "Training epoch 138, calculated loss: 0.39201906404765147\n",
      "Training epoch 139, calculated loss: 0.37598068659002826\n",
      "Training epoch 140, calculated loss: 0.3884200203564003\n",
      "Training epoch 141, calculated loss: 0.37221676436849005\n",
      "Training epoch 142, calculated loss: 0.3827025690686479\n",
      "Training epoch 143, calculated loss: 0.3670456571352279\n",
      "Training epoch 144, calculated loss: 0.3769410842812279\n",
      "Training epoch 145, calculated loss: 0.36290617892339516\n",
      "Training epoch 146, calculated loss: 0.37244115962483765\n",
      "Training epoch 147, calculated loss: 0.35921330827234615\n",
      "Training epoch 148, calculated loss: 0.3683303121158859\n",
      "Training epoch 149, calculated loss: 0.3545212266195792\n",
      "Training epoch 150, calculated loss: 0.3647687583212692\n",
      "Training epoch 151, calculated loss: 0.3518470006559172\n",
      "Training epoch 152, calculated loss: 0.3606027881223835\n",
      "Training epoch 153, calculated loss: 0.34833934880862\n",
      "Training epoch 154, calculated loss: 0.3566669030933647\n",
      "Training epoch 155, calculated loss: 0.34441522987012063\n",
      "Training epoch 156, calculated loss: 0.3523692009656374\n",
      "Training epoch 157, calculated loss: 0.34072188702271544\n",
      "Training epoch 158, calculated loss: 0.3496369815140565\n",
      "Training epoch 159, calculated loss: 0.3390714480764577\n",
      "Training epoch 160, calculated loss: 0.34684518395004255\n",
      "Training epoch 161, calculated loss: 0.33552082201737404\n",
      "Training epoch 162, calculated loss: 0.34314040863689577\n",
      "Training epoch 163, calculated loss: 0.33285615294612547\n",
      "Training epoch 164, calculated loss: 0.3401722742065142\n",
      "Training epoch 165, calculated loss: 0.3294862724366971\n",
      "Training epoch 166, calculated loss: 0.3378139115986085\n",
      "Training epoch 167, calculated loss: 0.32843180766221036\n",
      "Training epoch 168, calculated loss: 0.3361969646522565\n",
      "Training epoch 169, calculated loss: 0.32508742702697985\n",
      "Training epoch 170, calculated loss: 0.33274357565786006\n",
      "Training epoch 171, calculated loss: 0.32350692783346524\n",
      "Training epoch 172, calculated loss: 0.330407491304574\n",
      "Training epoch 173, calculated loss: 0.3203290621856485\n",
      "Training epoch 174, calculated loss: 0.32686450609023204\n",
      "Training epoch 175, calculated loss: 0.3178161982436203\n",
      "Training epoch 176, calculated loss: 0.32448104740948097\n",
      "Training epoch 177, calculated loss: 0.3155670944587417\n",
      "Training epoch 178, calculated loss: 0.32322691860213354\n",
      "Training epoch 179, calculated loss: 0.31440822201743546\n",
      "Training epoch 180, calculated loss: 0.3220037446187195\n",
      "Training epoch 181, calculated loss: 0.3127961518788952\n",
      "Training epoch 182, calculated loss: 0.3200863446156124\n",
      "Training epoch 183, calculated loss: 0.3120382711340796\n",
      "Training epoch 184, calculated loss: 0.3188729786356481\n",
      "Training epoch 185, calculated loss: 0.30808089429422136\n",
      "Training epoch 186, calculated loss: 0.31298557144302774\n",
      "Training epoch 187, calculated loss: 0.3025952392199226\n",
      "Training epoch 188, calculated loss: 0.30675148847334377\n",
      "Training epoch 189, calculated loss: 0.2971668699477486\n",
      "Training epoch 190, calculated loss: 0.3008043403648585\n",
      "Training epoch 191, calculated loss: 0.29238284961701877\n",
      "Training epoch 192, calculated loss: 0.2967674187374941\n",
      "Training epoch 193, calculated loss: 0.2886970791029165\n",
      "Training epoch 194, calculated loss: 0.2921750176123842\n",
      "Training epoch 195, calculated loss: 0.2851332301787894\n",
      "Training epoch 196, calculated loss: 0.28838703521493386\n",
      "Training epoch 197, calculated loss: 0.28145918887976906\n",
      "Training epoch 198, calculated loss: 0.28578600907466656\n",
      "Training epoch 199, calculated loss: 0.2802059682602916\n",
      "Training epoch 200, calculated loss: 0.2843878048487177\n",
      "Training epoch 201, calculated loss: 0.2784309889790165\n",
      "Training epoch 202, calculated loss: 0.28143479237350943\n",
      "Training epoch 203, calculated loss: 0.275220691441885\n",
      "Training epoch 204, calculated loss: 0.27898150156690554\n",
      "Training epoch 205, calculated loss: 0.27305884536060454\n",
      "Training epoch 206, calculated loss: 0.27573770892999644\n",
      "Training epoch 207, calculated loss: 0.2697525314163717\n",
      "Training epoch 208, calculated loss: 0.272011779294947\n",
      "Training epoch 209, calculated loss: 0.26593490830955985\n",
      "Training epoch 210, calculated loss: 0.26920777252762196\n",
      "Training epoch 211, calculated loss: 0.2649463519034471\n",
      "Training epoch 212, calculated loss: 0.2681539995266642\n",
      "Training epoch 213, calculated loss: 0.26347493995794063\n",
      "Training epoch 214, calculated loss: 0.2667134588180048\n",
      "Training epoch 215, calculated loss: 0.26290772820212527\n",
      "Training epoch 216, calculated loss: 0.2661344339199025\n",
      "Training epoch 217, calculated loss: 0.2603037866598469\n",
      "Training epoch 218, calculated loss: 0.26216318666998445\n",
      "Training epoch 219, calculated loss: 0.2572920822513607\n",
      "Training epoch 220, calculated loss: 0.25904651046915594\n",
      "Training epoch 221, calculated loss: 0.25401518842473614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 222, calculated loss: 0.2567581816807833\n",
      "Training epoch 223, calculated loss: 0.25316644629303486\n",
      "Training epoch 224, calculated loss: 0.2557753066875113\n",
      "Training epoch 225, calculated loss: 0.2515241090929929\n",
      "Training epoch 226, calculated loss: 0.25420005300033227\n",
      "Training epoch 227, calculated loss: 0.2501911106395019\n",
      "Training epoch 228, calculated loss: 0.25188876279894273\n",
      "Training epoch 229, calculated loss: 0.24702632514321965\n",
      "Training epoch 230, calculated loss: 0.24860317751189548\n",
      "Training epoch 231, calculated loss: 0.2451409281587686\n",
      "Training epoch 232, calculated loss: 0.24753019144668856\n",
      "Training epoch 233, calculated loss: 0.2436713679545271\n",
      "Training epoch 234, calculated loss: 0.24614171719256614\n",
      "Training epoch 235, calculated loss: 0.2430606708889134\n",
      "Training epoch 236, calculated loss: 0.24561998759259152\n",
      "Training epoch 237, calculated loss: 0.24292013387101621\n",
      "Training epoch 238, calculated loss: 0.245285515602299\n",
      "Training epoch 239, calculated loss: 0.24202759037221058\n",
      "Training epoch 240, calculated loss: 0.2444152685089157\n",
      "Training epoch 241, calculated loss: 0.24083588952394822\n",
      "Training epoch 242, calculated loss: 0.24337318102846747\n",
      "Training epoch 243, calculated loss: 0.23908194745938324\n",
      "Training epoch 244, calculated loss: 0.2406475431799664\n",
      "Training epoch 245, calculated loss: 0.23644408620380336\n",
      "Training epoch 246, calculated loss: 0.23875601792602286\n",
      "Training epoch 247, calculated loss: 0.2362407352050055\n",
      "Training epoch 248, calculated loss: 0.23829469837412096\n",
      "Training epoch 249, calculated loss: 0.2342927724568221\n",
      "Training epoch 250, calculated loss: 0.23545943168504957\n",
      "Training epoch 251, calculated loss: 0.23176727623149207\n",
      "Training epoch 252, calculated loss: 0.23354502484530879\n",
      "Training epoch 253, calculated loss: 0.2305317908595548\n",
      "Training epoch 254, calculated loss: 0.23251340832595319\n",
      "Training epoch 255, calculated loss: 0.22923774638393876\n",
      "Training epoch 256, calculated loss: 0.23128393454888993\n",
      "Training epoch 257, calculated loss: 0.22801412950054492\n",
      "Training epoch 258, calculated loss: 0.2302771289868142\n",
      "Training epoch 259, calculated loss: 0.22815600404451655\n",
      "Training epoch 260, calculated loss: 0.23020045967368144\n",
      "Training epoch 261, calculated loss: 0.22579857625363875\n",
      "Training epoch 262, calculated loss: 0.22683299147231384\n",
      "Training epoch 263, calculated loss: 0.22358728916362064\n",
      "Training epoch 264, calculated loss: 0.2243987414749155\n",
      "Training epoch 265, calculated loss: 0.22182523293494594\n",
      "Training epoch 266, calculated loss: 0.2235408778682101\n",
      "Training epoch 267, calculated loss: 0.22153525734370355\n",
      "Training epoch 268, calculated loss: 0.22333360183506007\n",
      "Training epoch 269, calculated loss: 0.22022825952863329\n",
      "Training epoch 270, calculated loss: 0.22192389035522544\n",
      "Training epoch 271, calculated loss: 0.220126658377719\n",
      "Training epoch 272, calculated loss: 0.22200804231086677\n",
      "Training epoch 273, calculated loss: 0.21950202083603368\n",
      "Training epoch 274, calculated loss: 0.2212438071397033\n",
      "Training epoch 275, calculated loss: 0.2188977559554726\n",
      "Training epoch 276, calculated loss: 0.2207562521900618\n",
      "Training epoch 277, calculated loss: 0.21867817415516766\n",
      "Training epoch 278, calculated loss: 0.2205819111382623\n",
      "Training epoch 279, calculated loss: 0.21682774207953617\n",
      "Training epoch 280, calculated loss: 0.21767543429454145\n",
      "Training epoch 281, calculated loss: 0.21486056234562348\n",
      "Training epoch 282, calculated loss: 0.21551772245742828\n",
      "Training epoch 283, calculated loss: 0.2125201213708388\n",
      "Training epoch 284, calculated loss: 0.21419309620481525\n",
      "Training epoch 285, calculated loss: 0.21311097259144257\n",
      "Training epoch 286, calculated loss: 0.21443135596948634\n",
      "Training epoch 287, calculated loss: 0.211625626181299\n",
      "Training epoch 288, calculated loss: 0.2131913148260483\n",
      "Training epoch 289, calculated loss: 0.21206384791977415\n",
      "Training epoch 290, calculated loss: 0.2132581628350166\n",
      "Training epoch 291, calculated loss: 0.21113115429176735\n",
      "Training epoch 292, calculated loss: 0.21266865968191415\n",
      "Training epoch 293, calculated loss: 0.2117190634217544\n",
      "Training epoch 294, calculated loss: 0.21312303071241367\n",
      "Training epoch 295, calculated loss: 0.2099672014195293\n",
      "Training epoch 296, calculated loss: 0.21133312765374526\n",
      "Training epoch 297, calculated loss: 0.2091512107510743\n",
      "Training epoch 298, calculated loss: 0.21039567339720955\n",
      "Training epoch 299, calculated loss: 0.20839254703200438\n",
      "Training epoch 300, calculated loss: 0.2094494002992896\n",
      "Training epoch 301, calculated loss: 0.20741356699353833\n",
      "Training epoch 302, calculated loss: 0.20846701182038035\n",
      "Training epoch 303, calculated loss: 0.20581463380028225\n",
      "Training epoch 304, calculated loss: 0.20805623646451915\n",
      "Training epoch 305, calculated loss: 0.20626665827591623\n",
      "Training epoch 306, calculated loss: 0.2083637763927286\n",
      "Training epoch 307, calculated loss: 0.2074028624441012\n",
      "Training epoch 308, calculated loss: 0.2090465782616874\n",
      "Training epoch 309, calculated loss: 0.20815012877337483\n",
      "Training epoch 310, calculated loss: 0.20973587370820504\n",
      "Training epoch 311, calculated loss: 0.20877193202505157\n",
      "Training epoch 312, calculated loss: 0.2101430917735797\n",
      "Training epoch 313, calculated loss: 0.206378762891832\n",
      "Training epoch 314, calculated loss: 0.20768195840610384\n",
      "Training epoch 315, calculated loss: 0.20574041619838587\n",
      "Training epoch 316, calculated loss: 0.2066461808984588\n",
      "Training epoch 317, calculated loss: 0.2045346367105215\n",
      "Training epoch 318, calculated loss: 0.20481930122399894\n",
      "Training epoch 319, calculated loss: 0.20126684355665214\n",
      "Training epoch 320, calculated loss: 0.20164240629532318\n",
      "Training epoch 321, calculated loss: 0.19888846526693754\n",
      "Training epoch 322, calculated loss: 0.19877659852750124\n",
      "Training epoch 323, calculated loss: 0.1953866216765748\n",
      "Training epoch 324, calculated loss: 0.19538374109307027\n",
      "Training epoch 325, calculated loss: 0.19304007756627412\n",
      "Training epoch 326, calculated loss: 0.1925754387086129\n",
      "Training epoch 327, calculated loss: 0.1897115191964764\n",
      "Training epoch 328, calculated loss: 0.18931904054587487\n",
      "Training epoch 329, calculated loss: 0.18708527837506886\n",
      "Training epoch 330, calculated loss: 0.18654444002776888\n",
      "Training epoch 331, calculated loss: 0.18421353287667525\n",
      "Training epoch 332, calculated loss: 0.18356069940438421\n",
      "Training epoch 333, calculated loss: 0.1814888995850588\n",
      "Training epoch 334, calculated loss: 0.1811892508222703\n",
      "Training epoch 335, calculated loss: 0.17927648854166972\n",
      "Training epoch 336, calculated loss: 0.17863357579362915\n",
      "Training epoch 337, calculated loss: 0.17727252135538799\n",
      "Training epoch 338, calculated loss: 0.17691756455213248\n",
      "Training epoch 339, calculated loss: 0.17568929934048055\n",
      "Training epoch 340, calculated loss: 0.17511154917164146\n",
      "Training epoch 341, calculated loss: 0.1735482105577724\n",
      "Training epoch 342, calculated loss: 0.17329571922299356\n",
      "Training epoch 343, calculated loss: 0.17235682080667492\n",
      "Training epoch 344, calculated loss: 0.17169836387463316\n",
      "Training epoch 345, calculated loss: 0.17044604061066812\n",
      "Training epoch 346, calculated loss: 0.17008783380869832\n",
      "Training epoch 347, calculated loss: 0.16944158389809091\n",
      "Training epoch 348, calculated loss: 0.16881966363108572\n",
      "Training epoch 349, calculated loss: 0.16731663964947344\n",
      "Training epoch 350, calculated loss: 0.1669574970712201\n",
      "Training epoch 351, calculated loss: 0.1658600544955526\n",
      "Training epoch 352, calculated loss: 0.16523916881313347\n",
      "Training epoch 353, calculated loss: 0.1644801856715864\n",
      "Training epoch 354, calculated loss: 0.16411576397173527\n",
      "Training epoch 355, calculated loss: 0.1634336032766636\n",
      "Training epoch 356, calculated loss: 0.16304244088299374\n",
      "Training epoch 357, calculated loss: 0.16259985315664227\n",
      "Training epoch 358, calculated loss: 0.1619584727377762\n",
      "Training epoch 359, calculated loss: 0.16108830990344591\n",
      "Training epoch 360, calculated loss: 0.1606580885444469\n",
      "Training epoch 361, calculated loss: 0.15943730714854645\n",
      "Training epoch 362, calculated loss: 0.15883013406121516\n",
      "Training epoch 363, calculated loss: 0.1581656998945605\n",
      "Training epoch 364, calculated loss: 0.15775167682925204\n",
      "Training epoch 365, calculated loss: 0.15721162568311806\n",
      "Training epoch 366, calculated loss: 0.15665339175980864\n",
      "Training epoch 367, calculated loss: 0.15588851733369877\n",
      "Training epoch 368, calculated loss: 0.15552724173596083\n",
      "Training epoch 369, calculated loss: 0.15499629492056488\n",
      "Training epoch 370, calculated loss: 0.15441736851435317\n",
      "Training epoch 371, calculated loss: 0.15377710306716633\n",
      "Training epoch 372, calculated loss: 0.1532319671188244\n",
      "Training epoch 373, calculated loss: 0.1529939102572874\n",
      "Training epoch 374, calculated loss: 0.15224455339072915\n",
      "Training epoch 375, calculated loss: 0.15134155852109674\n",
      "Training epoch 376, calculated loss: 0.15074490295963144\n",
      "Training epoch 377, calculated loss: 0.1504734139784181\n",
      "Training epoch 378, calculated loss: 0.1497909612393987\n",
      "Training epoch 379, calculated loss: 0.14925345153877684\n",
      "Training epoch 380, calculated loss: 0.1487427970062584\n",
      "Training epoch 381, calculated loss: 0.14832539762273328\n",
      "Training epoch 382, calculated loss: 0.14774844495612255\n",
      "Training epoch 383, calculated loss: 0.14727412898010886\n",
      "Training epoch 384, calculated loss: 0.14674506043458999\n",
      "Training epoch 385, calculated loss: 0.14637547238855045\n",
      "Training epoch 386, calculated loss: 0.14571271586299717\n",
      "Training epoch 387, calculated loss: 0.14533057833232532\n",
      "Training epoch 388, calculated loss: 0.14479308467666394\n",
      "Training epoch 389, calculated loss: 0.14468666720196854\n",
      "Training epoch 390, calculated loss: 0.14394354239165247\n",
      "Training epoch 391, calculated loss: 0.14365510242473786\n",
      "Training epoch 392, calculated loss: 0.14312520700917053\n",
      "Training epoch 393, calculated loss: 0.1425483488299802\n",
      "Training epoch 394, calculated loss: 0.14208870114193817\n",
      "Training epoch 395, calculated loss: 0.14193635795358395\n",
      "Training epoch 396, calculated loss: 0.14131381011138403\n",
      "Training epoch 397, calculated loss: 0.14103470222945877\n",
      "Training epoch 398, calculated loss: 0.14047021287629655\n",
      "Training epoch 399, calculated loss: 0.14022501166599832\n",
      "Training epoch 400, calculated loss: 0.13963455831467939\n",
      "Training epoch 401, calculated loss: 0.13933145879479616\n",
      "Training epoch 402, calculated loss: 0.13873705872919176\n",
      "Training epoch 403, calculated loss: 0.13823178350182513\n",
      "Training epoch 404, calculated loss: 0.1378521454958514\n",
      "Training epoch 405, calculated loss: 0.13771284630631647\n",
      "Training epoch 406, calculated loss: 0.13711534327998784\n",
      "Training epoch 407, calculated loss: 0.13677776619274892\n",
      "Training epoch 408, calculated loss: 0.13637969789342264\n",
      "Training epoch 409, calculated loss: 0.13600519518037396\n",
      "Training epoch 410, calculated loss: 0.13541956872581415\n",
      "Training epoch 411, calculated loss: 0.13516389021294833\n",
      "Training epoch 412, calculated loss: 0.13474322434468408\n",
      "Training epoch 413, calculated loss: 0.13456734582117721\n",
      "Training epoch 414, calculated loss: 0.13411247984740501\n",
      "Training epoch 415, calculated loss: 0.1338021385076863\n",
      "Training epoch 416, calculated loss: 0.13345846159637734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 417, calculated loss: 0.1334589788343568\n",
      "Training epoch 418, calculated loss: 0.1327938720051603\n",
      "Training epoch 419, calculated loss: 0.13267623600571102\n",
      "Training epoch 420, calculated loss: 0.13212732982855893\n",
      "Training epoch 421, calculated loss: 0.13199037559649954\n",
      "Training epoch 422, calculated loss: 0.13148461215875706\n",
      "Training epoch 423, calculated loss: 0.13135790637609865\n",
      "Training epoch 424, calculated loss: 0.13081951034676814\n",
      "Training epoch 425, calculated loss: 0.1308160715532935\n",
      "Training epoch 426, calculated loss: 0.13020904565820948\n",
      "Training epoch 427, calculated loss: 0.1301332842367548\n",
      "Training epoch 428, calculated loss: 0.12954439009912422\n",
      "Training epoch 429, calculated loss: 0.12936244967772664\n",
      "Training epoch 430, calculated loss: 0.12879009836129812\n",
      "Training epoch 431, calculated loss: 0.12849553133820035\n",
      "Training epoch 432, calculated loss: 0.12797577642608343\n",
      "Training epoch 433, calculated loss: 0.1280888328061388\n",
      "Training epoch 434, calculated loss: 0.12756502178467047\n",
      "Training epoch 435, calculated loss: 0.12759077710613273\n",
      "Training epoch 436, calculated loss: 0.12693297771991846\n",
      "Training epoch 437, calculated loss: 0.12650779879369048\n",
      "Training epoch 438, calculated loss: 0.1264921227178182\n",
      "Training epoch 439, calculated loss: 0.12647853286843067\n",
      "Training epoch 440, calculated loss: 0.12578660353014112\n",
      "Training epoch 441, calculated loss: 0.12594247765601704\n",
      "Training epoch 442, calculated loss: 0.12537588199380556\n",
      "Training epoch 443, calculated loss: 0.12538442783782416\n",
      "Training epoch 444, calculated loss: 0.12472781296830442\n",
      "Training epoch 445, calculated loss: 0.12450683082123283\n",
      "Training epoch 446, calculated loss: 0.12426216996274836\n",
      "Training epoch 447, calculated loss: 0.12450955646808465\n",
      "Training epoch 448, calculated loss: 0.12377910945871902\n",
      "Training epoch 449, calculated loss: 0.12362793720369229\n",
      "Training epoch 450, calculated loss: 0.1233008979838309\n",
      "Training epoch 451, calculated loss: 0.12340548243286568\n",
      "Training epoch 452, calculated loss: 0.12266124437370528\n",
      "Training epoch 453, calculated loss: 0.12264106890602444\n",
      "Training epoch 454, calculated loss: 0.12232765727416965\n",
      "Training epoch 455, calculated loss: 0.12227467125443058\n",
      "Training epoch 456, calculated loss: 0.12171876945248138\n",
      "Training epoch 457, calculated loss: 0.12188837812421607\n",
      "Training epoch 458, calculated loss: 0.12129144378633959\n",
      "Training epoch 459, calculated loss: 0.12125481153511597\n",
      "Training epoch 460, calculated loss: 0.12080693164338795\n",
      "Training epoch 461, calculated loss: 0.12084032923286114\n",
      "Training epoch 462, calculated loss: 0.1202618000443313\n",
      "Training epoch 463, calculated loss: 0.1203228755172352\n",
      "Training epoch 464, calculated loss: 0.11970983332920826\n",
      "Training epoch 465, calculated loss: 0.1196012576170586\n",
      "Training epoch 466, calculated loss: 0.11916918504465576\n",
      "Training epoch 467, calculated loss: 0.11921805478660387\n",
      "Training epoch 468, calculated loss: 0.11870065593369067\n",
      "Training epoch 469, calculated loss: 0.11873635128989095\n",
      "Training epoch 470, calculated loss: 0.11816232315375008\n",
      "Training epoch 471, calculated loss: 0.11802619148394915\n",
      "Training epoch 472, calculated loss: 0.11760474563700093\n",
      "Training epoch 473, calculated loss: 0.11764844570949082\n",
      "Training epoch 474, calculated loss: 0.1171710667943919\n",
      "Training epoch 475, calculated loss: 0.11732573380809623\n",
      "Training epoch 476, calculated loss: 0.11678424265900418\n",
      "Training epoch 477, calculated loss: 0.11697010250473133\n",
      "Training epoch 478, calculated loss: 0.1163573988301814\n",
      "Training epoch 479, calculated loss: 0.1165593761611462\n",
      "Training epoch 480, calculated loss: 0.11595359095411474\n",
      "Training epoch 481, calculated loss: 0.11610077608856452\n",
      "Training epoch 482, calculated loss: 0.11550813177805096\n",
      "Training epoch 483, calculated loss: 0.11552668481336271\n",
      "Training epoch 484, calculated loss: 0.11497537163213477\n",
      "Training epoch 485, calculated loss: 0.1150329969031731\n",
      "Training epoch 486, calculated loss: 0.1144802089483459\n",
      "Training epoch 487, calculated loss: 0.11463837878980684\n",
      "Training epoch 488, calculated loss: 0.11405640226462486\n",
      "Training epoch 489, calculated loss: 0.1141582438552464\n",
      "Training epoch 490, calculated loss: 0.11350903184995789\n",
      "Training epoch 491, calculated loss: 0.1136280667334347\n",
      "Training epoch 492, calculated loss: 0.11298492351768341\n",
      "Training epoch 493, calculated loss: 0.11275257468264817\n",
      "Training epoch 494, calculated loss: 0.11232020322927812\n",
      "Training epoch 495, calculated loss: 0.11229653130418732\n",
      "Training epoch 496, calculated loss: 0.11181470940383487\n",
      "Training epoch 497, calculated loss: 0.11193840140443913\n",
      "Training epoch 498, calculated loss: 0.11144267062274786\n",
      "Training epoch 499, calculated loss: 0.11152959166164719\n",
      "Training epoch 500, calculated loss: 0.1110255374062235\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "nn3.fit(Xtrain3, ytrain3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "\n",
    "nn3.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred3 = nn3.predict(Xtrain3)\n",
    "test_pred3 = nn3.predict(Xtest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 97%\n",
      "Test accuracy is 77%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn3.accuracy(ytrain3, train_pred3)))\n",
    "print(\"Test accuracy is {}%\".format(nn3.accuracy(ytest3, test_pred3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
