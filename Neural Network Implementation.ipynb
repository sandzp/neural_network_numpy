{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy-only 2-layer Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from neural_net import NeuralNet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using UCI Heart Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header names\n",
    "\n",
    "headers = ['age', 'sex','chest_pain','resting_blood_pressure',  \n",
    "        'serum_cholestoral', 'fasting_blood_sugar', 'resting_ecg_results',\n",
    "        'max_heart_rate_achieved', 'exercise_induced_angina', 'oldpeak',\"slope of the peak\",\n",
    "        'num_of_major_vessels','thal', 'heart_disease']\n",
    "\n",
    "# Make DF\n",
    "\n",
    "heart_df = pd.read_csv('Data/heart.dat', sep = ' ', names = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        0\n",
       "sex                        0\n",
       "chest_pain                 0\n",
       "resting_blood_pressure     0\n",
       "serum_cholestoral          0\n",
       "fasting_blood_sugar        0\n",
       "resting_ecg_results        0\n",
       "max_heart_rate_achieved    0\n",
       "exercise_induced_angina    0\n",
       "oldpeak                    0\n",
       "slope of the peak          0\n",
       "num_of_major_vessels       0\n",
       "thal                       0\n",
       "heart_disease              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "heart_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        float64\n",
       "sex                        float64\n",
       "chest_pain                 float64\n",
       "resting_blood_pressure     float64\n",
       "serum_cholestoral          float64\n",
       "fasting_blood_sugar        float64\n",
       "resting_ecg_results        float64\n",
       "max_heart_rate_achieved    float64\n",
       "exercise_induced_angina    float64\n",
       "oldpeak                    float64\n",
       "slope of the peak          float64\n",
       "num_of_major_vessels       float64\n",
       "thal                       float64\n",
       "heart_disease                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "heart_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X = heart_df.drop(columns = ['heart_disease'])\n",
    "\n",
    "# Enumerate target class i.e. labels\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(1,0)\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(2,1)\n",
    "\n",
    "y_label = heart_df['heart_disease'].values.reshape(X.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (216, 13)\n",
      "Shape of test set is (54, 13)\n",
      "Shape of train label is (216, 1)\n",
      "Shape of test labels is (54, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale values\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(Xtrain)\n",
    "Xtrain = sc.transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)\n",
    "\n",
    "print(f\"Shape of train set is {Xtrain.shape}\")\n",
    "print(f\"Shape of test set is {Xtest.shape}\")\n",
    "print(f\"Shape of train label is {ytrain.shape}\")\n",
    "print(f\"Shape of test labels is {ytest.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model and Use it to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class of Neural Net using default parameters\n",
    "\n",
    "nn = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1, calculated loss: 1.406818099455712\n",
      "Training epoch 2, calculated loss: 1.2283644538724772\n",
      "Training epoch 3, calculated loss: 1.1027579343396263\n",
      "Training epoch 4, calculated loss: 1.010277834670556\n",
      "Training epoch 5, calculated loss: 0.9375456811690593\n",
      "Training epoch 6, calculated loss: 0.8771342674193834\n",
      "Training epoch 7, calculated loss: 0.8256029193292977\n",
      "Training epoch 8, calculated loss: 0.7809976438171825\n",
      "Training epoch 9, calculated loss: 0.7419983603962279\n",
      "Training epoch 10, calculated loss: 0.707447080573352\n",
      "Training epoch 11, calculated loss: 0.6761308099914313\n",
      "Training epoch 12, calculated loss: 0.6479719529677453\n",
      "Training epoch 13, calculated loss: 0.6225758410329949\n",
      "Training epoch 14, calculated loss: 0.5995873203513066\n",
      "Training epoch 15, calculated loss: 0.5787978131048888\n",
      "Training epoch 16, calculated loss: 0.5599182115487729\n",
      "Training epoch 17, calculated loss: 0.5427118970422696\n",
      "Training epoch 18, calculated loss: 0.5270878681118275\n",
      "Training epoch 19, calculated loss: 0.5127870376789314\n",
      "Training epoch 20, calculated loss: 0.499702605855535\n",
      "Training epoch 21, calculated loss: 0.48738521253819006\n",
      "Training epoch 22, calculated loss: 0.476190151824692\n",
      "Training epoch 23, calculated loss: 0.4660017416103337\n",
      "Training epoch 24, calculated loss: 0.4566221760866736\n",
      "Training epoch 25, calculated loss: 0.44801274903895083\n",
      "Training epoch 26, calculated loss: 0.44006195321689057\n",
      "Training epoch 27, calculated loss: 0.432662096983625\n",
      "Training epoch 28, calculated loss: 0.42579673582802635\n",
      "Training epoch 29, calculated loss: 0.4193526960161447\n",
      "Training epoch 30, calculated loss: 0.41324847065923803\n",
      "Training epoch 31, calculated loss: 0.4074599294185932\n",
      "Training epoch 32, calculated loss: 0.40202222246799607\n",
      "Training epoch 33, calculated loss: 0.3968899842644129\n",
      "Training epoch 34, calculated loss: 0.3920397357291702\n",
      "Training epoch 35, calculated loss: 0.3874509214748642\n",
      "Training epoch 36, calculated loss: 0.3830958350064345\n",
      "Training epoch 37, calculated loss: 0.3789607677725741\n",
      "Training epoch 38, calculated loss: 0.37502934313790387\n",
      "Training epoch 39, calculated loss: 0.37128637914437745\n",
      "Training epoch 40, calculated loss: 0.36771249418561974\n",
      "Training epoch 41, calculated loss: 0.3642833861662177\n",
      "Training epoch 42, calculated loss: 0.3610015600693919\n",
      "Training epoch 43, calculated loss: 0.3578906297976741\n",
      "Training epoch 44, calculated loss: 0.3549156523498778\n",
      "Training epoch 45, calculated loss: 0.3520709025825898\n",
      "Training epoch 46, calculated loss: 0.3493598084388319\n",
      "Training epoch 47, calculated loss: 0.346768845132244\n",
      "Training epoch 48, calculated loss: 0.34431068879480353\n",
      "Training epoch 49, calculated loss: 0.34194282652162544\n",
      "Training epoch 50, calculated loss: 0.3396583732933476\n",
      "Training epoch 51, calculated loss: 0.3374581250738244\n",
      "Training epoch 52, calculated loss: 0.33534760607313313\n",
      "Training epoch 53, calculated loss: 0.33330547606352195\n",
      "Training epoch 54, calculated loss: 0.33133938501962\n",
      "Training epoch 55, calculated loss: 0.32943711682743476\n",
      "Training epoch 56, calculated loss: 0.327561226258419\n",
      "Training epoch 57, calculated loss: 0.32575478917638145\n",
      "Training epoch 58, calculated loss: 0.3240014979718713\n",
      "Training epoch 59, calculated loss: 0.3222911256958769\n",
      "Training epoch 60, calculated loss: 0.320614686718432\n",
      "Training epoch 61, calculated loss: 0.3189950287443454\n",
      "Training epoch 62, calculated loss: 0.31743139477363524\n",
      "Training epoch 63, calculated loss: 0.315898744016534\n",
      "Training epoch 64, calculated loss: 0.3144261330659574\n",
      "Training epoch 65, calculated loss: 0.312979010473956\n",
      "Training epoch 66, calculated loss: 0.3115704557826012\n",
      "Training epoch 67, calculated loss: 0.3102387909138682\n",
      "Training epoch 68, calculated loss: 0.30892274240067386\n",
      "Training epoch 69, calculated loss: 0.30765010587991815\n",
      "Training epoch 70, calculated loss: 0.30639677395985704\n",
      "Training epoch 71, calculated loss: 0.3052113102149649\n",
      "Training epoch 72, calculated loss: 0.3040603570147317\n",
      "Training epoch 73, calculated loss: 0.30293845881919423\n",
      "Training epoch 74, calculated loss: 0.30183492009912677\n",
      "Training epoch 75, calculated loss: 0.3007443230734181\n",
      "Training epoch 76, calculated loss: 0.29967130343718507\n",
      "Training epoch 77, calculated loss: 0.29862341684204396\n",
      "Training epoch 78, calculated loss: 0.2975953115831883\n",
      "Training epoch 79, calculated loss: 0.2965805948342925\n",
      "Training epoch 80, calculated loss: 0.2955858072889425\n",
      "Training epoch 81, calculated loss: 0.2946025425015101\n",
      "Training epoch 82, calculated loss: 0.2936528622103815\n",
      "Training epoch 83, calculated loss: 0.29270959325424567\n",
      "Training epoch 84, calculated loss: 0.2917867375977857\n",
      "Training epoch 85, calculated loss: 0.2908766664032826\n",
      "Training epoch 86, calculated loss: 0.2899858794918911\n",
      "Training epoch 87, calculated loss: 0.2891217726542763\n",
      "Training epoch 88, calculated loss: 0.2882534060741006\n",
      "Training epoch 89, calculated loss: 0.28741232695069635\n",
      "Training epoch 90, calculated loss: 0.28658222439112885\n",
      "Training epoch 91, calculated loss: 0.2857393049629605\n",
      "Training epoch 92, calculated loss: 0.28488377530196873\n",
      "Training epoch 93, calculated loss: 0.2840357881439885\n",
      "Training epoch 94, calculated loss: 0.2831916166264467\n",
      "Training epoch 95, calculated loss: 0.2823680182360841\n",
      "Training epoch 96, calculated loss: 0.28153740084295914\n",
      "Training epoch 97, calculated loss: 0.28071874261248086\n",
      "Training epoch 98, calculated loss: 0.2799062326813389\n",
      "Training epoch 99, calculated loss: 0.2790974743022777\n",
      "Training epoch 100, calculated loss: 0.2783107364057317\n",
      "Training epoch 101, calculated loss: 0.27752267595856\n",
      "Training epoch 102, calculated loss: 0.276746158103033\n",
      "Training epoch 103, calculated loss: 0.27598304715732486\n",
      "Training epoch 104, calculated loss: 0.27526560371594744\n",
      "Training epoch 105, calculated loss: 0.2745564389923945\n",
      "Training epoch 106, calculated loss: 0.27386237629456256\n",
      "Training epoch 107, calculated loss: 0.27319294620008727\n",
      "Training epoch 108, calculated loss: 0.27252215813264585\n",
      "Training epoch 109, calculated loss: 0.2718580260151962\n",
      "Training epoch 110, calculated loss: 0.2711923110045857\n",
      "Training epoch 111, calculated loss: 0.2705296337982729\n",
      "Training epoch 112, calculated loss: 0.2698683935245679\n",
      "Training epoch 113, calculated loss: 0.269213933482101\n",
      "Training epoch 114, calculated loss: 0.2685728735027599\n",
      "Training epoch 115, calculated loss: 0.2679320976157896\n",
      "Training epoch 116, calculated loss: 0.26729387112097147\n",
      "Training epoch 117, calculated loss: 0.26666744357147076\n",
      "Training epoch 118, calculated loss: 0.2660492049364576\n",
      "Training epoch 119, calculated loss: 0.2654278058608459\n",
      "Training epoch 120, calculated loss: 0.26481833920220205\n",
      "Training epoch 121, calculated loss: 0.2642025845392992\n",
      "Training epoch 122, calculated loss: 0.2635980803620852\n",
      "Training epoch 123, calculated loss: 0.2629979179693585\n",
      "Training epoch 124, calculated loss: 0.26240444035825994\n",
      "Training epoch 125, calculated loss: 0.2618081094455079\n",
      "Training epoch 126, calculated loss: 0.26122323147646254\n",
      "Training epoch 127, calculated loss: 0.26063438451572873\n",
      "Training epoch 128, calculated loss: 0.2600492974146751\n",
      "Training epoch 129, calculated loss: 0.25946706587215085\n",
      "Training epoch 130, calculated loss: 0.2589003214821959\n",
      "Training epoch 131, calculated loss: 0.25832673077991153\n",
      "Training epoch 132, calculated loss: 0.25775349782981116\n",
      "Training epoch 133, calculated loss: 0.2571809593347549\n",
      "Training epoch 134, calculated loss: 0.2566217662792042\n",
      "Training epoch 135, calculated loss: 0.25605310226066813\n",
      "Training epoch 136, calculated loss: 0.2554915633487888\n",
      "Training epoch 137, calculated loss: 0.2549334344446226\n",
      "Training epoch 138, calculated loss: 0.25437693001749573\n",
      "Training epoch 139, calculated loss: 0.25382471688608255\n",
      "Training epoch 140, calculated loss: 0.25327245432645695\n",
      "Training epoch 141, calculated loss: 0.2527279179406165\n",
      "Training epoch 142, calculated loss: 0.25217915831365895\n",
      "Training epoch 143, calculated loss: 0.25163781252928474\n",
      "Training epoch 144, calculated loss: 0.2510910290687681\n",
      "Training epoch 145, calculated loss: 0.2505554057293129\n",
      "Training epoch 146, calculated loss: 0.25001024432803864\n",
      "Training epoch 147, calculated loss: 0.24947413070800084\n",
      "Training epoch 148, calculated loss: 0.24894197281573963\n",
      "Training epoch 149, calculated loss: 0.24841204559725721\n",
      "Training epoch 150, calculated loss: 0.2478847909568514\n",
      "Training epoch 151, calculated loss: 0.24735737248299336\n",
      "Training epoch 152, calculated loss: 0.2468379362565074\n",
      "Training epoch 153, calculated loss: 0.2463162713651728\n",
      "Training epoch 154, calculated loss: 0.24580155619808317\n",
      "Training epoch 155, calculated loss: 0.2452840582774365\n",
      "Training epoch 156, calculated loss: 0.24476858805520996\n",
      "Training epoch 157, calculated loss: 0.2442588084173961\n",
      "Training epoch 158, calculated loss: 0.2437435362084511\n",
      "Training epoch 159, calculated loss: 0.24323462312043742\n",
      "Training epoch 160, calculated loss: 0.24272893998407646\n",
      "Training epoch 161, calculated loss: 0.24223230619843128\n",
      "Training epoch 162, calculated loss: 0.24174067957462392\n",
      "Training epoch 163, calculated loss: 0.2412492025963735\n",
      "Training epoch 164, calculated loss: 0.24075542394516838\n",
      "Training epoch 165, calculated loss: 0.24026088887203656\n",
      "Training epoch 166, calculated loss: 0.23977173927195933\n",
      "Training epoch 167, calculated loss: 0.2392809939288868\n",
      "Training epoch 168, calculated loss: 0.23879307544349948\n",
      "Training epoch 169, calculated loss: 0.23831239789946876\n",
      "Training epoch 170, calculated loss: 0.2378248066660743\n",
      "Training epoch 171, calculated loss: 0.23734146228200687\n",
      "Training epoch 172, calculated loss: 0.23685621606243795\n",
      "Training epoch 173, calculated loss: 0.2363680485279514\n",
      "Training epoch 174, calculated loss: 0.23587090742674685\n",
      "Training epoch 175, calculated loss: 0.23537863633274222\n",
      "Training epoch 176, calculated loss: 0.2348903195279528\n",
      "Training epoch 177, calculated loss: 0.2344042567609047\n",
      "Training epoch 178, calculated loss: 0.23391175624870872\n",
      "Training epoch 179, calculated loss: 0.23343017770514024\n",
      "Training epoch 180, calculated loss: 0.23293822165842706\n",
      "Training epoch 181, calculated loss: 0.2324608315998643\n",
      "Training epoch 182, calculated loss: 0.23197788724357066\n",
      "Training epoch 183, calculated loss: 0.23149935858861018\n",
      "Training epoch 184, calculated loss: 0.2310124666397743\n",
      "Training epoch 185, calculated loss: 0.23053584748400333\n",
      "Training epoch 186, calculated loss: 0.23004806467142344\n",
      "Training epoch 187, calculated loss: 0.22957871113285785\n",
      "Training epoch 188, calculated loss: 0.22909778836401165\n",
      "Training epoch 189, calculated loss: 0.22862656328057268\n",
      "Training epoch 190, calculated loss: 0.22814334755455365\n",
      "Training epoch 191, calculated loss: 0.2276709835482682\n",
      "Training epoch 192, calculated loss: 0.22719038218911317\n",
      "Training epoch 193, calculated loss: 0.22672138605294875\n",
      "Training epoch 194, calculated loss: 0.22624451954760144\n",
      "Training epoch 195, calculated loss: 0.22578737541844868\n",
      "Training epoch 196, calculated loss: 0.2253161523571191\n",
      "Training epoch 197, calculated loss: 0.22485424516447283\n",
      "Training epoch 198, calculated loss: 0.22439134919349\n",
      "Training epoch 199, calculated loss: 0.2239280865753238\n",
      "Training epoch 200, calculated loss: 0.22346958764249777\n",
      "Training epoch 201, calculated loss: 0.22301092124771404\n",
      "Training epoch 202, calculated loss: 0.22255614764838536\n",
      "Training epoch 203, calculated loss: 0.22209400264876789\n",
      "Training epoch 204, calculated loss: 0.22164462420329273\n",
      "Training epoch 205, calculated loss: 0.22119825473660013\n",
      "Training epoch 206, calculated loss: 0.2207642169610009\n",
      "Training epoch 207, calculated loss: 0.22033261045718436\n",
      "Training epoch 208, calculated loss: 0.2199035580153558\n",
      "Training epoch 209, calculated loss: 0.2194706450749502\n",
      "Training epoch 210, calculated loss: 0.21904803815622734\n",
      "Training epoch 211, calculated loss: 0.21863185760205067\n",
      "Training epoch 212, calculated loss: 0.21819885067632558\n",
      "Training epoch 213, calculated loss: 0.21778363187425878\n",
      "Training epoch 214, calculated loss: 0.21735914641300724\n",
      "Training epoch 215, calculated loss: 0.2169282705446066\n",
      "Training epoch 216, calculated loss: 0.21651063110765484\n",
      "Training epoch 217, calculated loss: 0.21608748592860538\n",
      "Training epoch 218, calculated loss: 0.21566699955088842\n",
      "Training epoch 219, calculated loss: 0.21526024515912645\n",
      "Training epoch 220, calculated loss: 0.2148374729826649\n",
      "Training epoch 221, calculated loss: 0.21440432542116394\n",
      "Training epoch 222, calculated loss: 0.2139958620223014\n",
      "Training epoch 223, calculated loss: 0.2135679645465027\n",
      "Training epoch 224, calculated loss: 0.21315127653190946\n",
      "Training epoch 225, calculated loss: 0.2127425135582099\n",
      "Training epoch 226, calculated loss: 0.21232183920557945\n",
      "Training epoch 227, calculated loss: 0.2119010342251094\n",
      "Training epoch 228, calculated loss: 0.21148229859970416\n",
      "Training epoch 229, calculated loss: 0.21106058291309945\n",
      "Training epoch 230, calculated loss: 0.21065369209809542\n",
      "Training epoch 231, calculated loss: 0.21023967067439361\n",
      "Training epoch 232, calculated loss: 0.20982902672714915\n",
      "Training epoch 233, calculated loss: 0.20939613306028493\n",
      "Training epoch 234, calculated loss: 0.2089876071558346\n",
      "Training epoch 235, calculated loss: 0.20857927972805704\n",
      "Training epoch 236, calculated loss: 0.20816528682003066\n",
      "Training epoch 237, calculated loss: 0.2077553652163357\n",
      "Training epoch 238, calculated loss: 0.20733396932847467\n",
      "Training epoch 239, calculated loss: 0.20691243935101947\n",
      "Training epoch 240, calculated loss: 0.2064980819185977\n",
      "Training epoch 241, calculated loss: 0.20609397478307614\n",
      "Training epoch 242, calculated loss: 0.20566760276359936\n",
      "Training epoch 243, calculated loss: 0.20525538044164046\n",
      "Training epoch 244, calculated loss: 0.20484343749057887\n",
      "Training epoch 245, calculated loss: 0.20441927750730154\n",
      "Training epoch 246, calculated loss: 0.2040126296924069\n",
      "Training epoch 247, calculated loss: 0.20362076530831855\n",
      "Training epoch 248, calculated loss: 0.20319828652626068\n",
      "Training epoch 249, calculated loss: 0.20277780790284053\n",
      "Training epoch 250, calculated loss: 0.20236571298216968\n",
      "Training epoch 251, calculated loss: 0.20195238099992036\n",
      "Training epoch 252, calculated loss: 0.20156261838124942\n",
      "Training epoch 253, calculated loss: 0.20115952703475037\n",
      "Training epoch 254, calculated loss: 0.2007483563441138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 255, calculated loss: 0.2003377703110201\n",
      "Training epoch 256, calculated loss: 0.1999290356881397\n",
      "Training epoch 257, calculated loss: 0.19954320203371445\n",
      "Training epoch 258, calculated loss: 0.199182485115241\n",
      "Training epoch 259, calculated loss: 0.19880105043699905\n",
      "Training epoch 260, calculated loss: 0.19840192422729222\n",
      "Training epoch 261, calculated loss: 0.1980207292858923\n",
      "Training epoch 262, calculated loss: 0.19764848083957062\n",
      "Training epoch 263, calculated loss: 0.19726230645401954\n",
      "Training epoch 264, calculated loss: 0.19688050465637108\n",
      "Training epoch 265, calculated loss: 0.19651275770998625\n",
      "Training epoch 266, calculated loss: 0.19612384179538422\n",
      "Training epoch 267, calculated loss: 0.1957630147609834\n",
      "Training epoch 268, calculated loss: 0.1953722569566314\n",
      "Training epoch 269, calculated loss: 0.19498930212044344\n",
      "Training epoch 270, calculated loss: 0.19461641793214018\n",
      "Training epoch 271, calculated loss: 0.1942441575982174\n",
      "Training epoch 272, calculated loss: 0.19386863167293533\n",
      "Training epoch 273, calculated loss: 0.19348478056753204\n",
      "Training epoch 274, calculated loss: 0.19311064629426253\n",
      "Training epoch 275, calculated loss: 0.192726628477748\n",
      "Training epoch 276, calculated loss: 0.19235661948829577\n",
      "Training epoch 277, calculated loss: 0.19196502553151357\n",
      "Training epoch 278, calculated loss: 0.1915986397577637\n",
      "Training epoch 279, calculated loss: 0.19122341022500783\n",
      "Training epoch 280, calculated loss: 0.19083085175095132\n",
      "Training epoch 281, calculated loss: 0.19046145055031888\n",
      "Training epoch 282, calculated loss: 0.19009102370011108\n",
      "Training epoch 283, calculated loss: 0.18970648638050977\n",
      "Training epoch 284, calculated loss: 0.18934881202258527\n",
      "Training epoch 285, calculated loss: 0.1889738749204568\n",
      "Training epoch 286, calculated loss: 0.1886218398356439\n",
      "Training epoch 287, calculated loss: 0.18825106361899036\n",
      "Training epoch 288, calculated loss: 0.18786522308620968\n",
      "Training epoch 289, calculated loss: 0.1875090812628159\n",
      "Training epoch 290, calculated loss: 0.1871345473867985\n",
      "Training epoch 291, calculated loss: 0.18676501545272986\n",
      "Training epoch 292, calculated loss: 0.18641840187728403\n",
      "Training epoch 293, calculated loss: 0.18604482687305232\n",
      "Training epoch 294, calculated loss: 0.18568545639933703\n",
      "Training epoch 295, calculated loss: 0.18532975861852727\n",
      "Training epoch 296, calculated loss: 0.18498483777153138\n",
      "Training epoch 297, calculated loss: 0.18461185314995485\n",
      "Training epoch 298, calculated loss: 0.18425013332752715\n",
      "Training epoch 299, calculated loss: 0.18391152812839598\n",
      "Training epoch 300, calculated loss: 0.18355248732093069\n",
      "Training epoch 301, calculated loss: 0.18320601006803303\n",
      "Training epoch 302, calculated loss: 0.1828540115677848\n",
      "Training epoch 303, calculated loss: 0.18251686411492968\n",
      "Training epoch 304, calculated loss: 0.1821732197656081\n",
      "Training epoch 305, calculated loss: 0.1818170142445932\n",
      "Training epoch 306, calculated loss: 0.1814719297134536\n",
      "Training epoch 307, calculated loss: 0.18113902574112584\n",
      "Training epoch 308, calculated loss: 0.1807825447128808\n",
      "Training epoch 309, calculated loss: 0.18045659233714778\n",
      "Training epoch 310, calculated loss: 0.1801078197240961\n",
      "Training epoch 311, calculated loss: 0.17979673876733465\n",
      "Training epoch 312, calculated loss: 0.1794675592851685\n",
      "Training epoch 313, calculated loss: 0.1791372893996969\n",
      "Training epoch 314, calculated loss: 0.1788165049438514\n",
      "Training epoch 315, calculated loss: 0.17849558383987\n",
      "Training epoch 316, calculated loss: 0.17816066147475693\n",
      "Training epoch 317, calculated loss: 0.1778349304644559\n",
      "Training epoch 318, calculated loss: 0.17751311246325255\n",
      "Training epoch 319, calculated loss: 0.17719705052409218\n",
      "Training epoch 320, calculated loss: 0.17686148026632312\n",
      "Training epoch 321, calculated loss: 0.17655129211747075\n",
      "Training epoch 322, calculated loss: 0.17623781820387602\n",
      "Training epoch 323, calculated loss: 0.1759291444132664\n",
      "Training epoch 324, calculated loss: 0.1756326973705929\n",
      "Training epoch 325, calculated loss: 0.17534433945718303\n",
      "Training epoch 326, calculated loss: 0.17505326948737784\n",
      "Training epoch 327, calculated loss: 0.17476336279446636\n",
      "Training epoch 328, calculated loss: 0.17447883345769297\n",
      "Training epoch 329, calculated loss: 0.17419661358090496\n",
      "Training epoch 330, calculated loss: 0.1739487350853175\n",
      "Training epoch 331, calculated loss: 0.1736765846606785\n",
      "Training epoch 332, calculated loss: 0.1734207068236491\n",
      "Training epoch 333, calculated loss: 0.1731866336973259\n",
      "Training epoch 334, calculated loss: 0.17294227082277783\n",
      "Training epoch 335, calculated loss: 0.1726946777873684\n",
      "Training epoch 336, calculated loss: 0.17243647813121277\n",
      "Training epoch 337, calculated loss: 0.1721803136433498\n",
      "Training epoch 338, calculated loss: 0.17195028784810754\n",
      "Training epoch 339, calculated loss: 0.17169346297302743\n",
      "Training epoch 340, calculated loss: 0.17145706532276012\n",
      "Training epoch 341, calculated loss: 0.1711999783518241\n",
      "Training epoch 342, calculated loss: 0.17097264265344975\n",
      "Training epoch 343, calculated loss: 0.17072128054425517\n",
      "Training epoch 344, calculated loss: 0.17048209845709925\n",
      "Training epoch 345, calculated loss: 0.17023203315163624\n",
      "Training epoch 346, calculated loss: 0.1699941945008134\n",
      "Training epoch 347, calculated loss: 0.16974128122812893\n",
      "Training epoch 348, calculated loss: 0.16948323256936773\n",
      "Training epoch 349, calculated loss: 0.16924536060296808\n",
      "Training epoch 350, calculated loss: 0.16898732423520862\n",
      "Training epoch 351, calculated loss: 0.1687340466210543\n",
      "Training epoch 352, calculated loss: 0.16847927733185036\n",
      "Training epoch 353, calculated loss: 0.16825200969880444\n",
      "Training epoch 354, calculated loss: 0.1680184489296365\n",
      "Training epoch 355, calculated loss: 0.16775547357820084\n",
      "Training epoch 356, calculated loss: 0.16751159568829121\n",
      "Training epoch 357, calculated loss: 0.1672623117203635\n",
      "Training epoch 358, calculated loss: 0.1670342009015608\n",
      "Training epoch 359, calculated loss: 0.16680226621584215\n",
      "Training epoch 360, calculated loss: 0.16657145511818808\n",
      "Training epoch 361, calculated loss: 0.16630767040088318\n",
      "Training epoch 362, calculated loss: 0.16607419005800161\n",
      "Training epoch 363, calculated loss: 0.16585328429306176\n",
      "Training epoch 364, calculated loss: 0.16560557466739984\n",
      "Training epoch 365, calculated loss: 0.16536030056040155\n",
      "Training epoch 366, calculated loss: 0.16513041112805482\n",
      "Training epoch 367, calculated loss: 0.1648936309149001\n",
      "Training epoch 368, calculated loss: 0.16466072675561894\n",
      "Training epoch 369, calculated loss: 0.1644214020694981\n",
      "Training epoch 370, calculated loss: 0.16418212751306865\n",
      "Training epoch 371, calculated loss: 0.16394143349632975\n",
      "Training epoch 372, calculated loss: 0.16371446101932746\n",
      "Training epoch 373, calculated loss: 0.16349024136128779\n",
      "Training epoch 374, calculated loss: 0.16324980532824224\n",
      "Training epoch 375, calculated loss: 0.16301616856081388\n",
      "Training epoch 376, calculated loss: 0.16277348779936215\n",
      "Training epoch 377, calculated loss: 0.16255532024251432\n",
      "Training epoch 378, calculated loss: 0.16231595238518823\n",
      "Training epoch 379, calculated loss: 0.1620866097438773\n",
      "Training epoch 380, calculated loss: 0.16184450053069116\n",
      "Training epoch 381, calculated loss: 0.16163600750490625\n",
      "Training epoch 382, calculated loss: 0.16140096811266103\n",
      "Training epoch 383, calculated loss: 0.16116446442739135\n",
      "Training epoch 384, calculated loss: 0.16093534478141389\n",
      "Training epoch 385, calculated loss: 0.1607157263221618\n",
      "Training epoch 386, calculated loss: 0.1604797973286627\n",
      "Training epoch 387, calculated loss: 0.1602524754063829\n",
      "Training epoch 388, calculated loss: 0.16002321456542906\n",
      "Training epoch 389, calculated loss: 0.15980338227215293\n",
      "Training epoch 390, calculated loss: 0.15958713099884503\n",
      "Training epoch 391, calculated loss: 0.15938264923304102\n",
      "Training epoch 392, calculated loss: 0.15914168270242113\n",
      "Training epoch 393, calculated loss: 0.1589254563886855\n",
      "Training epoch 394, calculated loss: 0.15871048884938777\n",
      "Training epoch 395, calculated loss: 0.15849417766186147\n",
      "Training epoch 396, calculated loss: 0.158271723475272\n",
      "Training epoch 397, calculated loss: 0.15804571410781076\n",
      "Training epoch 398, calculated loss: 0.15784562631186577\n",
      "Training epoch 399, calculated loss: 0.1576369728677055\n",
      "Training epoch 400, calculated loss: 0.15742515984689748\n",
      "Training epoch 401, calculated loss: 0.1571858211718213\n",
      "Training epoch 402, calculated loss: 0.15698599481324108\n",
      "Training epoch 403, calculated loss: 0.1567750978752191\n",
      "Training epoch 404, calculated loss: 0.15655796423517826\n",
      "Training epoch 405, calculated loss: 0.1563291245160032\n",
      "Training epoch 406, calculated loss: 0.1561085345953448\n",
      "Training epoch 407, calculated loss: 0.155913303447273\n",
      "Training epoch 408, calculated loss: 0.15569104338576284\n",
      "Training epoch 409, calculated loss: 0.1554782958364362\n",
      "Training epoch 410, calculated loss: 0.15524938642490338\n",
      "Training epoch 411, calculated loss: 0.1550566188530461\n",
      "Training epoch 412, calculated loss: 0.15484971009912843\n",
      "Training epoch 413, calculated loss: 0.15462897713674464\n",
      "Training epoch 414, calculated loss: 0.1543972762265905\n",
      "Training epoch 415, calculated loss: 0.1542018293446751\n",
      "Training epoch 416, calculated loss: 0.15398717441734588\n",
      "Training epoch 417, calculated loss: 0.1537806086744322\n",
      "Training epoch 418, calculated loss: 0.15354170684750246\n",
      "Training epoch 419, calculated loss: 0.15335972380353524\n",
      "Training epoch 420, calculated loss: 0.15314548771175726\n",
      "Training epoch 421, calculated loss: 0.15292860566390828\n",
      "Training epoch 422, calculated loss: 0.15271025318384124\n",
      "Training epoch 423, calculated loss: 0.1525127899603239\n",
      "Training epoch 424, calculated loss: 0.15231247878553822\n",
      "Training epoch 425, calculated loss: 0.1521108764328087\n",
      "Training epoch 426, calculated loss: 0.15190066513571876\n",
      "Training epoch 427, calculated loss: 0.15174397750898871\n",
      "Training epoch 428, calculated loss: 0.15154324967660396\n",
      "Training epoch 429, calculated loss: 0.1513364468688277\n",
      "Training epoch 430, calculated loss: 0.1511480397586336\n",
      "Training epoch 431, calculated loss: 0.15095051070204293\n",
      "Training epoch 432, calculated loss: 0.1507677867515939\n",
      "Training epoch 433, calculated loss: 0.1505720638455779\n",
      "Training epoch 434, calculated loss: 0.1503764324283442\n",
      "Training epoch 435, calculated loss: 0.15019125186645826\n",
      "Training epoch 436, calculated loss: 0.14999649628648593\n",
      "Training epoch 437, calculated loss: 0.14981817237114212\n",
      "Training epoch 438, calculated loss: 0.14960372682190795\n",
      "Training epoch 439, calculated loss: 0.14943510557256834\n",
      "Training epoch 440, calculated loss: 0.14923156198711446\n",
      "Training epoch 441, calculated loss: 0.1490323110302897\n",
      "Training epoch 442, calculated loss: 0.14883953673094974\n",
      "Training epoch 443, calculated loss: 0.1486518024178283\n",
      "Training epoch 444, calculated loss: 0.14846139440125317\n",
      "Training epoch 445, calculated loss: 0.14826404317333006\n",
      "Training epoch 446, calculated loss: 0.14807262470931376\n",
      "Training epoch 447, calculated loss: 0.14789467609569903\n",
      "Training epoch 448, calculated loss: 0.14768747847749986\n",
      "Training epoch 449, calculated loss: 0.1474960417560906\n",
      "Training epoch 450, calculated loss: 0.14730509278325882\n",
      "Training epoch 451, calculated loss: 0.1471359462572863\n",
      "Training epoch 452, calculated loss: 0.14693179436827272\n",
      "Training epoch 453, calculated loss: 0.1467399177914218\n",
      "Training epoch 454, calculated loss: 0.14655330684991322\n",
      "Training epoch 455, calculated loss: 0.14637761127055562\n",
      "Training epoch 456, calculated loss: 0.1461791588962648\n",
      "Training epoch 457, calculated loss: 0.14598535187828576\n",
      "Training epoch 458, calculated loss: 0.14580865595688222\n",
      "Training epoch 459, calculated loss: 0.14562088851220337\n",
      "Training epoch 460, calculated loss: 0.14544189874108715\n",
      "Training epoch 461, calculated loss: 0.14524241900311582\n",
      "Training epoch 462, calculated loss: 0.14507407066121558\n",
      "Training epoch 463, calculated loss: 0.14487802313960552\n",
      "Training epoch 464, calculated loss: 0.14469551592094915\n",
      "Training epoch 465, calculated loss: 0.14451702463635938\n",
      "Training epoch 466, calculated loss: 0.14433866494052291\n",
      "Training epoch 467, calculated loss: 0.14415349748542902\n",
      "Training epoch 468, calculated loss: 0.1439582078598358\n",
      "Training epoch 469, calculated loss: 0.14378872473525617\n",
      "Training epoch 470, calculated loss: 0.14360374420805855\n",
      "Training epoch 471, calculated loss: 0.14341147973412957\n",
      "Training epoch 472, calculated loss: 0.14322867211484686\n",
      "Training epoch 473, calculated loss: 0.14305041911098115\n",
      "Training epoch 474, calculated loss: 0.142869843422236\n",
      "Training epoch 475, calculated loss: 0.14268017204880018\n",
      "Training epoch 476, calculated loss: 0.1425192685560983\n",
      "Training epoch 477, calculated loss: 0.14233459141981308\n",
      "Training epoch 478, calculated loss: 0.1421460365936759\n",
      "Training epoch 479, calculated loss: 0.1419797122652719\n",
      "Training epoch 480, calculated loss: 0.14182147289761057\n",
      "Training epoch 481, calculated loss: 0.14163200080777402\n",
      "Training epoch 482, calculated loss: 0.14143658552482288\n",
      "Training epoch 483, calculated loss: 0.14128617243524833\n",
      "Training epoch 484, calculated loss: 0.14111372747242554\n",
      "Training epoch 485, calculated loss: 0.14092477685885882\n",
      "Training epoch 486, calculated loss: 0.1407490590506156\n",
      "Training epoch 487, calculated loss: 0.14058417728075964\n",
      "Training epoch 488, calculated loss: 0.14042727248020903\n",
      "Training epoch 489, calculated loss: 0.1402195545926085\n",
      "Training epoch 490, calculated loss: 0.14006221153078655\n",
      "Training epoch 491, calculated loss: 0.13990036718235757\n",
      "Training epoch 492, calculated loss: 0.13972298261865274\n",
      "Training epoch 493, calculated loss: 0.13955754921744454\n",
      "Training epoch 494, calculated loss: 0.1393711758206643\n",
      "Training epoch 495, calculated loss: 0.139227297141135\n",
      "Training epoch 496, calculated loss: 0.1390445106681748\n",
      "Training epoch 497, calculated loss: 0.1388813906825923\n",
      "Training epoch 498, calculated loss: 0.13870089357746185\n",
      "Training epoch 499, calculated loss: 0.13854056453284114\n",
      "Training epoch 500, calculated loss: 0.13838941320395895\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "nn.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model loss\n",
    "\n",
    "nn.plot_loss(imagepath=\"Images/heart_disease_model1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 94%\n",
      "Test accuracy is 68%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn.accuracy(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {}%\".format(nn.accuracy(ytest, test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Banknote Authentication Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe\n",
    "\n",
    "banknote_df = pd.read_csv('Data/data_banknote_authentication.txt', sep = ',', names = ['variance', 'skewness', 'kurtosis', 'entropy', 'classification'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "banknote_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance          0\n",
       "skewness          0\n",
       "kurtosis          0\n",
       "entropy           0\n",
       "classification    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for na's\n",
    "\n",
    "banknote_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance          float64\n",
       "skewness          float64\n",
       "kurtosis          float64\n",
       "entropy           float64\n",
       "classification      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "\n",
    "banknote_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X2 = banknote_df.drop(columns = ['classification'])\n",
    "\n",
    "y_label2 = banknote_df['classification'].values.reshape(X2.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain2, Xtest2, ytrain2, ytest2 = train_test_split(X2, y_label2, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (1097, 4)\n",
      "Shape of test set is (275, 4)\n",
      "Shape of train label is (1097, 1)\n",
      "Shape of test labels is (275, 1)\n"
     ]
    }
   ],
   "source": [
    "# Scale Data\n",
    "\n",
    "sc.fit(Xtrain2)\n",
    "Xtrain2 = sc.transform(Xtrain2)\n",
    "Xtest2 = sc.transform(Xtest2)\n",
    "\n",
    "print(f\"Shape of train set is {Xtrain2.shape}\")\n",
    "print(f\"Shape of test set is {Xtest2.shape}\")\n",
    "print(f\"Shape of train label is {ytrain2.shape}\")\n",
    "print(f\"Shape of test labels is {ytest2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new neural network model with layers corresponding to input dimensions of 4\n",
    "\n",
    "nn2 = NeuralNet(layers=[4, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1, calculated loss: 1.5543208674243605\n",
      "Training epoch 2, calculated loss: 1.0206359831199538\n",
      "Training epoch 3, calculated loss: 1.0457455064409347\n",
      "Training epoch 4, calculated loss: 0.68995373571536\n",
      "Training epoch 5, calculated loss: 0.6397701113244398\n",
      "Training epoch 6, calculated loss: 0.541329566993813\n",
      "Training epoch 7, calculated loss: 0.4659455632611285\n",
      "Training epoch 8, calculated loss: 0.3925161037720746\n",
      "Training epoch 9, calculated loss: 0.3346998923982945\n",
      "Training epoch 10, calculated loss: 0.29078823986940117\n",
      "Training epoch 11, calculated loss: 0.2562930989024405\n",
      "Training epoch 12, calculated loss: 0.2282851319826521\n",
      "Training epoch 13, calculated loss: 0.20488885261056808\n",
      "Training epoch 14, calculated loss: 0.1851624968060071\n",
      "Training epoch 15, calculated loss: 0.16851992047006334\n",
      "Training epoch 16, calculated loss: 0.15451366892854068\n",
      "Training epoch 17, calculated loss: 0.14271935539455005\n",
      "Training epoch 18, calculated loss: 0.1327178585577677\n",
      "Training epoch 19, calculated loss: 0.1242114119817114\n",
      "Training epoch 20, calculated loss: 0.11693653948818471\n",
      "Training epoch 21, calculated loss: 0.11065652425393313\n",
      "Training epoch 22, calculated loss: 0.10521455155627073\n",
      "Training epoch 23, calculated loss: 0.10046281052859143\n",
      "Training epoch 24, calculated loss: 0.09627467937115043\n",
      "Training epoch 25, calculated loss: 0.0925565607739147\n",
      "Training epoch 26, calculated loss: 0.08924572292820257\n",
      "Training epoch 27, calculated loss: 0.086269853956357\n",
      "Training epoch 28, calculated loss: 0.08357752772065255\n",
      "Training epoch 29, calculated loss: 0.08113397335597433\n",
      "Training epoch 30, calculated loss: 0.07891180142258644\n",
      "Training epoch 31, calculated loss: 0.07688088421106286\n",
      "Training epoch 32, calculated loss: 0.07500912241677232\n",
      "Training epoch 33, calculated loss: 0.07327944372750338\n",
      "Training epoch 34, calculated loss: 0.0716748154259605\n",
      "Training epoch 35, calculated loss: 0.07018269332773172\n",
      "Training epoch 36, calculated loss: 0.06879070248092817\n",
      "Training epoch 37, calculated loss: 0.0674882418533794\n",
      "Training epoch 38, calculated loss: 0.06626627301947387\n",
      "Training epoch 39, calculated loss: 0.06511619798812292\n",
      "Training epoch 40, calculated loss: 0.06403074115054787\n",
      "Training epoch 41, calculated loss: 0.0630067530724622\n",
      "Training epoch 42, calculated loss: 0.062038377206095965\n",
      "Training epoch 43, calculated loss: 0.06112257026858867\n",
      "Training epoch 44, calculated loss: 0.06025383015988778\n",
      "Training epoch 45, calculated loss: 0.059426603499905736\n",
      "Training epoch 46, calculated loss: 0.058641335949006036\n",
      "Training epoch 47, calculated loss: 0.05788980329925547\n",
      "Training epoch 48, calculated loss: 0.05717066311461107\n",
      "Training epoch 49, calculated loss: 0.05648121899775661\n",
      "Training epoch 50, calculated loss: 0.05581797061107715\n",
      "Training epoch 51, calculated loss: 0.055179673294310526\n",
      "Training epoch 52, calculated loss: 0.0545635054395891\n",
      "Training epoch 53, calculated loss: 0.053971044187427666\n",
      "Training epoch 54, calculated loss: 0.053399121615576375\n",
      "Training epoch 55, calculated loss: 0.05284763421797799\n",
      "Training epoch 56, calculated loss: 0.05231690899776951\n",
      "Training epoch 57, calculated loss: 0.05180354746070126\n",
      "Training epoch 58, calculated loss: 0.051306344978187146\n",
      "Training epoch 59, calculated loss: 0.05082316248311763\n",
      "Training epoch 60, calculated loss: 0.05035270501569593\n",
      "Training epoch 61, calculated loss: 0.04989467120753938\n",
      "Training epoch 62, calculated loss: 0.04944827600727314\n",
      "Training epoch 63, calculated loss: 0.04901420079460849\n",
      "Training epoch 64, calculated loss: 0.0485923925043825\n",
      "Training epoch 65, calculated loss: 0.048180721806699564\n",
      "Training epoch 66, calculated loss: 0.04777998415196984\n",
      "Training epoch 67, calculated loss: 0.04738861501936501\n",
      "Training epoch 68, calculated loss: 0.04700676105602569\n",
      "Training epoch 69, calculated loss: 0.0466338589631813\n",
      "Training epoch 70, calculated loss: 0.04626756950777004\n",
      "Training epoch 71, calculated loss: 0.04590865107738617\n",
      "Training epoch 72, calculated loss: 0.0455568603275545\n",
      "Training epoch 73, calculated loss: 0.045211871372383916\n",
      "Training epoch 74, calculated loss: 0.044873273421436786\n",
      "Training epoch 75, calculated loss: 0.044540955249159755\n",
      "Training epoch 76, calculated loss: 0.04421479609650159\n",
      "Training epoch 77, calculated loss: 0.04389400674958064\n",
      "Training epoch 78, calculated loss: 0.043578543277597526\n",
      "Training epoch 79, calculated loss: 0.04326822900578364\n",
      "Training epoch 80, calculated loss: 0.04296265039535079\n",
      "Training epoch 81, calculated loss: 0.04266276950719431\n",
      "Training epoch 82, calculated loss: 0.04236768834539286\n",
      "Training epoch 83, calculated loss: 0.04207690900328223\n",
      "Training epoch 84, calculated loss: 0.04179032054927925\n",
      "Training epoch 85, calculated loss: 0.041508026293390905\n",
      "Training epoch 86, calculated loss: 0.041229431199347605\n",
      "Training epoch 87, calculated loss: 0.04095482971965161\n",
      "Training epoch 88, calculated loss: 0.04068438729882652\n",
      "Training epoch 89, calculated loss: 0.04041748445608445\n",
      "Training epoch 90, calculated loss: 0.04015382058302206\n",
      "Training epoch 91, calculated loss: 0.03989351476831779\n",
      "Training epoch 92, calculated loss: 0.03963662029594152\n",
      "Training epoch 93, calculated loss: 0.03938327134298444\n",
      "Training epoch 94, calculated loss: 0.039133346475117466\n",
      "Training epoch 95, calculated loss: 0.03888655045579658\n",
      "Training epoch 96, calculated loss: 0.03864279982359351\n",
      "Training epoch 97, calculated loss: 0.03840189121100795\n",
      "Training epoch 98, calculated loss: 0.038163331959375435\n",
      "Training epoch 99, calculated loss: 0.03792738386710379\n",
      "Training epoch 100, calculated loss: 0.03769377383713873\n",
      "Training epoch 101, calculated loss: 0.037462689418297355\n",
      "Training epoch 102, calculated loss: 0.03723403693502435\n",
      "Training epoch 103, calculated loss: 0.03700769225878051\n",
      "Training epoch 104, calculated loss: 0.03678356514891007\n",
      "Training epoch 105, calculated loss: 0.03656173605209449\n",
      "Training epoch 106, calculated loss: 0.03634214315897342\n",
      "Training epoch 107, calculated loss: 0.036124582625798526\n",
      "Training epoch 108, calculated loss: 0.035908923336170705\n",
      "Training epoch 109, calculated loss: 0.0356951344847171\n",
      "Training epoch 110, calculated loss: 0.03548337916352172\n",
      "Training epoch 111, calculated loss: 0.035273534369115435\n",
      "Training epoch 112, calculated loss: 0.0350654350920906\n",
      "Training epoch 113, calculated loss: 0.034859169062589276\n",
      "Training epoch 114, calculated loss: 0.03465477518934033\n",
      "Training epoch 115, calculated loss: 0.034452217303191715\n",
      "Training epoch 116, calculated loss: 0.034251341906204574\n",
      "Training epoch 117, calculated loss: 0.03405209144837716\n",
      "Training epoch 118, calculated loss: 0.03385445319751331\n",
      "Training epoch 119, calculated loss: 0.033658426991012146\n",
      "Training epoch 120, calculated loss: 0.03346397588921645\n",
      "Training epoch 121, calculated loss: 0.03327123804660006\n",
      "Training epoch 122, calculated loss: 0.03308119855493258\n",
      "Training epoch 123, calculated loss: 0.0328944176523672\n",
      "Training epoch 124, calculated loss: 0.032710167420140206\n",
      "Training epoch 125, calculated loss: 0.03253565879959821\n",
      "Training epoch 126, calculated loss: 0.0323654759789022\n",
      "Training epoch 127, calculated loss: 0.032200233753250665\n",
      "Training epoch 128, calculated loss: 0.0320362383428567\n",
      "Training epoch 129, calculated loss: 0.031873527794667206\n",
      "Training epoch 130, calculated loss: 0.03171333682521618\n",
      "Training epoch 131, calculated loss: 0.031556156947719745\n",
      "Training epoch 132, calculated loss: 0.03140015770032732\n",
      "Training epoch 133, calculated loss: 0.031245408408042137\n",
      "Training epoch 134, calculated loss: 0.03109191352121647\n",
      "Training epoch 135, calculated loss: 0.030940204094052154\n",
      "Training epoch 136, calculated loss: 0.030790514266984977\n",
      "Training epoch 137, calculated loss: 0.030642029313758302\n",
      "Training epoch 138, calculated loss: 0.030494727417446726\n",
      "Training epoch 139, calculated loss: 0.03034856127636516\n",
      "Training epoch 140, calculated loss: 0.03020347071039323\n",
      "Training epoch 141, calculated loss: 0.030059420929505778\n",
      "Training epoch 142, calculated loss: 0.029916475764406165\n",
      "Training epoch 143, calculated loss: 0.029775021422206838\n",
      "Training epoch 144, calculated loss: 0.029635051917673818\n",
      "Training epoch 145, calculated loss: 0.02949613339139677\n",
      "Training epoch 146, calculated loss: 0.02935993840226471\n",
      "Training epoch 147, calculated loss: 0.02922833969513686\n",
      "Training epoch 148, calculated loss: 0.029097941234055902\n",
      "Training epoch 149, calculated loss: 0.028969798883189152\n",
      "Training epoch 150, calculated loss: 0.028848024305662412\n",
      "Training epoch 151, calculated loss: 0.028727262587807886\n",
      "Training epoch 152, calculated loss: 0.028607510565560833\n",
      "Training epoch 153, calculated loss: 0.028489185575254688\n",
      "Training epoch 154, calculated loss: 0.028377281942022336\n",
      "Training epoch 155, calculated loss: 0.028269220143985804\n",
      "Training epoch 156, calculated loss: 0.028162084411865917\n",
      "Training epoch 157, calculated loss: 0.028055871156658148\n",
      "Training epoch 158, calculated loss: 0.027950567489597563\n",
      "Training epoch 159, calculated loss: 0.02784614561575677\n",
      "Training epoch 160, calculated loss: 0.027742585946377997\n",
      "Training epoch 161, calculated loss: 0.027639864357811548\n",
      "Training epoch 162, calculated loss: 0.027537967615568898\n",
      "Training epoch 163, calculated loss: 0.027436898940108888\n",
      "Training epoch 164, calculated loss: 0.02733553830780845\n",
      "Training epoch 165, calculated loss: 0.02722675155685308\n",
      "Training epoch 166, calculated loss: 0.027118949229569714\n",
      "Training epoch 167, calculated loss: 0.027012104301842855\n",
      "Training epoch 168, calculated loss: 0.026906031501376302\n",
      "Training epoch 169, calculated loss: 0.026800709318339073\n",
      "Training epoch 170, calculated loss: 0.026696134748914416\n",
      "Training epoch 171, calculated loss: 0.026592336827085995\n",
      "Training epoch 172, calculated loss: 0.026490421784158523\n",
      "Training epoch 173, calculated loss: 0.026390662564046114\n",
      "Training epoch 174, calculated loss: 0.026291641973936186\n",
      "Training epoch 175, calculated loss: 0.026193262835695307\n",
      "Training epoch 176, calculated loss: 0.026095783196075414\n",
      "Training epoch 177, calculated loss: 0.02599903307711625\n",
      "Training epoch 178, calculated loss: 0.02590335043068263\n",
      "Training epoch 179, calculated loss: 0.025809522796089182\n",
      "Training epoch 180, calculated loss: 0.025716387716753292\n",
      "Training epoch 181, calculated loss: 0.02562391062653899\n",
      "Training epoch 182, calculated loss: 0.025535863574221718\n",
      "Training epoch 183, calculated loss: 0.0254517669312273\n",
      "Training epoch 184, calculated loss: 0.025369094884654924\n",
      "Training epoch 185, calculated loss: 0.025287875945016483\n",
      "Training epoch 186, calculated loss: 0.02520919821321161\n",
      "Training epoch 187, calculated loss: 0.025131155061378512\n",
      "Training epoch 188, calculated loss: 0.025053776873631514\n",
      "Training epoch 189, calculated loss: 0.02497699879850671\n",
      "Training epoch 190, calculated loss: 0.024900811382452148\n",
      "Training epoch 191, calculated loss: 0.024825196493763895\n",
      "Training epoch 192, calculated loss: 0.024750203250312897\n",
      "Training epoch 193, calculated loss: 0.024675755312424136\n",
      "Training epoch 194, calculated loss: 0.0246018101911868\n",
      "Training epoch 195, calculated loss: 0.024528412424500098\n",
      "Training epoch 196, calculated loss: 0.024455563550236755\n",
      "Training epoch 197, calculated loss: 0.024383256049459078\n",
      "Training epoch 198, calculated loss: 0.024311482464479794\n",
      "Training epoch 199, calculated loss: 0.02424029131600126\n",
      "Training epoch 200, calculated loss: 0.024169598157041484\n",
      "Training epoch 201, calculated loss: 0.024099418847350257\n",
      "Training epoch 202, calculated loss: 0.024029715734076598\n",
      "Training epoch 203, calculated loss: 0.023960476706554865\n",
      "Training epoch 204, calculated loss: 0.02389175714727621\n",
      "Training epoch 205, calculated loss: 0.023823533863734196\n",
      "Training epoch 206, calculated loss: 0.023755818941390325\n",
      "Training epoch 207, calculated loss: 0.02368865603602478\n",
      "Training epoch 208, calculated loss: 0.023621971869010196\n",
      "Training epoch 209, calculated loss: 0.02355576090957149\n",
      "Training epoch 210, calculated loss: 0.023490019750692922\n",
      "Training epoch 211, calculated loss: 0.02342474204123094\n",
      "Training epoch 212, calculated loss: 0.023359920265250397\n",
      "Training epoch 213, calculated loss: 0.02329555815918436\n",
      "Training epoch 214, calculated loss: 0.023231615511551312\n",
      "Training epoch 215, calculated loss: 0.023168167189470932\n",
      "Training epoch 216, calculated loss: 0.023106503740087346\n",
      "Training epoch 217, calculated loss: 0.023045262943421428\n",
      "Training epoch 218, calculated loss: 0.022984457449207463\n",
      "Training epoch 219, calculated loss: 0.022924086456384756\n",
      "Training epoch 220, calculated loss: 0.02286443977695804\n",
      "Training epoch 221, calculated loss: 0.02280572172555671\n",
      "Training epoch 222, calculated loss: 0.0227474077251105\n",
      "Training epoch 223, calculated loss: 0.022689477924454853\n",
      "Training epoch 224, calculated loss: 0.022631933175149947\n",
      "Training epoch 225, calculated loss: 0.02257474729379584\n",
      "Training epoch 226, calculated loss: 0.02251793322395557\n",
      "Training epoch 227, calculated loss: 0.02246149483281463\n",
      "Training epoch 228, calculated loss: 0.022405433208505115\n",
      "Training epoch 229, calculated loss: 0.022349741194258147\n",
      "Training epoch 230, calculated loss: 0.022294395850001023\n",
      "Training epoch 231, calculated loss: 0.02223941082052468\n",
      "Training epoch 232, calculated loss: 0.022185074002067543\n",
      "Training epoch 233, calculated loss: 0.022131642425037668\n",
      "Training epoch 234, calculated loss: 0.022078571450039428\n",
      "Training epoch 235, calculated loss: 0.02202585954728002\n",
      "Training epoch 236, calculated loss: 0.021973566337548836\n",
      "Training epoch 237, calculated loss: 0.021921986342531407\n",
      "Training epoch 238, calculated loss: 0.021870729922543308\n",
      "Training epoch 239, calculated loss: 0.021819795248452657\n",
      "Training epoch 240, calculated loss: 0.02176917289007299\n",
      "Training epoch 241, calculated loss: 0.021718871369279004\n",
      "Training epoch 242, calculated loss: 0.021668887503075624\n",
      "Training epoch 243, calculated loss: 0.021619218162804186\n",
      "Training epoch 244, calculated loss: 0.021569860213154584\n",
      "Training epoch 245, calculated loss: 0.021520809638471017\n",
      "Training epoch 246, calculated loss: 0.021472057204516306\n",
      "Training epoch 247, calculated loss: 0.021423631068818255\n",
      "Training epoch 248, calculated loss: 0.02137550013346932\n",
      "Training epoch 249, calculated loss: 0.02132765548280479\n",
      "Training epoch 250, calculated loss: 0.02128010485405213\n",
      "Training epoch 251, calculated loss: 0.02123284548711692\n",
      "Training epoch 252, calculated loss: 0.021186781607609013\n",
      "Training epoch 253, calculated loss: 0.021141733105287296\n",
      "Training epoch 254, calculated loss: 0.021097234128283016\n",
      "Training epoch 255, calculated loss: 0.02105349638515682\n",
      "Training epoch 256, calculated loss: 0.021010083801861694\n",
      "Training epoch 257, calculated loss: 0.02096707065064579\n",
      "Training epoch 258, calculated loss: 0.020924282671709576\n",
      "Training epoch 259, calculated loss: 0.020881732591795914\n",
      "Training epoch 260, calculated loss: 0.02083941797737827\n",
      "Training epoch 261, calculated loss: 0.020797336249471478\n",
      "Training epoch 262, calculated loss: 0.02075548494001586\n",
      "Training epoch 263, calculated loss: 0.020713861690447574\n",
      "Training epoch 264, calculated loss: 0.02067246471571141\n",
      "Training epoch 265, calculated loss: 0.020631294105844807\n",
      "Training epoch 266, calculated loss: 0.020590343323235658\n",
      "Training epoch 267, calculated loss: 0.02054959718955501\n",
      "Training epoch 268, calculated loss: 0.02050906791176532\n",
      "Training epoch 269, calculated loss: 0.02046875447928981\n",
      "Training epoch 270, calculated loss: 0.02042865518683383\n",
      "Training epoch 271, calculated loss: 0.02038876527021554\n",
      "Training epoch 272, calculated loss: 0.020349080353884982\n",
      "Training epoch 273, calculated loss: 0.020309602096758778\n",
      "Training epoch 274, calculated loss: 0.020270327237427506\n",
      "Training epoch 275, calculated loss: 0.020231249089619385\n",
      "Training epoch 276, calculated loss: 0.020192395220794416\n",
      "Training epoch 277, calculated loss: 0.02015378568914743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 278, calculated loss: 0.020115372937426216\n",
      "Training epoch 279, calculated loss: 0.02007715507788205\n",
      "Training epoch 280, calculated loss: 0.020039130259555525\n",
      "Training epoch 281, calculated loss: 0.020001296627910566\n",
      "Training epoch 282, calculated loss: 0.019963727902689618\n",
      "Training epoch 283, calculated loss: 0.019926469696129937\n",
      "Training epoch 284, calculated loss: 0.019889399057232634\n",
      "Training epoch 285, calculated loss: 0.01985250867954502\n",
      "Training epoch 286, calculated loss: 0.01981578902672583\n",
      "Training epoch 287, calculated loss: 0.019779244544301487\n",
      "Training epoch 288, calculated loss: 0.01974287469919798\n",
      "Training epoch 289, calculated loss: 0.019706668368943078\n",
      "Training epoch 290, calculated loss: 0.01967061229883547\n",
      "Training epoch 291, calculated loss: 0.01963472790177622\n",
      "Training epoch 292, calculated loss: 0.019599015432734384\n",
      "Training epoch 293, calculated loss: 0.01956352693739724\n",
      "Training epoch 294, calculated loss: 0.01952820511825909\n",
      "Training epoch 295, calculated loss: 0.019493048446728743\n",
      "Training epoch 296, calculated loss: 0.01945805525046133\n",
      "Training epoch 297, calculated loss: 0.01942322399259875\n",
      "Training epoch 298, calculated loss: 0.019388552764505806\n",
      "Training epoch 299, calculated loss: 0.01935403388231458\n",
      "Training epoch 300, calculated loss: 0.01931965884117488\n",
      "Training epoch 301, calculated loss: 0.019285439891646914\n",
      "Training epoch 302, calculated loss: 0.019251376014061048\n",
      "Training epoch 303, calculated loss: 0.019217466591435518\n",
      "Training epoch 304, calculated loss: 0.019183708937534107\n",
      "Training epoch 305, calculated loss: 0.019150101708966392\n",
      "Training epoch 306, calculated loss: 0.019116643422541553\n",
      "Training epoch 307, calculated loss: 0.019083332714025326\n",
      "Training epoch 308, calculated loss: 0.01905016833109527\n",
      "Training epoch 309, calculated loss: 0.019017148767328167\n",
      "Training epoch 310, calculated loss: 0.018984331600369872\n",
      "Training epoch 311, calculated loss: 0.01895173337927134\n",
      "Training epoch 312, calculated loss: 0.018919273520084094\n",
      "Training epoch 313, calculated loss: 0.01888695576066753\n",
      "Training epoch 314, calculated loss: 0.01885478211541755\n",
      "Training epoch 315, calculated loss: 0.018822737788818468\n",
      "Training epoch 316, calculated loss: 0.018790876850025277\n",
      "Training epoch 317, calculated loss: 0.018759243829079793\n",
      "Training epoch 318, calculated loss: 0.01872774015249987\n",
      "Training epoch 319, calculated loss: 0.018696362957911668\n",
      "Training epoch 320, calculated loss: 0.01866509330963509\n",
      "Training epoch 321, calculated loss: 0.018633926944898675\n",
      "Training epoch 322, calculated loss: 0.01860288387934998\n",
      "Training epoch 323, calculated loss: 0.01857196461163533\n",
      "Training epoch 324, calculated loss: 0.018541167970029127\n",
      "Training epoch 325, calculated loss: 0.018510492766506743\n",
      "Training epoch 326, calculated loss: 0.018479937951811003\n",
      "Training epoch 327, calculated loss: 0.018449502414069962\n",
      "Training epoch 328, calculated loss: 0.018419185053160942\n",
      "Training epoch 329, calculated loss: 0.018388984780467292\n",
      "Training epoch 330, calculated loss: 0.018358900518674528\n",
      "Training epoch 331, calculated loss: 0.018328931201589113\n",
      "Training epoch 332, calculated loss: 0.018299075773971402\n",
      "Training epoch 333, calculated loss: 0.018269333191377725\n",
      "Training epoch 334, calculated loss: 0.018239702420008955\n",
      "Training epoch 335, calculated loss: 0.018210183853748407\n",
      "Training epoch 336, calculated loss: 0.018180778734885623\n",
      "Training epoch 337, calculated loss: 0.0181514859466068\n",
      "Training epoch 338, calculated loss: 0.018122346460517375\n",
      "Training epoch 339, calculated loss: 0.018093313408349514\n",
      "Training epoch 340, calculated loss: 0.018064388249859332\n",
      "Training epoch 341, calculated loss: 0.018035575986085354\n",
      "Training epoch 342, calculated loss: 0.01800687472140334\n",
      "Training epoch 343, calculated loss: 0.01797828760862383\n",
      "Training epoch 344, calculated loss: 0.017949801530181893\n",
      "Training epoch 345, calculated loss: 0.01792141572979192\n",
      "Training epoch 346, calculated loss: 0.017893129274587822\n",
      "Training epoch 347, calculated loss: 0.017864941289680997\n",
      "Training epoch 348, calculated loss: 0.01783685088304736\n",
      "Training epoch 349, calculated loss: 0.017808857172134884\n",
      "Training epoch 350, calculated loss: 0.017780958674743066\n",
      "Training epoch 351, calculated loss: 0.017753154363452588\n",
      "Training epoch 352, calculated loss: 0.01772544420130223\n",
      "Training epoch 353, calculated loss: 0.017697827336810353\n",
      "Training epoch 354, calculated loss: 0.017670505139356477\n",
      "Training epoch 355, calculated loss: 0.01764312157620326\n",
      "Training epoch 356, calculated loss: 0.017615794666522796\n",
      "Training epoch 357, calculated loss: 0.01758877334368935\n",
      "Training epoch 358, calculated loss: 0.01756160260899918\n",
      "Training epoch 359, calculated loss: 0.017534618154305317\n",
      "Training epoch 360, calculated loss: 0.017507773942776382\n",
      "Training epoch 361, calculated loss: 0.017480852005937925\n",
      "Training epoch 362, calculated loss: 0.017454183100791016\n",
      "Training epoch 363, calculated loss: 0.017427496928137515\n",
      "Training epoch 364, calculated loss: 0.01740082613664551\n",
      "Training epoch 365, calculated loss: 0.017374472107268273\n",
      "Training epoch 366, calculated loss: 0.01734795801760313\n",
      "Training epoch 367, calculated loss: 0.017321616845881228\n",
      "Training epoch 368, calculated loss: 0.01729542228899278\n",
      "Training epoch 369, calculated loss: 0.017269151403135292\n",
      "Training epoch 370, calculated loss: 0.017243126761711802\n",
      "Training epoch 371, calculated loss: 0.01721709001590805\n",
      "Training epoch 372, calculated loss: 0.017191059244768204\n",
      "Training epoch 373, calculated loss: 0.0171653355779045\n",
      "Training epoch 374, calculated loss: 0.017139453513520645\n",
      "Training epoch 375, calculated loss: 0.0171137314795393\n",
      "Training epoch 376, calculated loss: 0.017088155704738508\n",
      "Training epoch 377, calculated loss: 0.017062552776003256\n",
      "Training epoch 378, calculated loss: 0.01703717154459236\n",
      "Training epoch 379, calculated loss: 0.017011786313254278\n",
      "Training epoch 380, calculated loss: 0.016986398382719318\n",
      "Training epoch 381, calculated loss: 0.016961299064915135\n",
      "Training epoch 382, calculated loss: 0.016936067231226582\n",
      "Training epoch 383, calculated loss: 0.01691094674869566\n",
      "Training epoch 384, calculated loss: 0.016886008516901266\n",
      "Training epoch 385, calculated loss: 0.016860971292149196\n",
      "Training epoch 386, calculated loss: 0.01683611417695145\n",
      "Training epoch 387, calculated loss: 0.01681132174293982\n",
      "Training epoch 388, calculated loss: 0.01678649017626591\n",
      "Training epoch 389, calculated loss: 0.016761889207288282\n",
      "Training epoch 390, calculated loss: 0.016737233985821144\n",
      "Training epoch 391, calculated loss: 0.016712593479934926\n",
      "Training epoch 392, calculated loss: 0.016688231134902066\n",
      "Training epoch 393, calculated loss: 0.016663729211307553\n",
      "Training epoch 394, calculated loss: 0.01663935266942703\n",
      "Training epoch 395, calculated loss: 0.016615125763297142\n",
      "Training epoch 396, calculated loss: 0.016590814529033538\n",
      "Training epoch 397, calculated loss: 0.016566666218162246\n",
      "Training epoch 398, calculated loss: 0.016542568404101638\n",
      "Training epoch 399, calculated loss: 0.016518436432256322\n",
      "Training epoch 400, calculated loss: 0.016494509895733853\n",
      "Training epoch 401, calculated loss: 0.016470529463194432\n",
      "Training epoch 402, calculated loss: 0.016446559918497566\n",
      "Training epoch 403, calculated loss: 0.016422838466575714\n",
      "Training epoch 404, calculated loss: 0.016398990347542435\n",
      "Training epoch 405, calculated loss: 0.016375232609171946\n",
      "Training epoch 406, calculated loss: 0.01635164757184817\n",
      "Training epoch 407, calculated loss: 0.01632796486793203\n",
      "Training epoch 408, calculated loss: 0.016304410694098145\n",
      "Training epoch 409, calculated loss: 0.016280942600118104\n",
      "Training epoch 410, calculated loss: 0.016257478639199432\n",
      "Training epoch 411, calculated loss: 0.01623417543191887\n",
      "Training epoch 412, calculated loss: 0.016210849784015102\n",
      "Training epoch 413, calculated loss: 0.016187593674417976\n",
      "Training epoch 414, calculated loss: 0.01616442671750274\n",
      "Training epoch 415, calculated loss: 0.01614138714754772\n",
      "Training epoch 416, calculated loss: 0.01611828421367016\n",
      "Training epoch 417, calculated loss: 0.01609538633272783\n",
      "Training epoch 418, calculated loss: 0.01607245424551852\n",
      "Training epoch 419, calculated loss: 0.016049515297991753\n",
      "Training epoch 420, calculated loss: 0.016026772812110287\n",
      "Training epoch 421, calculated loss: 0.016003974410484165\n",
      "Training epoch 422, calculated loss: 0.01598122937285836\n",
      "Training epoch 423, calculated loss: 0.015958581203497874\n",
      "Training epoch 424, calculated loss: 0.015935996147799513\n",
      "Training epoch 425, calculated loss: 0.015913348680122976\n",
      "Training epoch 426, calculated loss: 0.015891002990864034\n",
      "Training epoch 427, calculated loss: 0.01586883105921396\n",
      "Training epoch 428, calculated loss: 0.015846671267835684\n",
      "Training epoch 429, calculated loss: 0.015824614083511875\n",
      "Training epoch 430, calculated loss: 0.015802601238421792\n",
      "Training epoch 431, calculated loss: 0.015780578292568057\n",
      "Training epoch 432, calculated loss: 0.015758642009637423\n",
      "Training epoch 433, calculated loss: 0.0157368177463153\n",
      "Training epoch 434, calculated loss: 0.01571487717244905\n",
      "Training epoch 435, calculated loss: 0.015693144401486798\n",
      "Training epoch 436, calculated loss: 0.0156714024601336\n",
      "Training epoch 437, calculated loss: 0.01564965718696547\n",
      "Training epoch 438, calculated loss: 0.01562799610545084\n",
      "Training epoch 439, calculated loss: 0.015606401550491589\n",
      "Training epoch 440, calculated loss: 0.01558484269601838\n",
      "Training epoch 441, calculated loss: 0.01556324910988694\n",
      "Training epoch 442, calculated loss: 0.015541846564335906\n",
      "Training epoch 443, calculated loss: 0.015520395000633826\n",
      "Training epoch 444, calculated loss: 0.01549896292420112\n",
      "Training epoch 445, calculated loss: 0.015477738434365033\n",
      "Training epoch 446, calculated loss: 0.015456451643058622\n",
      "Training epoch 447, calculated loss: 0.015435144268066293\n",
      "Training epoch 448, calculated loss: 0.015414030828237238\n",
      "Training epoch 449, calculated loss: 0.015392880378944332\n",
      "Training epoch 450, calculated loss: 0.015371699997255843\n",
      "Training epoch 451, calculated loss: 0.015350777775014758\n",
      "Training epoch 452, calculated loss: 0.01532985346038134\n",
      "Training epoch 453, calculated loss: 0.015308877608725546\n",
      "Training epoch 454, calculated loss: 0.015288066735668472\n",
      "Training epoch 455, calculated loss: 0.015267312552867986\n",
      "Training epoch 456, calculated loss: 0.015246479688444984\n",
      "Training epoch 457, calculated loss: 0.015225740138539464\n",
      "Training epoch 458, calculated loss: 0.015205136577414973\n",
      "Training epoch 459, calculated loss: 0.015184409081631182\n",
      "Training epoch 460, calculated loss: 0.01516372133056533\n",
      "Training epoch 461, calculated loss: 0.015143236011201027\n",
      "Training epoch 462, calculated loss: 0.015122625647630292\n",
      "Training epoch 463, calculated loss: 0.01510203096718133\n",
      "Training epoch 464, calculated loss: 0.01508156553376889\n",
      "Training epoch 465, calculated loss: 0.01506112028061662\n",
      "Training epoch 466, calculated loss: 0.015040623655817115\n",
      "Training epoch 467, calculated loss: 0.01502023340882088\n",
      "Training epoch 468, calculated loss: 0.014999864126617166\n",
      "Training epoch 469, calculated loss: 0.014979518283802541\n",
      "Training epoch 470, calculated loss: 0.014959208809232464\n",
      "Training epoch 471, calculated loss: 0.014938962000623479\n",
      "Training epoch 472, calculated loss: 0.014918670750383299\n",
      "Training epoch 473, calculated loss: 0.014898431477886132\n",
      "Training epoch 474, calculated loss: 0.01487834565419436\n",
      "Training epoch 475, calculated loss: 0.014858189172753956\n",
      "Training epoch 476, calculated loss: 0.01483858637358962\n",
      "Training epoch 477, calculated loss: 0.014819121424711272\n",
      "Training epoch 478, calculated loss: 0.014799598858749997\n",
      "Training epoch 479, calculated loss: 0.014780102467373048\n",
      "Training epoch 480, calculated loss: 0.014760734610105698\n",
      "Training epoch 481, calculated loss: 0.014741333240890721\n",
      "Training epoch 482, calculated loss: 0.014721925628113237\n",
      "Training epoch 483, calculated loss: 0.014702595955525666\n",
      "Training epoch 484, calculated loss: 0.01468331363495849\n",
      "Training epoch 485, calculated loss: 0.014663987657744671\n",
      "Training epoch 486, calculated loss: 0.01464470305266625\n",
      "Training epoch 487, calculated loss: 0.01462553212848643\n",
      "Training epoch 488, calculated loss: 0.014606324989534389\n",
      "Training epoch 489, calculated loss: 0.014587134029664316\n",
      "Training epoch 490, calculated loss: 0.014568042591314017\n",
      "Training epoch 491, calculated loss: 0.014548924830277485\n",
      "Training epoch 492, calculated loss: 0.014529812388646646\n",
      "Training epoch 493, calculated loss: 0.014510774305594302\n",
      "Training epoch 494, calculated loss: 0.014491769521271815\n",
      "Training epoch 495, calculated loss: 0.014472873689609119\n",
      "Training epoch 496, calculated loss: 0.014454969430756892\n",
      "Training epoch 497, calculated loss: 0.014437170641552787\n",
      "Training epoch 498, calculated loss: 0.014419318261127906\n",
      "Training epoch 499, calculated loss: 0.014401495725134454\n",
      "Training epoch 500, calculated loss: 0.014383760990065046\n"
     ]
    }
   ],
   "source": [
    "# Fit new model \n",
    "\n",
    "nn2.fit(Xtrain2, ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss \n",
    "\n",
    "nn2.plot_loss(imagepath=\"Images/bank_note_model1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred2 = nn2.predict(Xtrain2)\n",
    "test_pred2 = nn2.predict(Xtest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 99%\n",
      "Test accuracy is 97%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn2.accuracy(ytrain2, train_pred2)))\n",
    "print(\"Test accuracy is {}%\".format(nn2.accuracy(ytest2, test_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Sonar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe\n",
    "\n",
    "sonar_df = pd.read_csv('Data/sonar.all-data', header=None, names = ['variable {}'.format(i) for i in range(0,60)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 60)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of DF\n",
    "\n",
    "sonar_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set\n",
    "\n",
    "X3 = np.array(sonar_df.drop(columns = ['variable 59']))\n",
    "\n",
    "sonar_df['variable 59'] = sonar_df['variable 59'].replace({'R': 0, 'M': 1})\n",
    "\n",
    "y_label3 = sonar_df['variable 59'].values.reshape(X3.shape[0], 1)\n",
    "\n",
    "# Make train-test split\n",
    "Xtrain3, Xtest3, ytrain3, ytest3 = train_test_split(X3, y_label3, test_size = 0.3, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new neural network model with layers corresponding to input dimensions of 59 variables\n",
    "\n",
    "nn3 = NeuralNet(layers=[59, 59, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1, calculated loss: 11.286649803449817\n",
      "Training epoch 2, calculated loss: 5.2228808340986905\n",
      "Training epoch 3, calculated loss: 9.855243103494132\n",
      "Training epoch 4, calculated loss: 3.5142809761457268\n",
      "Training epoch 5, calculated loss: 9.512801962440474\n",
      "Training epoch 6, calculated loss: 4.601342669146589\n",
      "Training epoch 7, calculated loss: 8.836636601398274\n",
      "Training epoch 8, calculated loss: 5.268092593009029\n",
      "Training epoch 9, calculated loss: 7.323014717570067\n",
      "Training epoch 10, calculated loss: 6.974537198867016\n",
      "Training epoch 11, calculated loss: 4.029607617475905\n",
      "Training epoch 12, calculated loss: 6.683009814270618\n",
      "Training epoch 13, calculated loss: 3.7493159638812137\n",
      "Training epoch 14, calculated loss: 6.228103213316503\n",
      "Training epoch 15, calculated loss: 3.79748741750229\n",
      "Training epoch 16, calculated loss: 6.111327610215748\n",
      "Training epoch 17, calculated loss: 3.2189704273540802\n",
      "Training epoch 18, calculated loss: 5.229797819997524\n",
      "Training epoch 19, calculated loss: 3.437134143864084\n",
      "Training epoch 20, calculated loss: 5.270102134616651\n",
      "Training epoch 21, calculated loss: 2.7202737167601816\n",
      "Training epoch 22, calculated loss: 4.17621089579708\n",
      "Training epoch 23, calculated loss: 3.119399523478084\n",
      "Training epoch 24, calculated loss: 4.355332347582015\n",
      "Training epoch 25, calculated loss: 2.3919303018602007\n",
      "Training epoch 26, calculated loss: 3.3685176046560175\n",
      "Training epoch 27, calculated loss: 2.477985746432463\n",
      "Training epoch 28, calculated loss: 3.257341564070651\n",
      "Training epoch 29, calculated loss: 2.125165402186916\n",
      "Training epoch 30, calculated loss: 2.75039835025167\n",
      "Training epoch 31, calculated loss: 1.9311899605136142\n",
      "Training epoch 32, calculated loss: 2.459144085206985\n",
      "Training epoch 33, calculated loss: 1.7009470192318257\n",
      "Training epoch 34, calculated loss: 2.182383486388943\n",
      "Training epoch 35, calculated loss: 1.5245448070862528\n",
      "Training epoch 36, calculated loss: 1.9814763784839053\n",
      "Training epoch 37, calculated loss: 1.3851441893553649\n",
      "Training epoch 38, calculated loss: 1.83017675079357\n",
      "Training epoch 39, calculated loss: 1.2780103923147315\n",
      "Training epoch 40, calculated loss: 1.710493792527453\n",
      "Training epoch 41, calculated loss: 1.1817264791377349\n",
      "Training epoch 42, calculated loss: 1.592213736248559\n",
      "Training epoch 43, calculated loss: 1.1006066216490817\n",
      "Training epoch 44, calculated loss: 1.489158014232807\n",
      "Training epoch 45, calculated loss: 1.0334306009402354\n",
      "Training epoch 46, calculated loss: 1.3921487141362607\n",
      "Training epoch 47, calculated loss: 0.9741094632881294\n",
      "Training epoch 48, calculated loss: 1.3061455021933448\n",
      "Training epoch 49, calculated loss: 0.9234741808177922\n",
      "Training epoch 50, calculated loss: 1.2212926663260024\n",
      "Training epoch 51, calculated loss: 0.87851429876547\n",
      "Training epoch 52, calculated loss: 1.1415370741797248\n",
      "Training epoch 53, calculated loss: 0.8332292973991757\n",
      "Training epoch 54, calculated loss: 1.0566375140411945\n",
      "Training epoch 55, calculated loss: 0.7918813661729578\n",
      "Training epoch 56, calculated loss: 0.9831407870013624\n",
      "Training epoch 57, calculated loss: 0.7520124661720433\n",
      "Training epoch 58, calculated loss: 0.9091909775455177\n",
      "Training epoch 59, calculated loss: 0.7143042426894242\n",
      "Training epoch 60, calculated loss: 0.8467643558079941\n",
      "Training epoch 61, calculated loss: 0.6764870238830403\n",
      "Training epoch 62, calculated loss: 0.7818646652674355\n",
      "Training epoch 63, calculated loss: 0.6438193908280061\n",
      "Training epoch 64, calculated loss: 0.7305840406999047\n",
      "Training epoch 65, calculated loss: 0.6126471578202154\n",
      "Training epoch 66, calculated loss: 0.6810044434172896\n",
      "Training epoch 67, calculated loss: 0.5833990531462938\n",
      "Training epoch 68, calculated loss: 0.6321515760864361\n",
      "Training epoch 69, calculated loss: 0.5511052036255407\n",
      "Training epoch 70, calculated loss: 0.5900722736653374\n",
      "Training epoch 71, calculated loss: 0.5221987996583575\n",
      "Training epoch 72, calculated loss: 0.5508426292099594\n",
      "Training epoch 73, calculated loss: 0.4917938735428421\n",
      "Training epoch 74, calculated loss: 0.5117052942472955\n",
      "Training epoch 75, calculated loss: 0.4670960692492558\n",
      "Training epoch 76, calculated loss: 0.4836927889994231\n",
      "Training epoch 77, calculated loss: 0.4477957109533518\n",
      "Training epoch 78, calculated loss: 0.4593978449283094\n",
      "Training epoch 79, calculated loss: 0.4299187132662213\n",
      "Training epoch 80, calculated loss: 0.43973590539301705\n",
      "Training epoch 81, calculated loss: 0.4148448358434856\n",
      "Training epoch 82, calculated loss: 0.42371828544121876\n",
      "Training epoch 83, calculated loss: 0.40237605269500615\n",
      "Training epoch 84, calculated loss: 0.41243918535484686\n",
      "Training epoch 85, calculated loss: 0.39266851705355565\n",
      "Training epoch 86, calculated loss: 0.40246698327058117\n",
      "Training epoch 87, calculated loss: 0.383928980120849\n",
      "Training epoch 88, calculated loss: 0.39112877668752993\n",
      "Training epoch 89, calculated loss: 0.37511345061915147\n",
      "Training epoch 90, calculated loss: 0.3819295183362489\n",
      "Training epoch 91, calculated loss: 0.36846497426493063\n",
      "Training epoch 92, calculated loss: 0.37380569926372775\n",
      "Training epoch 93, calculated loss: 0.36106569833962093\n",
      "Training epoch 94, calculated loss: 0.36684199130432993\n",
      "Training epoch 95, calculated loss: 0.3546758964320889\n",
      "Training epoch 96, calculated loss: 0.3591754311470666\n",
      "Training epoch 97, calculated loss: 0.3477180412304141\n",
      "Training epoch 98, calculated loss: 0.3513301456660486\n",
      "Training epoch 99, calculated loss: 0.3410562398760048\n",
      "Training epoch 100, calculated loss: 0.34344543268250693\n",
      "Training epoch 101, calculated loss: 0.3345726216031691\n",
      "Training epoch 102, calculated loss: 0.3368433173572009\n",
      "Training epoch 103, calculated loss: 0.3288164604431223\n",
      "Training epoch 104, calculated loss: 0.33055820360459554\n",
      "Training epoch 105, calculated loss: 0.32344791937319317\n",
      "Training epoch 106, calculated loss: 0.32511952840043534\n",
      "Training epoch 107, calculated loss: 0.3184248057098576\n",
      "Training epoch 108, calculated loss: 0.3198108875465308\n",
      "Training epoch 109, calculated loss: 0.3137267135772894\n",
      "Training epoch 110, calculated loss: 0.3150754602266564\n",
      "Training epoch 111, calculated loss: 0.30950199373469395\n",
      "Training epoch 112, calculated loss: 0.3105663785753547\n",
      "Training epoch 113, calculated loss: 0.30545757380938987\n",
      "Training epoch 114, calculated loss: 0.30652484320823525\n",
      "Training epoch 115, calculated loss: 0.3016711895961294\n",
      "Training epoch 116, calculated loss: 0.3018545635371855\n",
      "Training epoch 117, calculated loss: 0.2977345642297184\n",
      "Training epoch 118, calculated loss: 0.2982513161135959\n",
      "Training epoch 119, calculated loss: 0.29443929342883507\n",
      "Training epoch 120, calculated loss: 0.2946146578972525\n",
      "Training epoch 121, calculated loss: 0.29105816454047895\n",
      "Training epoch 122, calculated loss: 0.291185514781863\n",
      "Training epoch 123, calculated loss: 0.2879517136236854\n",
      "Training epoch 124, calculated loss: 0.2879797139729244\n",
      "Training epoch 125, calculated loss: 0.28495610654884423\n",
      "Training epoch 126, calculated loss: 0.2848529385274923\n",
      "Training epoch 127, calculated loss: 0.2817787316888763\n",
      "Training epoch 128, calculated loss: 0.28168117558215255\n",
      "Training epoch 129, calculated loss: 0.27879567955854406\n",
      "Training epoch 130, calculated loss: 0.27843568287570364\n",
      "Training epoch 131, calculated loss: 0.2763127853646628\n",
      "Training epoch 132, calculated loss: 0.2760042113211183\n",
      "Training epoch 133, calculated loss: 0.273851952998555\n",
      "Training epoch 134, calculated loss: 0.27330742627072724\n",
      "Training epoch 135, calculated loss: 0.2713232696388742\n",
      "Training epoch 136, calculated loss: 0.27101235982627364\n",
      "Training epoch 137, calculated loss: 0.26895021465305186\n",
      "Training epoch 138, calculated loss: 0.268443744248555\n",
      "Training epoch 139, calculated loss: 0.2665023874706907\n",
      "Training epoch 140, calculated loss: 0.2661887175584996\n",
      "Training epoch 141, calculated loss: 0.2642367859037332\n",
      "Training epoch 142, calculated loss: 0.26392237996177675\n",
      "Training epoch 143, calculated loss: 0.2620299809880459\n",
      "Training epoch 144, calculated loss: 0.26144589503492816\n",
      "Training epoch 145, calculated loss: 0.2597340794846734\n",
      "Training epoch 146, calculated loss: 0.25904805406388137\n",
      "Training epoch 147, calculated loss: 0.2575729548541286\n",
      "Training epoch 148, calculated loss: 0.2568569417707421\n",
      "Training epoch 149, calculated loss: 0.2554520789943446\n",
      "Training epoch 150, calculated loss: 0.25491948148533\n",
      "Training epoch 151, calculated loss: 0.2535507127262512\n",
      "Training epoch 152, calculated loss: 0.25284019865871776\n",
      "Training epoch 153, calculated loss: 0.2515043727971803\n",
      "Training epoch 154, calculated loss: 0.25085970183934964\n",
      "Training epoch 155, calculated loss: 0.24962944919742666\n",
      "Training epoch 156, calculated loss: 0.24893137592855402\n",
      "Training epoch 157, calculated loss: 0.24774918027731568\n",
      "Training epoch 158, calculated loss: 0.2471238931010741\n",
      "Training epoch 159, calculated loss: 0.24594144996153502\n",
      "Training epoch 160, calculated loss: 0.24532813508263146\n",
      "Training epoch 161, calculated loss: 0.24425074772352928\n",
      "Training epoch 162, calculated loss: 0.2435858907498234\n",
      "Training epoch 163, calculated loss: 0.24257440616835368\n",
      "Training epoch 164, calculated loss: 0.2419939614399792\n",
      "Training epoch 165, calculated loss: 0.2409768775374056\n",
      "Training epoch 166, calculated loss: 0.2403026638640008\n",
      "Training epoch 167, calculated loss: 0.23927385657023784\n",
      "Training epoch 168, calculated loss: 0.23861772246647944\n",
      "Training epoch 169, calculated loss: 0.23762628229417476\n",
      "Training epoch 170, calculated loss: 0.23691197013663354\n",
      "Training epoch 171, calculated loss: 0.23611508146848126\n",
      "Training epoch 172, calculated loss: 0.2355828104372454\n",
      "Training epoch 173, calculated loss: 0.2347793015784337\n",
      "Training epoch 174, calculated loss: 0.23424913066077382\n",
      "Training epoch 175, calculated loss: 0.23345186833851375\n",
      "Training epoch 176, calculated loss: 0.23286094996793888\n",
      "Training epoch 177, calculated loss: 0.23211220943851016\n",
      "Training epoch 178, calculated loss: 0.23158016176692095\n",
      "Training epoch 179, calculated loss: 0.2307999854907562\n",
      "Training epoch 180, calculated loss: 0.23026903974030336\n",
      "Training epoch 181, calculated loss: 0.2294735869934387\n",
      "Training epoch 182, calculated loss: 0.2289790018501202\n",
      "Training epoch 183, calculated loss: 0.22820851010810617\n",
      "Training epoch 184, calculated loss: 0.2277377272552004\n",
      "Training epoch 185, calculated loss: 0.22703041734885118\n",
      "Training epoch 186, calculated loss: 0.22657928025892235\n",
      "Training epoch 187, calculated loss: 0.22583909292361856\n",
      "Training epoch 188, calculated loss: 0.2253340467078807\n",
      "Training epoch 189, calculated loss: 0.22464217715647358\n",
      "Training epoch 190, calculated loss: 0.22416837256295702\n",
      "Training epoch 191, calculated loss: 0.2234789074819631\n",
      "Training epoch 192, calculated loss: 0.22304259453375838\n",
      "Training epoch 193, calculated loss: 0.22229565024318002\n",
      "Training epoch 194, calculated loss: 0.22188914471723725\n",
      "Training epoch 195, calculated loss: 0.22115747786418274\n",
      "Training epoch 196, calculated loss: 0.22074400322907411\n",
      "Training epoch 197, calculated loss: 0.2199987072951009\n",
      "Training epoch 198, calculated loss: 0.21949924136824975\n",
      "Training epoch 199, calculated loss: 0.21879090575237542\n",
      "Training epoch 200, calculated loss: 0.21834841496864643\n",
      "Training epoch 201, calculated loss: 0.2177139380110768\n",
      "Training epoch 202, calculated loss: 0.21723257395456574\n",
      "Training epoch 203, calculated loss: 0.21657001374421914\n",
      "Training epoch 204, calculated loss: 0.2162658792866052\n",
      "Training epoch 205, calculated loss: 0.21557872093442698\n",
      "Training epoch 206, calculated loss: 0.2151747318612531\n",
      "Training epoch 207, calculated loss: 0.21444988630502934\n",
      "Training epoch 208, calculated loss: 0.21388351908897538\n",
      "Training epoch 209, calculated loss: 0.2131512433497531\n",
      "Training epoch 210, calculated loss: 0.2126597975113616\n",
      "Training epoch 211, calculated loss: 0.2119266077015331\n",
      "Training epoch 212, calculated loss: 0.21147764580739667\n",
      "Training epoch 213, calculated loss: 0.21071638415798655\n",
      "Training epoch 214, calculated loss: 0.21020932706848347\n",
      "Training epoch 215, calculated loss: 0.20948857823277467\n",
      "Training epoch 216, calculated loss: 0.20903218085347078\n",
      "Training epoch 217, calculated loss: 0.20829284861460598\n",
      "Training epoch 218, calculated loss: 0.20785280379072907\n",
      "Training epoch 219, calculated loss: 0.20709380986021905\n",
      "Training epoch 220, calculated loss: 0.20663722264074583\n",
      "Training epoch 221, calculated loss: 0.2059136624815405\n",
      "Training epoch 222, calculated loss: 0.20550342019255832\n",
      "Training epoch 223, calculated loss: 0.20476807760488303\n",
      "Training epoch 224, calculated loss: 0.204345761202559\n",
      "Training epoch 225, calculated loss: 0.20360692198736852\n",
      "Training epoch 226, calculated loss: 0.20320381389877426\n",
      "Training epoch 227, calculated loss: 0.2026432548536166\n",
      "Training epoch 228, calculated loss: 0.20227870740391934\n",
      "Training epoch 229, calculated loss: 0.2015406983881239\n",
      "Training epoch 230, calculated loss: 0.201095587390373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 231, calculated loss: 0.20051965968125543\n",
      "Training epoch 232, calculated loss: 0.20017513289757488\n",
      "Training epoch 233, calculated loss: 0.19941856018219123\n",
      "Training epoch 234, calculated loss: 0.19907272092458694\n",
      "Training epoch 235, calculated loss: 0.1984425089409663\n",
      "Training epoch 236, calculated loss: 0.19792930808384798\n",
      "Training epoch 237, calculated loss: 0.19743158836426347\n",
      "Training epoch 238, calculated loss: 0.19701726894695681\n",
      "Training epoch 239, calculated loss: 0.196503476844018\n",
      "Training epoch 240, calculated loss: 0.1961529527168602\n",
      "Training epoch 241, calculated loss: 0.19559805130860086\n",
      "Training epoch 242, calculated loss: 0.19527906831551486\n",
      "Training epoch 243, calculated loss: 0.19467722350918837\n",
      "Training epoch 244, calculated loss: 0.19417612512611243\n",
      "Training epoch 245, calculated loss: 0.19370789839484506\n",
      "Training epoch 246, calculated loss: 0.19340602570497104\n",
      "Training epoch 247, calculated loss: 0.19278209747514874\n",
      "Training epoch 248, calculated loss: 0.1925290466771023\n",
      "Training epoch 249, calculated loss: 0.19191402933178842\n",
      "Training epoch 250, calculated loss: 0.1914089030324398\n",
      "Training epoch 251, calculated loss: 0.19090337378466793\n",
      "Training epoch 252, calculated loss: 0.19056837716555594\n",
      "Training epoch 253, calculated loss: 0.18996338575330282\n",
      "Training epoch 254, calculated loss: 0.18953200939502562\n",
      "Training epoch 255, calculated loss: 0.18906895658454798\n",
      "Training epoch 256, calculated loss: 0.1886882863904094\n",
      "Training epoch 257, calculated loss: 0.18806233630822303\n",
      "Training epoch 258, calculated loss: 0.18761667812634053\n",
      "Training epoch 259, calculated loss: 0.18715549495295522\n",
      "Training epoch 260, calculated loss: 0.18685026483534062\n",
      "Training epoch 261, calculated loss: 0.1862480973022548\n",
      "Training epoch 262, calculated loss: 0.18580540160889691\n",
      "Training epoch 263, calculated loss: 0.18529016620606983\n",
      "Training epoch 264, calculated loss: 0.18499338241553878\n",
      "Training epoch 265, calculated loss: 0.18439625365926895\n",
      "Training epoch 266, calculated loss: 0.18388504706000547\n",
      "Training epoch 267, calculated loss: 0.18341146884572757\n",
      "Training epoch 268, calculated loss: 0.18307688503035618\n",
      "Training epoch 269, calculated loss: 0.18255533529037674\n",
      "Training epoch 270, calculated loss: 0.1820458870854864\n",
      "Training epoch 271, calculated loss: 0.18161869969939845\n",
      "Training epoch 272, calculated loss: 0.18127885844268626\n",
      "Training epoch 273, calculated loss: 0.18076725544162603\n",
      "Training epoch 274, calculated loss: 0.18039470900361715\n",
      "Training epoch 275, calculated loss: 0.17990909795577334\n",
      "Training epoch 276, calculated loss: 0.17967855590263213\n",
      "Training epoch 277, calculated loss: 0.17915104974045742\n",
      "Training epoch 278, calculated loss: 0.17884068835696412\n",
      "Training epoch 279, calculated loss: 0.178317362611285\n",
      "Training epoch 280, calculated loss: 0.17810852537037788\n",
      "Training epoch 281, calculated loss: 0.17753708055972314\n",
      "Training epoch 282, calculated loss: 0.17715752404477078\n",
      "Training epoch 283, calculated loss: 0.1766973519159224\n",
      "Training epoch 284, calculated loss: 0.17643143980987744\n",
      "Training epoch 285, calculated loss: 0.17586775984932404\n",
      "Training epoch 286, calculated loss: 0.17557224362402132\n",
      "Training epoch 287, calculated loss: 0.17510042530558378\n",
      "Training epoch 288, calculated loss: 0.1748691122292317\n",
      "Training epoch 289, calculated loss: 0.17425782246377602\n",
      "Training epoch 290, calculated loss: 0.17396229916040323\n",
      "Training epoch 291, calculated loss: 0.1735913140369524\n",
      "Training epoch 292, calculated loss: 0.1734908989178499\n",
      "Training epoch 293, calculated loss: 0.17302234615563933\n",
      "Training epoch 294, calculated loss: 0.17284764430553345\n",
      "Training epoch 295, calculated loss: 0.17246782270420502\n",
      "Training epoch 296, calculated loss: 0.17252687487627985\n",
      "Training epoch 297, calculated loss: 0.17192377057793867\n",
      "Training epoch 298, calculated loss: 0.17178155078432497\n",
      "Training epoch 299, calculated loss: 0.1712602753829215\n",
      "Training epoch 300, calculated loss: 0.1713069408300746\n",
      "Training epoch 301, calculated loss: 0.17062172919142957\n",
      "Training epoch 302, calculated loss: 0.17037831107504717\n",
      "Training epoch 303, calculated loss: 0.16991836129604984\n",
      "Training epoch 304, calculated loss: 0.16998513121213393\n",
      "Training epoch 305, calculated loss: 0.16928967381128948\n",
      "Training epoch 306, calculated loss: 0.16899027797158656\n",
      "Training epoch 307, calculated loss: 0.1685797539686857\n",
      "Training epoch 308, calculated loss: 0.16861927650677253\n",
      "Training epoch 309, calculated loss: 0.16789539023883954\n",
      "Training epoch 310, calculated loss: 0.16778074077117439\n",
      "Training epoch 311, calculated loss: 0.16717797196484457\n",
      "Training epoch 312, calculated loss: 0.16723105270466268\n",
      "Training epoch 313, calculated loss: 0.16644984621176095\n",
      "Training epoch 314, calculated loss: 0.16615967932920714\n",
      "Training epoch 315, calculated loss: 0.16567478805161076\n",
      "Training epoch 316, calculated loss: 0.1656358818867776\n",
      "Training epoch 317, calculated loss: 0.16492196266896358\n",
      "Training epoch 318, calculated loss: 0.16498579681964756\n",
      "Training epoch 319, calculated loss: 0.16422071321172024\n",
      "Training epoch 320, calculated loss: 0.16402375888643098\n",
      "Training epoch 321, calculated loss: 0.16339420349087813\n",
      "Training epoch 322, calculated loss: 0.16318280171890806\n",
      "Training epoch 323, calculated loss: 0.16255742214846897\n",
      "Training epoch 324, calculated loss: 0.1622998422577879\n",
      "Training epoch 325, calculated loss: 0.1617887657479277\n",
      "Training epoch 326, calculated loss: 0.1617612233961217\n",
      "Training epoch 327, calculated loss: 0.1610898946306966\n",
      "Training epoch 328, calculated loss: 0.161037269881028\n",
      "Training epoch 329, calculated loss: 0.16036198275693408\n",
      "Training epoch 330, calculated loss: 0.1603068448405201\n",
      "Training epoch 331, calculated loss: 0.15966671076079844\n",
      "Training epoch 332, calculated loss: 0.15937962866582975\n",
      "Training epoch 333, calculated loss: 0.1588127623860776\n",
      "Training epoch 334, calculated loss: 0.1586985261731488\n",
      "Training epoch 335, calculated loss: 0.15811002213835756\n",
      "Training epoch 336, calculated loss: 0.15801914578779605\n",
      "Training epoch 337, calculated loss: 0.15742490334786313\n",
      "Training epoch 338, calculated loss: 0.1571248166504516\n",
      "Training epoch 339, calculated loss: 0.1566306694765428\n",
      "Training epoch 340, calculated loss: 0.1565154055129919\n",
      "Training epoch 341, calculated loss: 0.15598197170659753\n",
      "Training epoch 342, calculated loss: 0.15603857636605045\n",
      "Training epoch 343, calculated loss: 0.15541379988392198\n",
      "Training epoch 344, calculated loss: 0.1552487423563387\n",
      "Training epoch 345, calculated loss: 0.1547559183835619\n",
      "Training epoch 346, calculated loss: 0.1545576710054801\n",
      "Training epoch 347, calculated loss: 0.15400549991923812\n",
      "Training epoch 348, calculated loss: 0.15380284906766387\n",
      "Training epoch 349, calculated loss: 0.15337554803855502\n",
      "Training epoch 350, calculated loss: 0.15337294132910748\n",
      "Training epoch 351, calculated loss: 0.15279951708650596\n",
      "Training epoch 352, calculated loss: 0.15277113791908606\n",
      "Training epoch 353, calculated loss: 0.15220018689501574\n",
      "Training epoch 354, calculated loss: 0.15190343306385085\n",
      "Training epoch 355, calculated loss: 0.1513962072515862\n",
      "Training epoch 356, calculated loss: 0.15129722801779513\n",
      "Training epoch 357, calculated loss: 0.15079624198696084\n",
      "Training epoch 358, calculated loss: 0.15086378905333317\n",
      "Training epoch 359, calculated loss: 0.15026978149698098\n",
      "Training epoch 360, calculated loss: 0.15027252820367218\n",
      "Training epoch 361, calculated loss: 0.14964158657981136\n",
      "Training epoch 362, calculated loss: 0.1495987461227676\n",
      "Training epoch 363, calculated loss: 0.14900440151303238\n",
      "Training epoch 364, calculated loss: 0.1487650760046058\n",
      "Training epoch 365, calculated loss: 0.1482998415035484\n",
      "Training epoch 366, calculated loss: 0.14816629175582882\n",
      "Training epoch 367, calculated loss: 0.14762663811239168\n",
      "Training epoch 368, calculated loss: 0.1475278928241454\n",
      "Training epoch 369, calculated loss: 0.14693724961887164\n",
      "Training epoch 370, calculated loss: 0.14687005622370222\n",
      "Training epoch 371, calculated loss: 0.1463425357706676\n",
      "Training epoch 372, calculated loss: 0.14622582031878198\n",
      "Training epoch 373, calculated loss: 0.14567817180747708\n",
      "Training epoch 374, calculated loss: 0.14528845579289654\n",
      "Training epoch 375, calculated loss: 0.14490562680198119\n",
      "Training epoch 376, calculated loss: 0.1447337346554163\n",
      "Training epoch 377, calculated loss: 0.14427474832839182\n",
      "Training epoch 378, calculated loss: 0.1441238810416375\n",
      "Training epoch 379, calculated loss: 0.14374409216116113\n",
      "Training epoch 380, calculated loss: 0.1436960158823449\n",
      "Training epoch 381, calculated loss: 0.14324420491852088\n",
      "Training epoch 382, calculated loss: 0.14321031734650075\n",
      "Training epoch 383, calculated loss: 0.1427363794782687\n",
      "Training epoch 384, calculated loss: 0.14267873049607827\n",
      "Training epoch 385, calculated loss: 0.14212385565208188\n",
      "Training epoch 386, calculated loss: 0.14168686804011316\n",
      "Training epoch 387, calculated loss: 0.14116727630398496\n",
      "Training epoch 388, calculated loss: 0.14082588054014358\n",
      "Training epoch 389, calculated loss: 0.14039981587053071\n",
      "Training epoch 390, calculated loss: 0.1401687457986766\n",
      "Training epoch 391, calculated loss: 0.13974952398818136\n",
      "Training epoch 392, calculated loss: 0.13955219084882964\n",
      "Training epoch 393, calculated loss: 0.13908458290292974\n",
      "Training epoch 394, calculated loss: 0.13875450931261937\n",
      "Training epoch 395, calculated loss: 0.1384284154330099\n",
      "Training epoch 396, calculated loss: 0.13819661336824654\n",
      "Training epoch 397, calculated loss: 0.13780052077263233\n",
      "Training epoch 398, calculated loss: 0.13761546595960153\n",
      "Training epoch 399, calculated loss: 0.13724952207196625\n",
      "Training epoch 400, calculated loss: 0.136951462392761\n",
      "Training epoch 401, calculated loss: 0.13666260087093282\n",
      "Training epoch 402, calculated loss: 0.1364226805944169\n",
      "Training epoch 403, calculated loss: 0.13608449918937687\n",
      "Training epoch 404, calculated loss: 0.1358938182323169\n",
      "Training epoch 405, calculated loss: 0.13554930874596\n",
      "Training epoch 406, calculated loss: 0.13528259177683738\n",
      "Training epoch 407, calculated loss: 0.13501695679450562\n",
      "Training epoch 408, calculated loss: 0.13476266338109968\n",
      "Training epoch 409, calculated loss: 0.13445960786665834\n",
      "Training epoch 410, calculated loss: 0.13426574288288842\n",
      "Training epoch 411, calculated loss: 0.13393869168916112\n",
      "Training epoch 412, calculated loss: 0.13373907777871988\n",
      "Training epoch 413, calculated loss: 0.13346806094927918\n",
      "Training epoch 414, calculated loss: 0.13326917184894074\n",
      "Training epoch 415, calculated loss: 0.13297675822004518\n",
      "Training epoch 416, calculated loss: 0.13276300131315935\n",
      "Training epoch 417, calculated loss: 0.1324626423103472\n",
      "Training epoch 418, calculated loss: 0.13225276775651418\n",
      "Training epoch 419, calculated loss: 0.1319332847874203\n",
      "Training epoch 420, calculated loss: 0.13173382084338556\n",
      "Training epoch 421, calculated loss: 0.13144656883307043\n",
      "Training epoch 422, calculated loss: 0.13117204329555926\n",
      "Training epoch 423, calculated loss: 0.13093416277488257\n",
      "Training epoch 424, calculated loss: 0.13068786313670194\n",
      "Training epoch 425, calculated loss: 0.13041531904618295\n",
      "Training epoch 426, calculated loss: 0.1301908766921109\n",
      "Training epoch 427, calculated loss: 0.12988728118329979\n",
      "Training epoch 428, calculated loss: 0.12970399294503066\n",
      "Training epoch 429, calculated loss: 0.12941858916624202\n",
      "Training epoch 430, calculated loss: 0.12912350720168747\n",
      "Training epoch 431, calculated loss: 0.12890006303073454\n",
      "Training epoch 432, calculated loss: 0.12868337504151697\n",
      "Training epoch 433, calculated loss: 0.12842120419370798\n",
      "Training epoch 434, calculated loss: 0.12821122657117992\n",
      "Training epoch 435, calculated loss: 0.12793085923524705\n",
      "Training epoch 436, calculated loss: 0.12774410997862554\n",
      "Training epoch 437, calculated loss: 0.12745736982731762\n",
      "Training epoch 438, calculated loss: 0.1272516624382453\n",
      "Training epoch 439, calculated loss: 0.12699501401229019\n",
      "Training epoch 440, calculated loss: 0.12672647104133342\n",
      "Training epoch 441, calculated loss: 0.1265404340643755\n",
      "Training epoch 442, calculated loss: 0.1263064715275092\n",
      "Training epoch 443, calculated loss: 0.12605050066347323\n",
      "Training epoch 444, calculated loss: 0.12583180057481794\n",
      "Training epoch 445, calculated loss: 0.125545395355549\n",
      "Training epoch 446, calculated loss: 0.12533347105895334\n",
      "Training epoch 447, calculated loss: 0.12505888388178318\n",
      "Training epoch 448, calculated loss: 0.12475723412295117\n",
      "Training epoch 449, calculated loss: 0.12454029142539279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 450, calculated loss: 0.12430045675313199\n",
      "Training epoch 451, calculated loss: 0.1240928424014099\n",
      "Training epoch 452, calculated loss: 0.12387140123059924\n",
      "Training epoch 453, calculated loss: 0.12367567592512277\n",
      "Training epoch 454, calculated loss: 0.12341540623891381\n",
      "Training epoch 455, calculated loss: 0.1231732991851178\n",
      "Training epoch 456, calculated loss: 0.12296889712604749\n",
      "Training epoch 457, calculated loss: 0.1227277772425056\n",
      "Training epoch 458, calculated loss: 0.12247115378966125\n",
      "Training epoch 459, calculated loss: 0.12226740554059927\n",
      "Training epoch 460, calculated loss: 0.1220379751454405\n",
      "Training epoch 461, calculated loss: 0.12189774264761284\n",
      "Training epoch 462, calculated loss: 0.12165504742985678\n",
      "Training epoch 463, calculated loss: 0.12145132761442175\n",
      "Training epoch 464, calculated loss: 0.12124390451675353\n",
      "Training epoch 465, calculated loss: 0.1210414657986651\n",
      "Training epoch 466, calculated loss: 0.12079873470193989\n",
      "Training epoch 467, calculated loss: 0.120558001193105\n",
      "Training epoch 468, calculated loss: 0.1203330347063786\n",
      "Training epoch 469, calculated loss: 0.12000503871556016\n",
      "Training epoch 470, calculated loss: 0.1198150160561601\n",
      "Training epoch 471, calculated loss: 0.11954591513384571\n",
      "Training epoch 472, calculated loss: 0.1193642519447251\n",
      "Training epoch 473, calculated loss: 0.1191107053489777\n",
      "Training epoch 474, calculated loss: 0.11890213746293297\n",
      "Training epoch 475, calculated loss: 0.11870844953279124\n",
      "Training epoch 476, calculated loss: 0.11847661247741119\n",
      "Training epoch 477, calculated loss: 0.11826375686289045\n",
      "Training epoch 478, calculated loss: 0.11804955876529728\n",
      "Training epoch 479, calculated loss: 0.11779117180144892\n",
      "Training epoch 480, calculated loss: 0.11756770634643815\n",
      "Training epoch 481, calculated loss: 0.11734202111992559\n",
      "Training epoch 482, calculated loss: 0.117124798375965\n",
      "Training epoch 483, calculated loss: 0.11690037577195854\n",
      "Training epoch 484, calculated loss: 0.11662771706956446\n",
      "Training epoch 485, calculated loss: 0.11645610023968442\n",
      "Training epoch 486, calculated loss: 0.11622556144541456\n",
      "Training epoch 487, calculated loss: 0.11606863617672125\n",
      "Training epoch 488, calculated loss: 0.11581376643301895\n",
      "Training epoch 489, calculated loss: 0.11564318910538066\n",
      "Training epoch 490, calculated loss: 0.11540537081292565\n",
      "Training epoch 491, calculated loss: 0.11513924249702198\n",
      "Training epoch 492, calculated loss: 0.11492884267749436\n",
      "Training epoch 493, calculated loss: 0.11472877641522593\n",
      "Training epoch 494, calculated loss: 0.1144744341586042\n",
      "Training epoch 495, calculated loss: 0.11437219356033498\n",
      "Training epoch 496, calculated loss: 0.1141008295066524\n",
      "Training epoch 497, calculated loss: 0.11388081104366322\n",
      "Training epoch 498, calculated loss: 0.11365421931556777\n",
      "Training epoch 499, calculated loss: 0.11347722833247643\n",
      "Training epoch 500, calculated loss: 0.11322352004046027\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "nn3.fit(Xtrain3, ytrain3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "\n",
    "nn3.plot_loss(imagepath=\"Images/sonar_dataset_model1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use models to predict\n",
    "\n",
    "train_pred3 = nn3.predict(Xtrain3)\n",
    "test_pred3 = nn3.predict(Xtest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 98%\n",
      "Test accuracy is 85%\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy of predictions\n",
    "\n",
    "print(\"Train accuracy is {}%\".format(nn3.accuracy(ytrain3, train_pred3)))\n",
    "print(\"Test accuracy is {}%\".format(nn3.accuracy(ytest3, test_pred3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
